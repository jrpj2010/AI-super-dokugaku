import os
import sys
import json
import re
import requests
import time
from flask import Flask, request, jsonify, Response
from flask_cors import CORS
from datetime import datetime, timedelta
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound, TranscriptsDisabled
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import isodate
import markdown
import logging
import random
from health import health_bp
import yt_dlp
import tempfile
import io

# ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

app = Flask(__name__)
# CORSã®è¨­å®š - ã™ã¹ã¦ã®ã‚ªãƒªã‚¸ãƒ³ã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¨±å¯
CORS(app, origins="*", supports_credentials=True)

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯Blueprintã‚’ç™»éŒ²
app.register_blueprint(health_bp)

# YouTube APIè¨­å®š
YOUTUBE_API_KEY = os.getenv('YOUTUBE_API_KEY')
youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)

# å›½ã‚³ãƒ¼ãƒ‰ã‹ã‚‰è¨€èªã‚³ãƒ¼ãƒ‰ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°
COUNTRY_TO_LANGUAGE = {
    'US': 'en', 'GB': 'en', 'CA': 'en', 'AU': 'en',
    'JP': 'ja',
    'KR': 'ko',
    'CN': 'zh-Hans', 'TW': 'zh-Hant', 'HK': 'zh-Hant',
    'FR': 'fr',
    'DE': 'de',
    'IT': 'it',
    'ES': 'es',
    'RU': 'ru',
    'BR': 'pt',
    'MX': 'es',
    'IN': 'hi'
}

# ãƒ—ãƒ­ã‚­ã‚·ãƒªã‚¹ãƒˆï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€ãªã‘ã‚Œã°Noneã®ã¿ï¼‰
PROXY_LIST = [None]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒ—ãƒ­ã‚­ã‚·ãªã—
if os.getenv('HTTP_PROXY'):
    PROXY_LIST.append(os.getenv('HTTP_PROXY'))
if os.getenv('HTTPS_PROXY'):
    PROXY_LIST.append(os.getenv('HTTPS_PROXY'))
# Torãƒ—ãƒ­ã‚­ã‚·ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆ
if os.getenv('TOR_PROXY'):
    PROXY_LIST.append(os.getenv('TOR_PROXY'))

# ISO8601å½¢å¼ã®æœŸé–“ã‚’åˆ†ã«å¤‰æ›
def parse_duration_to_minutes(duration_str):
    duration = isodate.parse_duration(duration_str)
    return duration.total_seconds() / 60

# æ™‚é–“ã‚’èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
def format_time(seconds):
    minutes, seconds = divmod(int(seconds), 60)
    hours, minutes = divmod(minutes, 60)
    if hours > 0:
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
    else:
        return f"{minutes:02d}:{seconds:02d}"

# å­—å¹•å–å¾—é–¢æ•°ï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦ã™ï¼‰
def get_transcript(video_id):
    """
    è¤‡æ•°ã®æ–¹æ³•ã‚’è©¦ã—ã¦å­—å¹•ã‚’å–å¾—ã™ã‚‹é–¢æ•°
    """
    logger.info(f"Attempting to get transcript for video ID: {video_id}")
    
    # è¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œ
    methods = [
        get_transcript_with_yt_dlp,    # yt-dlpã‚’æœ€å„ªå…ˆï¼ˆæœ€ã‚‚ä¿¡é ¼æ€§ãŒé«˜ã„ï¼‰
        get_transcript_with_api,       # YouTube Transcript API
        get_transcript_with_scraping   # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯æœ€çµ‚æ‰‹æ®µ
    ]
    
    last_error = None
    for method in methods:
        try:
            result = method(video_id)
            if result and len(result) == 2 and not isinstance(result[0], str):
                # å­—å¹•ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ãå–å¾—ã§ããŸå ´åˆ
                transcript_text = "\n\n".join([f"[{format_time(item['start'])} - {format_time(item['start'] + item['duration'])}]\n{item['text']}" for item in result[0]])
                return transcript_text, result[1]
            elif result and len(result) == 2:
                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¿”ã•ã‚ŒãŸå ´åˆ
                if not result[0].startswith("å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“"):
                    return result
        except Exception as e:
            logger.warning(f"Method {method.__name__} failed: {str(e)}")
            last_error = e
    
    # ã™ã¹ã¦ã®æ–¹æ³•ãŒå¤±æ•—ã—ãŸå ´åˆ
    error_message = f"å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚YouTubeã®åˆ¶é™ã«ã‚ˆã‚Šå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚({str(last_error) if last_error else 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼'})"
    logger.error(f"All transcript methods failed for {video_id}: {error_message}")
    return error_message, ""

# yt-dlpã‚’ä½¿ç”¨ã—ã¦å­—å¹•ã‚’å–å¾—ï¼ˆæ–°ã—ã„æ–¹æ³•ï¼‰
def get_transcript_with_yt_dlp(video_id):
    """
    yt-dlpã‚’ä½¿ç”¨ã—ã¦å­—å¹•ã‚’å–å¾—ã™ã‚‹é–¢æ•°ï¼ˆæœ€ã‚‚ä¿¡é ¼æ€§ãŒé«˜ã„ï¼‰
    """
    logger.info(f"Trying to get transcript with yt-dlp for {video_id}")
    
    try:
        # ãƒ—ãƒ­ã‚­ã‚·ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ
        proxy = random.choice(PROXY_LIST)
        
        # yt-dlpã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'skip_download': True,  # å‹•ç”»ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãªã„
            'writesubtitles': True,
            'writeautomaticsub': True,  # è‡ªå‹•ç”Ÿæˆå­—å¹•ã‚‚å–å¾—
            'subtitleslangs': ['ja', 'en', 'ja-JP', 'en-US'],  # å–å¾—ã™ã‚‹è¨€èª
            'subtitlesformat': 'json3',  # JSONå½¢å¼ã§å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
        }
        
        # ãƒ—ãƒ­ã‚­ã‚·ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
        if proxy:
            logger.info(f"Using proxy: {proxy}")
            ydl_opts['proxy'] = proxy
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        with tempfile.TemporaryDirectory() as temp_dir:
            ydl_opts['outtmpl'] = f'{temp_dir}/%(id)s.%(ext)s'
            
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                try:
                    # å‹•ç”»æƒ…å ±ã‚’å–å¾—
                    info = ydl.extract_info(f"https://www.youtube.com/watch?v={video_id}", download=False)
                    
                    # åˆ©ç”¨å¯èƒ½ãªå­—å¹•ã‚’ç¢ºèª
                    subtitles = info.get('subtitles', {})
                    automatic_captions = info.get('automatic_captions', {})
                    
                    logger.info(f"Available subtitles: {list(subtitles.keys())}")
                    logger.info(f"Available automatic captions: {list(automatic_captions.keys())}")
                    
                    # å­—å¹•ã‚’é¸æŠï¼ˆå„ªå…ˆé †ä½ï¼šæ—¥æœ¬èªæ‰‹å‹• > æ—¥æœ¬èªè‡ªå‹• > è‹±èªæ‰‹å‹• > è‹±èªè‡ªå‹•ï¼‰
                    selected_lang = None
                    selected_subs = None
                    is_auto = False
                    
                    # æ—¥æœ¬èªå­—å¹•ã‚’ãƒã‚§ãƒƒã‚¯
                    for lang in ['ja', 'ja-JP']:
                        if lang in subtitles:
                            selected_lang = lang
                            selected_subs = subtitles[lang]
                            break
                    
                    # æ—¥æœ¬èªè‡ªå‹•å­—å¹•ã‚’ãƒã‚§ãƒƒã‚¯
                    if not selected_subs:
                        for lang in ['ja', 'ja-JP']:
                            if lang in automatic_captions:
                                selected_lang = lang
                                selected_subs = automatic_captions[lang]
                                is_auto = True
                                break
                    
                    # è‹±èªå­—å¹•ã‚’ãƒã‚§ãƒƒã‚¯
                    if not selected_subs:
                        for lang in ['en', 'en-US']:
                            if lang in subtitles:
                                selected_lang = lang
                                selected_subs = subtitles[lang]
                                break
                    
                    # è‹±èªè‡ªå‹•å­—å¹•ã‚’ãƒã‚§ãƒƒã‚¯
                    if not selected_subs:
                        for lang in ['en', 'en-US']:
                            if lang in automatic_captions:
                                selected_lang = lang
                                selected_subs = automatic_captions[lang]
                                is_auto = True
                                break
                    
                    # å­—å¹•ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
                    if not selected_subs:
                        logger.warning(f"No subtitles found via yt-dlp for {video_id}")
                        return "å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã“ã®å‹•ç”»ã«ã¯å­—å¹•ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", ""
                    
                    logger.info(f"Selected {'automatic' if is_auto else 'manual'} subtitle: {selected_lang}")
                    
                    # JSON3å½¢å¼ã®å­—å¹•URLã‚’æ¢ã™
                    json3_url = None
                    for sub in selected_subs:
                        if sub.get('ext') == 'json3':
                            json3_url = sub.get('url')
                            break
                    
                    if not json3_url:
                        # JSON3ãŒãªã„å ´åˆã¯ä»–ã®å½¢å¼ã‚’è©¦ã™
                        for sub in selected_subs:
                            if sub.get('url'):
                                json3_url = sub.get('url')
                                break
                    
                    if not json3_url:
                        logger.warning("No subtitle URL found")
                        return "å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚å­—å¹•URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", ""
                    
                    # å­—å¹•ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                    logger.info(f"Downloading subtitle from: {json3_url[:100]}...")
                    response = requests.get(json3_url, timeout=10)
                    response.raise_for_status()
                    
                    # JSON3å½¢å¼ã®è§£æ
                    subtitle_data = response.json()
                    
                    # å­—å¹•ã‚¨ãƒ³ãƒˆãƒªã‚’å¤‰æ›
                    transcript_data = []
                    for event in subtitle_data.get('events', []):
                        if 'segs' in event:
                            # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
                            text = ''.join([seg.get('utf8', '') for seg in event['segs']])
                            if text.strip():
                                transcript_data.append({
                                    'text': text.strip(),
                                    'start': event.get('tStartMs', 0) / 1000.0,  # ãƒŸãƒªç§’ã‚’ç§’ã«å¤‰æ›
                                    'duration': event.get('dDurationMs', 0) / 1000.0
                                })
                    
                    if not transcript_data:
                        logger.warning("No transcript data extracted from subtitle")
                        return "å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚å­—å¹•ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚", ""
                    
                    logger.info(f"Successfully extracted {len(transcript_data)} transcript entries via yt-dlp")
                    return transcript_data, selected_lang
                    
                except yt_dlp.utils.DownloadError as e:
                    logger.error(f"yt-dlp download error: {str(e)}")
                    if "Private video" in str(e):
                        return "å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã“ã®å‹•ç”»ã¯éå…¬é–‹ã§ã™ã€‚", ""
                    elif "Video unavailable" in str(e):
                        return "å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚å‹•ç”»ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚", ""
                    raise
                    
    except Exception as e:
        logger.error(f"Error in get_transcript_with_yt_dlp: {str(e)}")
        raise

# YouTube Transcript APIã‚’ä½¿ç”¨ã—ã¦å­—å¹•ã‚’å–å¾—
def get_transcript_with_api(video_id):
    """
    YouTube Transcript APIã‚’ä½¿ç”¨ã—ã¦å­—å¹•ã‚’å–å¾—ã™ã‚‹é–¢æ•°
    """
    logger.info(f"Trying to get transcript with YouTube Transcript API for {video_id}")
    
    try:
        # YouTube Data APIã‚’ä½¿ç”¨ã—ã¦å‹•ç”»æƒ…å ±ã‚’å–å¾—
        video_info = youtube.videos().list(
            part='snippet',
            id=video_id
        ).execute()
        
        if not video_info['items']:
            logger.warning(f"Video not found: {video_id}")
            return "å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", ""
        
        # å­—å¹•ã‚’å–å¾—
        try:
            # ãƒ—ãƒ­ã‚­ã‚·ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ
            proxy = random.choice(PROXY_LIST)
            proxies = None
            
            if proxy:
                logger.info(f"Using proxy for YouTube Transcript API: {proxy}")
                proxies = {'http': proxy, 'https': proxy}
            
            # åˆ©ç”¨å¯èƒ½ãªå­—å¹•ãƒªã‚¹ãƒˆã‚’å–å¾—
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id, proxies=proxies)
            
            # åˆ©ç”¨å¯èƒ½ãªå­—å¹•è¨€èªã‚’è¨˜éŒ²
            logger.info(f"Available transcript languages for {video_id}:")
            available_languages = []
            
            for transcript in transcript_list:
                available_languages.append(f"{transcript.language} (Generated: {transcript.is_generated})")
                logger.info(f"  - {transcript.language} (Generated: {transcript.is_generated})")
            
            # æ—¥æœ¬èªå­—å¹•ã‚’å„ªå…ˆã€ãªã‘ã‚Œã°è‹±èªã€ãã‚Œã‚‚ãªã‘ã‚Œã°æœ€åˆã®å­—å¹•ã‚’ä½¿ç”¨
            ja_transcript = None
            en_transcript = None
            first_transcript = None
            
            for transcript in transcript_list:
                if first_transcript is None:
                    first_transcript = transcript
                
                if transcript.language_code.startswith('ja'):
                    ja_transcript = transcript
                    break
                
                if transcript.language_code.startswith('en'):
                    en_transcript = transcript
            
            selected_transcript = ja_transcript or en_transcript or first_transcript
            
            if selected_transcript:
                logger.info(f"Selected transcript: {selected_transcript.language}")
                
                # å­—å¹•ã‚’å–å¾—
                try:
                    transcript_data = selected_transcript.fetch()
                    logger.info(f"Successfully fetched {len(transcript_data)} transcript entries")
                    
                    # ç›´æ¥transcript_dataã‚’è¿”ã™ï¼ˆå¾Œã§æ•´å½¢ï¼‰
                    return transcript_data, selected_transcript.language_code
                except Exception as fetch_error:
                    logger.error(f"Error fetching transcript data: {str(fetch_error)}")
                    logger.error(f"Error type: {type(fetch_error).__name__}")
                    
                    # XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã‚„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒç©ºã®å ´åˆ
                    if "xml" in str(fetch_error).lower() or "no element found" in str(fetch_error):
                        logger.warning(f"Transcript data appears to be empty or corrupted for video {video_id}")
                        # ä»–ã®è¨€èªã‚’è©¦ã—ã¦ã¿ã‚‹
                        logger.info("Attempting alternative languages...")
                        tried_languages = [selected_transcript.language_code]
                        
                        for transcript in transcript_list:
                            if transcript.language_code not in tried_languages:
                                try:
                                    logger.info(f"Trying alternative language: {transcript.language} ({transcript.language_code})")
                                    alt_data = transcript.fetch()
                                    if alt_data and len(alt_data) > 0:
                                        logger.info(f"Successfully fetched transcript in {transcript.language}")
                                        return alt_data, transcript.language_code
                                    tried_languages.append(transcript.language_code)
                                except Exception as alt_error:
                                    logger.warning(f"Failed to fetch {transcript.language}: {str(alt_error)}")
                                    tried_languages.append(transcript.language_code)
                                    continue
                        
                        # ã™ã¹ã¦ã®è¨€èªã§å¤±æ•—ã—ãŸå ´åˆ
                        return "å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã“ã®å‹•ç”»ã®å­—å¹•ãƒ‡ãƒ¼ã‚¿ã¯ç¾åœ¨ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„çŠ¶æ…‹ã§ã™ã€‚", ""
                    
                    raise fetch_error
            else:
                logger.warning(f"No suitable transcripts found for video {video_id}")
                return "å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚é©åˆ‡ãªå­—å¹•ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", ""
                
        except (NoTranscriptFound, TranscriptsDisabled) as e:
            logger.warning(f"No transcript available via API: {str(e)}")
            return "å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã“ã®å‹•ç”»ã«ã¯å­—å¹•ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", ""
        except Exception as e:
            logger.error(f"Error getting transcript via API: {str(e)}")
            # XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
            if "no element found" in str(e) or "xml" in str(e).lower():
                return "å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚å­—å¹•ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", ""
            raise
            
    except Exception as e:
        logger.error(f"Error in get_transcript_with_api: {str(e)}")
        raise

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦å­—å¹•ã‚’å–å¾—ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ–¹æ³•ï¼‰
def get_transcript_with_scraping(video_id):
    """
    ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦å­—å¹•ã‚’å–å¾—ã™ã‚‹é–¢æ•°ï¼ˆAPIãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰
    """
    logger.info(f"Trying to get transcript with scraping for {video_id}")
    
    # ãƒ—ãƒ­ã‚­ã‚·ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠï¼ˆNoneã‚’å«ã‚€ï¼‰
    proxy = random.choice(PROXY_LIST)
    proxies = {'http': proxy, 'https': proxy} if proxy else None
    
    try:
        # YouTubeã®å‹•ç”»ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
        url = f"https://www.youtube.com/watch?v={video_id}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8'
        }
        
        # ãƒ—ãƒ­ã‚­ã‚·ã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        logger.info(f"Requesting {url} with proxy: {proxy}")
        response = requests.get(url, headers=headers, proxies=proxies, timeout=10)
        response.raise_for_status()
        
        # HTMLã‚’è§£æ
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¿ã‚°ã‹ã‚‰JSONãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™
        script_tags = soup.find_all('script')
        transcript_data = None
        
        for script in script_tags:
            if script.string and 'captionTracks' in script.string:
                # JSONãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                json_data = re.search(r'ytInitialPlayerResponse\s*=\s*({.+?});', script.string)
                if json_data:
                    try:
                        data = json.loads(json_data.group(1))
                        captions = data.get('captions', {})
                        caption_tracks = captions.get('playerCaptionsTracklistRenderer', {}).get('captionTracks', [])
                        
                        if caption_tracks:
                            # æ—¥æœ¬èªå­—å¹•ã‚’å„ªå…ˆã€ãªã‘ã‚Œã°è‹±èªã€ãã‚Œã‚‚ãªã‘ã‚Œã°æœ€åˆã®å­—å¹•ã‚’ä½¿ç”¨
                            ja_track = None
                            en_track = None
                            first_track = caption_tracks[0]
                            
                            for track in caption_tracks:
                                lang_code = track.get('languageCode', '')
                                
                                if lang_code.startswith('ja'):
                                    ja_track = track
                                    break
                                
                                if lang_code.startswith('en'):
                                    en_track = track
                            
                            selected_track = ja_track or en_track or first_track
                            
                            if selected_track:
                                # å­—å¹•URLã‚’å–å¾—
                                base_url = selected_track.get('baseUrl', '')
                                if base_url:
                                    # å­—å¹•ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                                    transcript_response = requests.get(base_url, headers=headers, proxies=proxies, timeout=10)
                                    transcript_response.raise_for_status()
                                    
                                    # XMLã‚’è§£æï¼ˆlxml-xmlãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½¿ç”¨ï¼‰
                                    try:
                                        transcript_soup = BeautifulSoup(transcript_response.text, 'lxml-xml')
                                    except:
                                        # lxml-xmlãŒä½¿ãˆãªã„å ´åˆã¯lxmlã‚’ä½¿ã†
                                        transcript_soup = BeautifulSoup(transcript_response.text, 'lxml')
                                    text_elements = transcript_soup.find_all('text')
                                    
                                    if text_elements:
                                        # å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•´å½¢
                                        transcript_lines = []
                                        for element in text_elements:
                                            start = float(element.get('start', 0))
                                            duration = float(element.get('dur', 0)) if element.get('dur') else 2.0
                                            text = element.text
                                            
                                            if text:
                                                transcript_lines.append({
                                                    'start': start,
                                                    'duration': duration,
                                                    'text': text
                                                })
                                        
                                        return transcript_lines, selected_track.get('languageCode', 'unknown')
                    except Exception as e:
                        logger.error(f"Error parsing JSON data: {str(e)}")
        
        logger.warning(f"No transcript found via scraping for {video_id}")
        return "å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ã‚‚å­—å¹•ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", ""
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Request error in get_transcript_with_scraping: {str(e)}")
        raise
    except Exception as e:
        logger.error(f"Error in get_transcript_with_scraping: {str(e)}")
        raise

# æ¤œç´¢API
@app.route('/api/search', methods=['GET'])
def search_videos():
    keyword = request.args.get('keyword', '')
    max_results = int(request.args.get('maxResults', 50))
    published_after = request.args.get('publishedAfter', '')
    published_before = request.args.get('publishedBefore', '')
    video_duration = request.args.get('videoDuration', '')
    order = request.args.get('sortBy', request.args.get('order', 'relevance'))
    use_scraping = request.args.get('useScraping', '0') == '1'
    page_token = request.args.get('pageToken', '')
    
    if not keyword:
        return jsonify({'error': 'Keyword is required'}), 400
    
    try:
        # YouTube URLãŒç›´æ¥å…¥åŠ›ã•ã‚ŒãŸå ´åˆã€å‹•ç”»IDã‚’æŠ½å‡º
        video_id_match = re.search(r'(?:youtube\.com\/watch\?v=|youtu\.be\/)([^&\s]+)', keyword)
        if video_id_match:
            video_id = video_id_match.group(1)
            search_response = youtube.videos().list(
                part='snippet,contentDetails,statistics',
                id=video_id
            ).execute()
        else:
            # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
            search_params = {
                'q': keyword,
                'part': 'snippet',
                'maxResults': max_results,
                'type': 'video',
                'order': order
            }
            
            # ãƒšãƒ¼ã‚¸ãƒˆãƒ¼ã‚¯ãƒ³ãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
            if page_token:
                search_params['pageToken'] = page_token
            
            # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if published_after:
                search_params['publishedAfter'] = f"{published_after}T00:00:00Z"
            if published_before:
                search_params['publishedBefore'] = f"{published_before}T23:59:59Z"
            
            # å‹•ç”»ã®é•·ã•ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if video_duration in ['short', 'medium', 'long']:
                search_params['videoDuration'] = video_duration
            
            # æ¤œç´¢å®Ÿè¡Œ
            search_response = youtube.search().list(**search_params).execute()
            
            # å‹•ç”»IDã®ãƒªã‚¹ãƒˆã‚’å–å¾—
            video_ids = [item['id']['videoId'] for item in search_response['items']]
            
            # æ¬¡ã®ãƒšãƒ¼ã‚¸ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿å­˜
            next_page_token = search_response.get('nextPageToken')
            
            # å‹•ç”»ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
            if video_ids:
                videos_response = youtube.videos().list(
                    part='snippet,contentDetails,statistics',
                    id=','.join(video_ids)
                ).execute()
                
                # æ¤œç´¢ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ›´æ–°ï¼ˆæ¬¡ã®ãƒšãƒ¼ã‚¸ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿æŒï¼‰
                search_response = {
                    'items': videos_response.get('items', []),
                    'nextPageToken': next_page_token,
                    'pageInfo': search_response.get('pageInfo', {})
                }
        
        # çµæœã‚’æ•´å½¢
        items = []
        for item in search_response.get('items', []):
            snippet = item['snippet']
            content_details = item.get('contentDetails', {})
            statistics = item.get('statistics', {})
            
            # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã®URLã‚’å–å¾—ï¼ˆé«˜è§£åƒåº¦å„ªå…ˆï¼‰
            thumbnails = snippet.get('thumbnails', {})
            thumbnail_url = thumbnails.get('high', {}).get('url', '')
            
            # å‹•ç”»æ™‚é–“ã‚’åˆ†å˜ä½ã«å¤‰æ›
            duration = 0
            if 'duration' in content_details:
                duration = parse_duration_to_minutes(content_details['duration'])
            
            # çµæœã«è¿½åŠ 
            items.append({
                'videoId': item['id'],
                'title': snippet['title'],
                'description': snippet['description'],
                'publishedAt': snippet['publishedAt'],
                'channelTitle': snippet['channelTitle'],
                'thumbnail': thumbnail_url,
                'duration': duration,
                'viewCount': statistics.get('viewCount', '0'),
                'likeCount': statistics.get('likeCount', '0'),
                'commentCount': statistics.get('commentCount', '0'),
                'link': f"https://www.youtube.com/watch?v={item['id']}"
            })
        
        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±
        response_data = {
            'items': items,
            'nextPageToken': search_response.get('nextPageToken'),
            'pageInfo': search_response.get('pageInfo', {})
        }
        
        return jsonify(response_data), 200
    
    except Exception as e:
        logger.error(f"Search error: {str(e)}")
        return jsonify({'error': str(e)}), 500

# å­—å¹•å–å¾—API
@app.route('/api/video/<video_id>/transcript', methods=['GET'])
def get_video_transcript(video_id):
    use_scraping = request.args.get('useScraping', '0') == '1'
    
    try:
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹ãƒ­ã‚°
        logger.info(f"Transcript request received for video ID: {video_id}")
        
        # å­—å¹•å–å¾—ã‚’è©¦è¡Œï¼ˆè¤‡æ•°ã®æ–¹æ³•ï¼‰
        transcript_text, language = get_transcript(video_id)
        
        # æˆåŠŸãƒ­ã‚°
        if not isinstance(transcript_text, str) or not transcript_text.startswith("å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“"):
            logger.info(f"Successfully retrieved transcript for {video_id} in {language}")
        else:
            logger.warning(f"Failed to retrieve transcript for {video_id}: {transcript_text}")
        
        return jsonify({
            'transcript': transcript_text,
            'language': language,
            'hostCountry': 'US'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        }), 200
    
    except Exception as e:
        logger.error(f"Transcript error: {str(e)}")
        return jsonify({
            'error': str(e),
            'transcript': f"å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            'language': ""
        }), 500

# ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—API
@app.route('/api/video/<video_id>/comments', methods=['GET'])
def get_comments(video_id):
    max_results = int(request.args.get('maxResults', 20))
    
    try:
        # ã‚³ãƒ¡ãƒ³ãƒˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚’å–å¾—
        comments_response = youtube.commentThreads().list(
            part='snippet,replies',
            videoId=video_id,
            maxResults=max_results,
            order='relevance'
        ).execute()
        
        # ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ•´å½¢
        comments = []
        for item in comments_response['items']:
            comment = item['snippet']['topLevelComment']['snippet']
            
            # è¿”ä¿¡ã‚’å–å¾—
            replies = []
            if 'replies' in item:
                for reply_item in item['replies']['comments']:
                    reply_snippet = reply_item['snippet']
                    replies.append({
                        'id': reply_item['id'],
                        'authorDisplayName': reply_snippet['authorDisplayName'],
                        'authorProfileImageUrl': reply_snippet['authorProfileImageUrl'],
                        'textOriginal': reply_snippet['textOriginal'],
                        'likeCount': reply_snippet['likeCount'],
                        'publishedAt': reply_snippet['publishedAt']
                    })
            
            comments.append({
                'id': item['id'],
                'authorDisplayName': comment['authorDisplayName'],
                'authorProfileImageUrl': comment['authorProfileImageUrl'],
                'textOriginal': comment['textOriginal'],
                'likeCount': comment['likeCount'],
                'publishedAt': comment['publishedAt'],
                'updatedAt': comment['updatedAt'],
                'totalReplyCount': item['snippet']['totalReplyCount'],
                'replies': replies
            })
        
        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±
        response_data = {
            'comments': comments,
            'nextPageToken': comments_response.get('nextPageToken'),
            'pageInfo': {
                'totalResults': comments_response['pageInfo']['totalResults'],
                'resultsPerPage': comments_response['pageInfo']['resultsPerPage']
            }
        }
        
        return jsonify(response_data), 200
    
    except Exception as e:
        logger.error(f"Comments error: {str(e)}")
        return jsonify({'error': str(e)}), 500

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰API - å­—å¹•ã¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦æä¾›
@app.route('/api/download', methods=['GET'])
def download_data():
    video_id = request.args.get('videoId', '')
    use_scraping = request.args.get('useScraping', '0') == '1'
    
    if not video_id:
        return jsonify({'error': 'Video ID is required'}), 400
    
    try:
        # å‹•ç”»æƒ…å ±ã‚’å–å¾—
        video_info = youtube.videos().list(
            part='snippet,contentDetails,statistics',
            id=video_id
        ).execute()
        
        if not video_info['items']:
            return jsonify({'error': 'Video not found'}), 404
        
        video = video_info['items'][0]
        snippet = video['snippet']
        
        # å­—å¹•ã‚’å–å¾—
        transcript_text, language = get_transcript(video_id)
        
        # ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        try:
            comments_response = youtube.commentThreads().list(
                part='snippet,replies',
                videoId=video_id,
                maxResults=20,
                order='relevance'
            ).execute()
            
            comments = []
            for item in comments_response['items']:
                comment = item['snippet']['topLevelComment']['snippet']
                
                # è¿”ä¿¡ã‚’å–å¾—
                replies = []
                if 'replies' in item:
                    for reply_item in item['replies']['comments']:
                        reply_snippet = reply_item['snippet']
                        replies.append({
                            'author': reply_snippet['authorDisplayName'],
                            'text': reply_snippet['textOriginal'],
                            'likes': reply_snippet['likeCount'],
                            'date': reply_snippet['publishedAt']
                        })
                
                comments.append({
                    'author': comment['authorDisplayName'],
                    'text': comment['textOriginal'],
                    'likes': comment['likeCount'],
                    'date': comment['publishedAt'],
                    'replies': replies
                })
        except Exception as e:
            logger.warning(f"Error getting comments: {str(e)}")
            comments = []
        
        # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        md_content = f"# {snippet['title']}\n\n"
        md_content += f"**ãƒãƒ£ãƒ³ãƒãƒ«:** {snippet['channelTitle']}\n"
        md_content += f"**å…¬é–‹æ—¥:** {snippet['publishedAt'][:10]}\n"
        md_content += f"**URL:** https://www.youtube.com/watch?v={video_id}\n\n"
        
        # èª¬æ˜æ–‡
        md_content += "## èª¬æ˜\n\n"
        md_content += f"{snippet['description']}\n\n"
        
        # å­—å¹•
        md_content += "## å­—å¹•\n\n"
        if transcript_text and not isinstance(transcript_text, str):
            # transcript_textãŒãƒªã‚¹ãƒˆã®å ´åˆï¼ˆæ­£å¸¸ã«å–å¾—ã§ããŸå ´åˆï¼‰
            formatted_transcript = "\n\n".join([f"[{format_time(item['start'])} - {format_time(item['start'] + item['duration'])}]\n{item['text']}" for item in transcript_text])
            md_content += f"{formatted_transcript}\n\n"
        elif transcript_text and not transcript_text.startswith("å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“"):
            md_content += f"{transcript_text}\n\n"
        else:
            md_content += "å­—å¹•ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚\n\n"
        
        # ã‚³ãƒ¡ãƒ³ãƒˆ
        md_content += "## ã‚³ãƒ¡ãƒ³ãƒˆ\n\n"
        if comments:
            for i, comment in enumerate(comments, 1):
                md_content += f"### {i}. {comment['author']} (ğŸ‘ {comment['likes']})\n"
                md_content += f"{comment['text']}\n\n"
                
                # è¿”ä¿¡
                if comment['replies']:
                    for reply in comment['replies']:
                        md_content += f"> **{reply['author']}** (ğŸ‘ {reply['likes']}): {reply['text']}\n\n"
        else:
            md_content += "ã‚³ãƒ¡ãƒ³ãƒˆã¯åˆ©ç”¨ã§ããªã„ã‹ã€å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n\n"
        
        # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¨ã—ã¦è¿”ã™
        response = Response(md_content, mimetype='text/markdown')
        response.headers['Content-Disposition'] = f'attachment; filename="{video_id}_transcript.md"'
        return response
    
    except Exception as e:
        logger.error(f"Download error: {str(e)}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=8000)
