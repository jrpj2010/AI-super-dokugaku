# Every Essential AI Skill in 25 Minutes (2025)

**„ÉÅ„É£„É≥„Éç„É´:** Tina Huang
**ÂÖ¨ÈñãÊó•:** 2025-06-12
**URL:** https://www.youtube.com/watch?v=nuEhBT31KQw

## Ë™¨Êòé

Check out retool.com/tina to build enterprise-grade AI agents!

Want to get ahead in your career using AI? Join the waitlist for my AI Agent Bootcamp: https://www.lonelyoctopus.com/ai-agent-bootcamp

In this video I talk about the essential AI skills for 2025.

ü§ù Business Inquiries: https://tally.so/r/mRDV99

üñ±Ô∏èLinks mentioned in video
========================
AI Essentials Video: https://www.youtube.com/watch?v=0Kr1eh1wwb8
Prompt Engineering Video: https://www.youtube.com/watch?v=p09yRj47kNM
YC video: https://www.youtube.com/watch?v=ASABxNenD_U
OpenAI agents SDK: https://openai.github.io/openai-agents-python/
Google ADK: https://google.github.io/adk-docs/ 
Claude Code SDK: https://docs.anthropic.com/en/docs/claude-code/sdk
Anthropic Building Effective Agents: https://www.anthropic.com/engineering/building-effective-agents
Anthropic MCP: https://docs.anthropic.com/en/docs/mcp
Vibe Coding Video: https://www.youtube.com/watch?v=iLCDSY2XX7E
Claude 4 Models & Claude Code Video: https://www.youtube.com/watch?v=3xo1nQGqmYI
Firebase Studio Video: https://www.youtube.com/watch?v=Rd6F5wHIysM
PRD Example: https://docs.google.com/document/d/1NdXLvHqIz4Tmo2RPqigkTx54eBlzGaYnU9pu8ce3Ozo/edit?usp=sharing

üîóAffiliates
========================
My SQL for data science interviews course (10 full interviews):
https://365datascience.com/learn-sql-for-data-science-interviews/ 

365 Data Science: 
https://365datascience.pxf.io/WD0za3 (link for 57% discount for their complete data science training)

Check out StrataScratch for data science interview prep: 
https://stratascratch.com/?via=tina

üé• My filming setup 
========================
üì∑ camera: https://amzn.to/3LHbi7N
üé§ mic: https://amzn.to/3LqoFJb
üî≠ tripod: https://amzn.to/3DkjGHe
üí° lights: https://amzn.to/3LmOhqk

‚è∞Timestamps
========================
00:00 intro 
00:53 AI Basics & Terminologies 
02:30 Prompt Engineering 
09:20 Agents 
16:13 Vibe Coding 
23:15 Future Stuff 

üì≤Socials 
========================
instagram: https://www.instagram.com/hellotinah/
linkedin: https://www.linkedin.com/in/tinaw-h/ 
discord: https://discord.gg/5mMAtprshX

üé•Other videos you might be interested in
========================
How I consistently study with a full time job:
https://www.youtube.com/watch?v=INymz5VwLmk

How I would learn to code (if I could start over): 
https://www.youtube.com/watch?v=MHPGeQD8TvI&t=84s

üêà‚Äç‚¨õüêà‚Äç‚¨õAbout me 
========================
Hi, my name is Tina and I'm an ex-Meta data scientist turned internet person! 

üìßContact
========================
youtube: youtube comments are by far the best way to get a response from me! 
linkedin: https://www.linkedin.com/in/tinaw-h/ 
email for business inquiries only: hellotinah@gmail.com 

========================
Some links are affiliate links and I may receive a small portion of sales price at no cost to you. I really appreciate your support in helping improve this channel! :)

## Â≠óÂπï

[00:00 - 00:03]
I have learned all the AI things for

[00:01 - 00:05]
you. So, here's the cliffnotes version

[00:03 - 00:08]
of everything you need to know about AI

[00:05 - 00:09]
in my opinion in 2025. We'll be going

[00:08 - 00:11]
from beginner to intermediate to

[00:09 - 00:12]
advanced and I'll be giving you a crash

[00:11 - 00:14]
course on each topic as well as

[00:12 - 00:16]
providing more resources for you if you

[00:14 - 00:17]
want to dig deeper into any of them. By

[00:16 - 00:20]
the end of this video, you will know

[00:17 - 00:22]
more about AI than like 99% of the

[00:20 - 00:24]
population. But not if if you don't

[00:22 - 00:25]
actually retain that information. So

[00:24 - 00:26]
there will be little assessments

[00:25 - 00:29]
throughout the video. Now, pay

[00:26 - 00:30]
attention. Let's go. A portion of this

[00:29 - 00:32]
video is sponsored by Retool. Here's the

[00:30 - 00:33]
structure of the video. First, we're

[00:32 - 00:36]
going to go over the basic definitions

[00:33 - 00:38]
of AI and how they work. Then, we'll be

[00:36 - 00:41]
covering prompting, followed by agents

[00:38 - 00:42]
very hot these days, followed by AI

[00:41 - 00:44]
assisted coding. We're building

[00:42 - 00:46]
applications through what is called vibe

[00:44 - 00:48]
coding, and finally looking at some

[00:46 - 00:50]
emerging technologies going into the

[00:48 - 00:52]
second, half, of, 2025., All right,, let's

[00:50 - 00:54]
get started by first defining artificial

[00:52 - 00:56]
intelligence. Artificial intelligence

[00:54 - 00:58]
refers to computer programs that can

[00:56 - 01:00]
complete cognitive tasks typically

[00:58 - 01:02]
associated with human intelligence. Now

[01:00 - 01:04]
AI as a field has been around for a very

[01:02 - 01:06]
long time. And some examples of

[01:04 - 01:08]
traditional artificial intelligence

[01:06 - 01:09]
which back in the day we used to call

[01:08 - 01:11]
machine learning, include things like

[01:09 - 01:13]
Google search algorithms or YouTube's

[01:11 - 01:15]
recommendation system for recommending

[01:13 - 01:18]
you content like this video. But what we

[01:15 - 01:20]
typically refer to as AI these days is

[01:18 - 01:21]
what is called generative AI which is a

[01:20 - 01:23]
specific subset of artificial

[01:21 - 01:25]
intelligence that can generate new

[01:23 - 01:27]
content such as text, images, audio

[01:25 - 01:29]
video, and other types of media. The

[01:27 - 01:32]
most popular example of a generative AI

[01:29 - 01:34]
model is one that can process text and

[01:32 - 01:36]
output text otherwise known as a large

[01:34 - 01:38]
language model or LLM. Some examples of

[01:36 - 01:41]
large language models include the GPT

[01:38 - 01:43]
family from OpenAI, Gemini from Google

[01:41 - 01:45]
and the Claude models from Anthropic.

[01:43 - 01:47]
These days there are so many different

[01:45 - 01:49]
types of models now and many models are

[01:47 - 01:51]
also natively multimodal which means

[01:49 - 01:54]
that you can input and output not only

[01:51 - 01:56]
text but also images, audio and video.

[01:54 - 01:58]
Your favorite models like GPD40 or

[01:56 - 02:00]
Gemini 2.5 Pro are all multimodal. Okay

[01:58 - 02:02]
great. Now you know some of the basic

[02:00 - 02:03]
key terms that is used in the AI world.

[02:02 - 02:05]
So now I'm going to put on screen a

[02:03 - 02:07]
little quiz for this section. Please put

[02:05 - 02:09]
it in the comments below your answers to

[02:07 - 02:11]
these questions. Also, if you want more

[02:09 - 02:12]
details about these Genaii models

[02:11 - 02:14]
including a deeper dive under the hood

[02:12 - 02:16]
of these models, how they're being used

[02:14 - 02:18]
in our workplaces, as well as how to use

[02:16 - 02:19]
AI responsibly, I recommend that you

[02:18 - 02:21]
check out this video, which I'll link

[02:19 - 02:23]
over here, where I condense Google's

[02:21 - 02:25]
8-hour AI essentials course into 15

[02:23 - 02:27]
minutes. But for now, let's move on to

[02:25 - 02:29]
the next section on how to actually get

[02:27 - 02:31]
the most out of these AI models through

[02:29 - 02:33]
prompting. Let's first define prompting.

[02:31 - 02:36]
Prompting is the process of providing

[02:33 - 02:38]
specific instructions to a Genai tool to

[02:36 - 02:40]
receive new information or to achieve a

[02:38 - 02:42]
desired outcome on a task. This can be

[02:40 - 02:44]
through text, images, audio, video, or

[02:42 - 02:46]
even code. Prompting is the single

[02:44 - 02:48]
highest return on investment skill that

[02:46 - 02:50]
you can possibly learn. It's also

[02:48 - 02:51]
foundational for every other more

[02:50 - 02:53]
advanced AI skill. And this makes sense

[02:51 - 02:55]
because prompting is how to communicate

[02:53 - 02:57]
with these AI models. Like you can have

[02:55 - 02:58]
the fanciest models, the fanciest tools

[02:57 - 03:00]
the fanciest whatever, but if you don't

[02:58 - 03:02]
know how to interact with it, it's still

[03:00 - 03:04]
useless. So, if you want to get started

[03:02 - 03:06]
and practice prompting as a beginner

[03:04 - 03:08]
the first step is just to choose your

[03:06 - 03:11]
favorite AI chatbot. That could be Chad

[03:08 - 03:13]
GBT or Gemini or Claude or whatever it

[03:11 - 03:15]
is that you like. Next, I have two

[03:13 - 03:17]
pneumonics for you, which if you can

[03:15 - 03:19]
remember and implement will make you

[03:17 - 03:21]
better at prompting than 98% of the

[03:19 - 03:23]
population. The first one is what I call

[03:21 - 03:26]
the tiny crabs ride enormous iguanas

[03:23 - 03:28]
framework, which stands for task

[03:26 - 03:30]
context resources evaluate and

[03:28 - 03:31]
iterate. When you are crafting a prompt

[03:30 - 03:33]
the first thing that you want to think

[03:31 - 03:35]
of is the task that you want it to do.

[03:33 - 03:37]
What do you want the AI to do? For

[03:35 - 03:39]
example, maybe you want the AI to help

[03:37 - 03:41]
you make some IG posts to market your

[03:39 - 03:43]
new octopus merch line. You could just

[03:41 - 03:45]
prompt it, create an IG post, marketing

[03:43 - 03:47]
my new octopus merch line. And with

[03:45 - 03:49]
that, you'll probably get some okay

[03:47 - 03:51]
results, but you can make the results

[03:49 - 03:54]
much better. First, you can add in a

[03:51 - 03:56]
persona by telling the AI to act as an

[03:54 - 03:58]
expert IG influencer to make the IG

[03:56 - 04:00]
post. This allows the AI to take on the

[03:58 - 04:02]
role of an IG influencer and use some of

[04:00 - 04:04]
that more specific domain knowledge to

[04:02 - 04:06]
make a better IG post. Then you can also

[04:04 - 04:08]
add in the desired format of the output.

[04:06 - 04:10]
The default right now is a generic

[04:08 - 04:11]
caption with some hashtags, right? But

[04:10 - 04:13]
maybe you want something that's a little

[04:11 - 04:15]
bit more structured. You can ask it to

[04:13 - 04:17]
start the caption with a fun fact about

[04:15 - 04:18]
octtopi, then followed by the

[04:17 - 04:21]
announcement and ending with three

[04:18 - 04:23]
relevant hashtags. Great. This is now

[04:21 - 04:24]
already looking much better, but there

[04:23 - 04:27]
is still so much more we can do. The

[04:24 - 04:28]
next part of this framework is context.

[04:27 - 04:30]
The general rule of thumb is that the

[04:28 - 04:32]
more context that you can provide to the

[04:30 - 04:34]
AI, the more specific and the better the

[04:32 - 04:35]
results are going to be. The most

[04:34 - 04:36]
obvious piece of context that we can

[04:35 - 04:38]
provide right now is some pictures of

[04:36 - 04:40]
the actual merch that we're selling. We

[04:38 - 04:41]
can also add in some background about

[04:40 - 04:44]
our company. Like our company is called

[04:41 - 04:46]
Lonely Octopus, where we teach people AI

[04:44 - 04:47]
skills, like our recent AI agents boot

[04:46 - 04:49]
camp, which by the way, we sold out last

[04:47 - 04:51]
time within just 40 hours through the

[04:49 - 04:52]
wait list. So, thank you so much for

[04:51 - 04:54]
that. And we're actually going to be

[04:52 - 04:55]
opening up a new cohort soon. So, do

[04:54 - 04:56]
sign up for the weight list if you're

[04:55 - 04:58]
interested. I will link it over here

[04:56 - 04:59]
also linked in description. Anyways

[04:58 - 05:01]
some additional context that we can give

[04:59 - 05:04]
the AI is that our mascot, which is what

[05:01 - 05:05]
is on the merch here, is called Inky. We

[05:04 - 05:07]
can also be more specific about our

[05:05 - 05:08]
launch date and our target audience for

[05:07 - 05:10]
the merch, like people between the ages

[05:08 - 05:12]
of 20 to 40, mostly working

[05:10 - 05:13]
professionals, something like that. With

[05:12 - 05:16]
this context, your results are going to

[05:13 - 05:18]
be so much more precise and specific to

[05:16 - 05:19]
what you want. But we can do even

[05:18 - 05:20]
better. That's where the next step of

[05:19 - 05:22]
the framework comes in, which is

[05:20 - 05:24]
references. This is where you can

[05:22 - 05:25]
provide examples of some other IG posts

[05:24 - 05:28]
that you like. This way, the AI can take

[05:25 - 05:29]
inspiration from this example. Providing

[05:28 - 05:31]
examples can be so powerful because you

[05:29 - 05:32]
can describe things with words as much

[05:31 - 05:34]
as you like. But, you know, if you just

[05:32 - 05:36]
provide it with an example, there's like

[05:34 - 05:38]
so much there that you can capture the

[05:36 - 05:40]
nuances that you can incorporate into

[05:38 - 05:42]
the results. And voila, you press enter

[05:40 - 05:44]
and here is your IG post. Now, you want

[05:42 - 05:45]
to evaluate. Do you like it? Is there

[05:44 - 05:47]
anything that you want to tweak or want

[05:45 - 05:48]
to change? If so, you go into the final

[05:47 - 05:50]
step of the framework, which is to

[05:48 - 05:52]
iterate. When interacting with AI

[05:50 - 05:54]
models, it is a very iterative process.

[05:52 - 05:55]
So even at the first time it doesn't get

[05:54 - 05:57]
what you want, you can tell it like

[05:55 - 05:58]
tweak a little bit about this, add

[05:57 - 06:01]
something over here, change the color of

[05:58 - 06:02]
something, and you work alongside AI to

[06:01 - 06:05]
get the result that you finally want.

[06:02 - 06:06]
Tiny crabs ride enormous iguanas. If you

[06:05 - 06:08]
can remember this pneummonic and how to

[06:06 - 06:10]
use it, you would be better than 80% of

[06:08 - 06:12]
people at prompting. Let's call it 88

[06:10 - 06:15]
because that is a lucky Chinese number.

[06:12 - 06:16]
But if you want to be better than 98% of

[06:15 - 06:18]
the population, I have one more

[06:16 - 06:20]
framework for you. This is when you do

[06:18 - 06:21]
the tiny crabs ride enormous iguanas

[06:20 - 06:23]
framework and you feel like the results

[06:21 - 06:25]
are still not quite there. Well, you can

[06:23 - 06:27]
elevate this even further using the

[06:25 - 06:29]
ramen saves tragic idiots framework.

[06:27 - 06:31]
First part of the framework is just to

[06:29 - 06:32]
revisit the tiny crabs ride enormous

[06:31 - 06:34]
iguanas framework. See if you can add in

[06:32 - 06:35]
something else, maybe a persona. Be more

[06:34 - 06:37]
detailed about the output, more

[06:35 - 06:39]
references. Also consider taking out

[06:37 - 06:40]
something. Is there any conflicting

[06:39 - 06:42]
information in there that could be

[06:40 - 06:43]
confusing for the AI? Second part of the

[06:42 - 06:45]
framework is to separate the prompt into

[06:43 - 06:48]
shorter sentences. Talking to AI is

[06:45 - 06:50]
similar to talking to a human if you

[06:48 - 06:52]
just like word vomit all over them and

[06:50 - 06:54]
just say like a bunch of things. It can

[06:52 - 06:56]
be confusing for the AI. So you can

[06:54 - 06:58]
consider splitting what you're saying

[06:56 - 07:00]
into shorter sentences to make it more

[06:58 - 07:01]
clear and more concise. So instead of

[07:00 - 07:03]
just being like blah blah blah blah blah

[07:01 - 07:04]
blah blah blah blah blah blah blah blah

[07:03 - 07:09]
blah all over the place, you could just

[07:04 - 07:11]
be like blah then blah then blah. Make

[07:09 - 07:13]
sense? Third part of the framework is to

[07:11 - 07:14]
try different phrasing and analogous

[07:13 - 07:16]
task. For example, maybe you're asking

[07:14 - 07:18]
AI to help you write your speech and

[07:16 - 07:19]
it's just like not quite there, you

[07:18 - 07:21]
know? It's just like not really hitting

[07:19 - 07:22]
it. So maybe you can reframe this.

[07:21 - 07:24]
Instead of saying, "Help me write a

[07:22 - 07:27]
speech," say instead, "Help me write a

[07:24 - 07:28]
story illustrating whatever it is that

[07:27 - 07:30]
you want to illustrate." After all, what

[07:28 - 07:32]
makes a good speech is a compelling and

[07:30 - 07:34]
powerful story. Hello. So, this is Tina

[07:32 - 07:37]
from the future. I have just gotten back

[07:34 - 07:39]
to Hong Kong from Austin, and it seems

[07:37 - 07:41]
like in my jetlegged state, I have

[07:39 - 07:43]
forgotten to record the last part of

[07:41 - 07:45]
this framework. So I'm going to do that

[07:43 - 07:46]
now which is introducing constraints. Do

[07:45 - 07:48]
you have one of those friends where you

[07:46 - 07:50]
know maybe you are that friend when

[07:48 - 07:51]
someone asks like hey what do you want

[07:50 - 07:56]
to get for lunch and they're just like

[07:51 - 07:57]
oh anything. Yeah not very helpful.

[07:56 - 07:58]
Similarly if you feel like the output

[07:57 - 08:00]
from your AI is just like not quite

[07:58 - 08:02]
there. You can consider introducing

[08:00 - 08:04]
constraints to make the results more

[08:02 - 08:06]
specific and targeted. For example maybe

[08:04 - 08:08]
you're making your playlist for a road

[08:06 - 08:09]
trip that you're going on across Texas

[08:08 - 08:11]
and you know you're just really not

[08:09 - 08:12]
quite vibing with it. You can introduce

[08:11 - 08:15]
a constraint like only include country

[08:12 - 08:18]
music in the summertime. Much more

[08:15 - 08:20]
suitable, vibes., All right,, now, back, to

[08:18 - 08:22]
pastina. Got that? Ramen saves tragic

[08:20 - 08:24]
idiots. With these two frameworks

[08:22 - 08:26]
together, you'll be better than 98% of

[08:24 - 08:27]
people at prompting. By the way, I also

[08:26 - 08:29]
just want to say that I didn't just make

[08:27 - 08:30]
up these frameworks myself. I only take

[08:29 - 08:32]
credit for the cool pneumonics. The

[08:30 - 08:34]
actual framework comes from Google

[08:32 - 08:36]
itself. So, if you want to dive even

[08:34 - 08:38]
deeper and be better than like 99% or

[08:36 - 08:40]
even 100% of people at prompting, I

[08:38 - 08:41]
recommend that you check out this video

[08:40 - 08:43]
over here, which I'll link, in which I

[08:41 - 08:44]
summarize Google's prompting course

[08:43 - 08:46]
which is the best general prompting

[08:44 - 08:47]
course that I found so far. Also, I

[08:46 - 08:50]
would recommend checking out some of the

[08:47 - 08:51]
prompt generators for specific models

[08:50 - 08:54]
like this one from OpenAI, this one from

[08:51 - 08:55]
Gemini, and this one from Anthropic.

[08:54 - 08:57]
These are helpful for generating a first

[08:55 - 08:59]
draft and for getting the most out of

[08:57 - 09:00]
specific models. For anybody that thinks

[08:59 - 09:02]
that prompting as a skill is going to

[09:00 - 09:04]
become obsolete, think again. Especially

[09:02 - 09:06]
for more advanced applications like

[09:04 - 09:08]
building agents and coding, prompting is

[09:06 - 09:09]
getting more important than ever. It's

[09:08 - 09:11]
like the glue that holds everything

[09:09 - 09:13]
together to make sure that you get the

[09:11 - 09:15]
results that you want consistently. Now

[09:13 - 09:17]
speaking of more advanced skills, let's

[09:15 - 09:20]
now move on to the next topic, which is

[09:17 - 09:23]
agents.

[09:20 - 09:25]
AI agents are software systems that use

[09:23 - 09:28]
AI to pursue goals and complete tasks on

[09:25 - 09:30]
behalf of users. When we refer to AI

[09:28 - 09:32]
agents, we usually refer to it as an AI

[09:30 - 09:34]
version of a specific type of role. For

[09:32 - 09:36]
example, a customer service AI agent

[09:34 - 09:38]
should be able to receive an email maybe

[09:36 - 09:40]
of somebody being like, I forgot my

[09:38 - 09:41]
password and I can't log in. And it

[09:40 - 09:42]
should be able to reply to that email

[09:41 - 09:45]
and should be able to reference the

[09:42 - 09:47]
forgot password page on the website. As

[09:45 - 09:48]
of today, it can't do everything and it

[09:47 - 09:50]
can't handle all of the queries that a

[09:48 - 09:53]
customer service person should receive

[09:50 - 09:56]
but it can handle a lot of these kind of

[09:53 - 09:58]
generic or common questions that people

[09:56 - 10:00]
may have all autonomously. Similarly

[09:58 - 10:02]
for a coding agent, if you prompt it

[10:00 - 10:04]
well and you tell it to build like a web

[10:02 - 10:06]
application, it should be able to come

[10:04 - 10:08]
back with an MVP version of that web

[10:06 - 10:10]
application. Still got to like add on a

[10:08 - 10:11]
bunch of things and tweak it for sure

[10:10 - 10:13]
but it can write the code for the first

[10:11 - 10:15]
version of it. AI agents is a space

[10:13 - 10:16]
where there's a lot of interest and a

[10:15 - 10:18]
lot of money that is being poured into

[10:16 - 10:19]
it and I really expect them to get

[10:18 - 10:21]
better and better over time and

[10:19 - 10:23]
incorporate into all sorts of products

[10:21 - 10:24]
and businesses. In fact, the most golden

[10:23 - 10:27]
piece of advice that I have ever heard

[10:24 - 10:29]
about AI agents was from this YC video

[10:27 - 10:31]
which is for every SAS software as a

[10:29 - 10:33]
service company there will be a vertical

[10:31 - 10:36]
AI agent version of it. Every company

[10:33 - 10:38]
that is a SAS unicorn you could imagine

[10:36 - 10:40]
there's a vertical AI unicorn

[10:38 - 10:42]
equivalent. So what exactly makes up an

[10:40 - 10:44]
AI agent? Well, there are a lot of

[10:42 - 10:46]
frameworks out there, but the best one

[10:44 - 10:48]
that I've seen so far comes from OpenAI.

[10:46 - 10:51]
They list six components that make up an

[10:48 - 10:53]
AI agent. The first one is the actual AI

[10:51 - 10:55]
model. Can't have an AI agent without a

[10:53 - 10:56]
model. This is the engine that powers

[10:55 - 10:59]
the reasoning and the decision-m

[10:56 - 11:01]
capabilities of the AI agent. Second is

[10:59 - 11:03]
tools. By providing your AI agent with

[11:01 - 11:05]
different types of tools, you allow it

[11:03 - 11:07]
to be able to interact with different

[11:05 - 11:08]
interfaces and access to different

[11:07 - 11:10]
information. For example, you can give

[11:08 - 11:12]
your AI agent an email tool where it's

[11:10 - 11:15]
able to access your email account and be

[11:12 - 11:16]
able to send emails on your behalf. Next

[11:15 - 11:18]
up is knowledge and memory. You can give

[11:16 - 11:21]
your agent access to say like a specific

[11:18 - 11:22]
database about your company so that it's

[11:21 - 11:24]
able to answer questions and be able to

[11:22 - 11:26]
analyze data specific to your company.

[11:24 - 11:28]
Memory is also important when it comes

[11:26 - 11:31]
to specific types of agents. Like say if

[11:28 - 11:32]
you have a therapy agent and you have

[11:31 - 11:33]
like a really great session with it and

[11:32 - 11:34]
then next time around it just like

[11:33 - 11:36]
completely forgets what you're talking

[11:34 - 11:38]
about. That probably wouldn't be great.

[11:36 - 11:40]
So that's why you want to allow your

[11:38 - 11:41]
agent to have access to memory. So it's

[11:40 - 11:43]
able to remember all the different

[11:41 - 11:44]
sessions that you've had previously.

[11:43 - 11:46]
Then we have audio and speech. This

[11:44 - 11:48]
gives your AI agent the capability of

[11:46 - 11:50]
interacting with you through natural

[11:48 - 11:51]
language like being able to just to talk

[11:50 - 11:54]
to it in a variety of different

[11:51 - 11:56]
languages. Then we have guardrails. Be

[11:54 - 11:57]
no good if your AI agent goes rogue and

[11:56 - 11:59]
starts doing things that you don't

[11:57 - 12:01]
intend it to do. So we have systems for

[11:59 - 12:03]
that to make sure that your AI agent is

[12:01 - 12:05]
kept in check. And finally, there is

[12:03 - 12:07]
orchestration. These are processes that

[12:05 - 12:09]
allow you to deploy your agent in

[12:07 - 12:11]
specific environments, monitor them, and

[12:09 - 12:12]
also improve them over time. After you

[12:11 - 12:14]
build an AI agent, you don't just run

[12:12 - 12:16]
away and hope that it works by itself.

[12:14 - 12:18]
Speaking of AI agents, Retool just

[12:16 - 12:19]
launched its enterprisegrade agentic

[12:18 - 12:21]
development platform. Right now, there's

[12:19 - 12:23]
still a big gap between building AI

[12:21 - 12:25]
demos and AI that actually does useful

[12:23 - 12:26]
stuff in your business. Retool allows

[12:25 - 12:28]
you to build apps that connect to your

[12:26 - 12:30]
actual systems and take real actions.

[12:28 - 12:32]
You can use any LM like Claude, Gemini

[12:30 - 12:33]
OpenAI, whatever you want. Your agents

[12:32 - 12:35]
can actually read and write to your

[12:33 - 12:37]
databases, not just chat with you. It

[12:35 - 12:39]
also has endto-end support, including

[12:37 - 12:41]
test and emails to track performance

[12:39 - 12:43]
monitoring, access control, and a lot

[12:41 - 12:45]
more. These are all things that are not

[12:43 - 12:46]
flashy, but really crucial to real

[12:45 - 12:48]
implementation in your business.

[12:46 - 12:50]
Companies that are using retool plus AI

[12:48 - 12:51]
are already seeing really genuinely

[12:50 - 12:53]
impressive results. For example, the

[12:51 - 12:55]
University of Texas Medical Branch has

[12:53 - 12:57]
increased their diagnostic capacity by

[12:55 - 12:59]
10 times. Over 10,000 companies already

[12:57 - 13:01]
use Retool. So if you want to build AI

[12:59 - 13:02]
that is actually useful instead of just

[13:01 - 13:04]
look impressive, do check out

[13:02 - 13:05]
retool.com/tina

[13:04 - 13:07]
also linked in description. Thank you so

[13:05 - 13:08]
much retool for sponsoring this portion

[13:07 - 13:11]
of the video. Models provide

[13:08 - 13:14]
intelligence, tools enable action

[13:11 - 13:15]
memory and knowledge informs decisions

[13:14 - 13:18]
voice and audio enables natural

[13:15 - 13:21]
interaction. Guard rails ensure safety

[13:18 - 13:23]
and orchestration manages them all. I do

[13:21 - 13:24]
also want to point out that prompting is

[13:23 - 13:26]
also really really important when it

[13:24 - 13:28]
comes to agents, especially if you're

[13:26 - 13:30]
building multi- aent systems where

[13:28 - 13:31]
you're not just having a single agent

[13:30 - 13:33]
but you actually have networks of agents

[13:31 - 13:35]
that are interacting with each other.

[13:33 - 13:37]
Your prompts need to be very precise and

[13:35 - 13:38]
produce consistent results. So, how do

[13:37 - 13:40]
we actually build these AI agents like

[13:38 - 13:41]
what are the technologies for this?

[13:40 - 13:43]
There are quite a few currently

[13:41 - 13:46]
available for no code and low code

[13:43 - 13:48]
tools. I personally think nend is the

[13:46 - 13:50]
best for general use cases and gum loop

[13:48 - 13:51]
is great for enterprise use cases. If

[13:50 - 13:53]
you do know how to code, I recommend

[13:51 - 13:55]
checking out OpenAI's agents SDK, which

[13:53 - 13:56]
does have all these components built

[13:55 - 13:59]
into it. Or if you want something that

[13:56 - 14:01]
is free, there is Google's ADK agent

[13:59 - 14:03]
development kit. There's also the Claude

[14:01 - 14:05]
Code SDK, which is specific for coding

[14:03 - 14:07]
agents. Honestly, these different

[14:05 - 14:08]
technologies implementation methods are

[14:07 - 14:10]
going to keep changing over time, and

[14:08 - 14:11]
I'm sure within the next few months

[14:10 - 14:13]
there's going to be even more agent

[14:11 - 14:14]
builders for you to build agents with.

[14:13 - 14:15]
That's why I really recommend that you

[14:14 - 14:17]
actually focus on this fundamental

[14:15 - 14:18]
knowledge about the components of AI

[14:17 - 14:20]
agents, what are the different protocols

[14:18 - 14:21]
and the different systems because this

[14:20 - 14:23]
foundational fundamental knowledge is

[14:21 - 14:24]
not going to change so quickly and it's

[14:23 - 14:26]
going to be applicable to whatever new

[14:24 - 14:27]
tool and technology comes out. So, if

[14:26 - 14:29]
you do want to dive a little bit deeper

[14:27 - 14:31]
into AI agents, I have a video over here

[14:29 - 14:33]
that I made about AI agent fundamentals.

[14:31 - 14:34]
And if you want to get started in

[14:33 - 14:36]
building your AI agents, I also have

[14:34 - 14:38]
another video called building AI agents

[14:36 - 14:40]
which you can check out over here as

[14:38 - 14:42]
well. And I go into a lot more detail

[14:40 - 14:44]
about AI agents. So these are the

[14:42 - 14:45]
components that make up a single AI

[14:44 - 14:47]
agent. But often times you may also want

[14:45 - 14:49]
to build multi- aent systems in which

[14:47 - 14:50]
you don't have just one agent, but you

[14:49 - 14:52]
could have a system of agents that are

[14:50 - 14:53]
working together. And the reason for

[14:52 - 14:54]
this is kind of like if you have a

[14:53 - 14:55]
company and you just have like one

[14:54 - 14:58]
person trying to do everything in the

[14:55 - 14:59]
company, it's probably going to not be

[14:58 - 15:00]
great, right? That person is going to

[14:59 - 15:01]
get very confused trying to manage

[15:00 - 15:03]
everything at the same time. So it's

[15:01 - 15:06]
much better to have people with specific

[15:03 - 15:07]
roles that make up that company. Very

[15:06 - 15:08]
similar with agents. If you just have

[15:07 - 15:10]
one single agent trying to do

[15:08 - 15:11]
everything, then it's going to get

[15:10 - 15:12]
confused. there's going to be like a lot

[15:11 - 15:14]
of stuff that's happening. So, it's

[15:12 - 15:16]
often good to break it down into

[15:14 - 15:17]
different sub aents that have specific

[15:16 - 15:19]
roles and work together in order to get

[15:17 - 15:21]
the result that you want. If you want to

[15:19 - 15:22]
learn more about multi- aent systems

[15:21 - 15:23]
Anthropic has a really great article for

[15:22 - 15:25]
that and I'll link it in the

[15:23 - 15:27]
description. By the way, I'll link all

[15:25 - 15:28]
the resources that I'm referring to in

[15:27 - 15:30]
the descriptions. You may also have

[15:28 - 15:31]
heard about MCP, which is what a lot of

[15:30 - 15:33]
people are talking about these days.

[15:31 - 15:34]
This is also developed from Anthropic

[15:33 - 15:37]
and it's basically a standardized way

[15:34 - 15:38]
for your agents to have access to tools

[15:37 - 15:41]
and knowledge. You can think about it

[15:38 - 15:42]
like a universal USB plug. Prior to MCP

[15:41 - 15:45]
it was actually quite difficult to give

[15:42 - 15:46]
your agents access to certain tools

[15:45 - 15:48]
because all the different websites and

[15:46 - 15:49]
all the different APIs, they do it in a

[15:48 - 15:51]
different way and databases as well.

[15:49 - 15:52]
They're all configured slightly

[15:51 - 15:54]
differently., So,, it, was, kind, of a, pain

[15:52 - 15:57]
in the ass trying to like connect that

[15:54 - 15:59]
with your agent. But with MCP, because

[15:57 - 16:00]
there's a universal USB plug, you're now

[15:59 - 16:02]
able to give your agents any type of

[16:00 - 16:04]
tool and any kind of knowledge very

[16:02 - 16:06]
easily, assuming it follows the MCP

[16:04 - 16:08]
protocol. All right, here is a little

[16:06 - 16:10]
assessment on this agent section. Write

[16:08 - 16:12]
the answers in the comments. Next up

[16:10 - 16:15]
let's move on to using AI to build

[16:12 - 16:19]
applications, aka AI assisted coding

[16:15 - 16:19]
aka vibe coding.

[16:19 - 16:23]
In February of 2025, Andre Kaparthy, the

[16:21 - 16:25]
co-founder of OpenAI, made a viral

[16:23 - 16:27]
tweet. He says, "There's a new kind of

[16:25 - 16:29]
coding I call vibe coding, where you

[16:27 - 16:31]
fully give into the vibes, embrace

[16:29 - 16:33]
exponentials, and forget that the code

[16:31 - 16:35]
even exists. It's possible because the

[16:33 - 16:37]
LMS are getting too good. You simply

[16:35 - 16:39]
tell the AI what it is that you wanted

[16:37 - 16:40]
to build and it just handles the

[16:39 - 16:43]
implementation for you. And this, in my

[16:40 - 16:45]
opinion, is the new way of incorporating

[16:43 - 16:47]
AI into your products and your workflows

[16:45 - 16:49]
using vibe coding to build things. For

[16:47 - 16:51]
example, you can simply tell an LM

[16:49 - 16:54]
please create for me a simple React web

[16:51 - 16:55]
app called Daily Vibes. Users can select

[16:54 - 16:57]
a mood from a list of emojis.

[16:55 - 16:59]
Optionally, write a short note and

[16:57 - 17:01]
submit it below. Show a list of past

[16:59 - 17:03]
mood entries with a date and a note. And

[17:01 - 17:05]
you just click enter. And the LLM writes

[17:03 - 17:08]
the code for you. and generates this

[17:05 - 17:09]
app. And voila, there you go. But it

[17:08 - 17:11]
doesn't just end there. There still are

[17:09 - 17:14]
skills, principles, and best practices

[17:11 - 17:15]
for how to work with AI in order to vibe

[17:14 - 17:17]
code properly and produce products that

[17:15 - 17:19]
are actually usable and scalable. Let me

[17:17 - 17:22]
present to you now a five-step framework

[17:19 - 17:24]
for vibe coding with the pneummonic tiny

[17:22 - 17:26]
ferrets carry dangerous code. dangerous

[17:24 - 17:27]
code because if you don't do it

[17:26 - 17:29]
properly, you could potentially end up

[17:27 - 17:31]
like this guy over here who vibe coded

[17:29 - 17:33]
an app and then lost all of it because

[17:31 - 17:35]
he didn't understand something called

[17:33 - 17:37]
version control. Tiny ferrets carry

[17:35 - 17:39]
dangerous code stands for thinking

[17:37 - 17:42]
frameworks checkpoints debugging and

[17:39 - 17:44]
context. Thinking, as it sounds, is

[17:42 - 17:45]
about thinking really hard about what it

[17:44 - 17:47]
is that you actually want to build. If

[17:45 - 17:48]
you don't even know exactly what it is

[17:47 - 17:50]
that you want to build, how do you

[17:48 - 17:52]
expect AI to be able to do so? The best

[17:50 - 17:53]
way of doing this, in my opinion, is to

[17:52 - 17:55]
create something called a product

[17:53 - 17:57]
requirements document or a PRD. This is

[17:55 - 17:59]
where you define your target audience

[17:57 - 18:00]
your core features, and what it is that

[17:59 - 18:02]
you're going to use to build the product

[18:00 - 18:04]
with. I'll link an example PRD in the

[18:02 - 18:05]
description, but basically, you just

[18:04 - 18:07]
want to spend significant amount of time

[18:05 - 18:10]
thinking through what it is that you're

[18:07 - 18:11]
trying to build. Next up is frameworks.

[18:10 - 18:13]
Whatever it is that you're trying to

[18:11 - 18:14]
build, there has probably been very

[18:13 - 18:16]
similar things that have been built

[18:14 - 18:18]
before. So instead of just trying to

[18:16 - 18:19]
reinvent everything and telling the AI

[18:18 - 18:20]
to figure everything out, it's much

[18:19 - 18:22]
better to point the AI towards the

[18:20 - 18:24]
correct tools for building your specific

[18:22 - 18:27]
product by telling it to use React or

[18:24 - 18:29]
Tailwind or 3.js if you're making 3D

[18:27 - 18:31]
interactive experiences. But Tina, you

[18:29 - 18:33]
may ask, how am I supposed to know what

[18:31 - 18:34]
to tell the AI to use if I don't even

[18:33 - 18:36]
know what it's supposed to use? Great

[18:34 - 18:38]
question. AI can help you with that

[18:36 - 18:39]
too. When you're building your PRD, ask

[18:38 - 18:40]
the AI directly. I'm trying to build

[18:39 - 18:43]
something that's like, you know, like

[18:40 - 18:45]
this and it's very 3D animationheavy

[18:43 - 18:48]
for example,, and, I, want, it, to, be, a, web

[18:45 - 18:49]
app. What are the common frameworks for

[18:48 - 18:50]
building something like this? When

[18:49 - 18:52]
you're asking in this way, you're also

[18:50 - 18:53]
learning yourself what are the common

[18:52 - 18:55]
frameworks for building specific things.

[18:53 - 18:57]
And over time, you're going to have a

[18:55 - 18:59]
much better grasp of what you need to

[18:57 - 19:00]
use as well. In the era of vibe coding

[18:59 - 19:02]
you may not need to code everything by

[19:00 - 19:04]
yourself, but it still serves you very

[19:02 - 19:05]
well to understand the common frameworks

[19:04 - 19:07]
that are used for building different

[19:05 - 19:08]
types of applications. You should also

[19:07 - 19:10]
know how different parts and different

[19:08 - 19:12]
files in your project are interacting

[19:10 - 19:13]
with each other. This is going to help

[19:12 - 19:15]
you out so much as you're building more

[19:13 - 19:17]
and more complex features into your

[19:15 - 19:19]
product. Third step of the framework is

[19:17 - 19:21]
checkpoints. Always use version control

[19:19 - 19:22]
like Git or GitHub or else things will

[19:21 - 19:24]
break and you will lose your progress

[19:22 - 19:26]
and you will feel very very sad like

[19:24 - 19:27]
this guy who vibe coded an entire

[19:26 - 19:29]
application and then lost all of it

[19:27 - 19:31]
because he didn't understand version

[19:29 - 19:32]
control. Fourth step, debugging. you are

[19:31 - 19:34]
probably going to spend more time

[19:32 - 19:37]
debugging and fixing your code than

[19:34 - 19:39]
actually building anything new. That is

[19:37 - 19:41]
the reality. Be methodical and be

[19:39 - 19:43]
patient and guide the AI towards where

[19:41 - 19:45]
it is that it needs to fix. When you're

[19:43 - 19:46]
debugging, if you understand the file

[19:45 - 19:48]
structures and what's happening, then

[19:46 - 19:50]
you're much better at providing specific

[19:48 - 19:52]
instructions for where in your codebase

[19:50 - 19:53]
the AI should be debugging. The first

[19:52 - 19:55]
place to start when you come across an

[19:53 - 19:57]
error is to copy paste the error message

[19:55 - 19:59]
directly into the AI and tell it to try

[19:57 - 20:00]
to fix it. If it's something visual that

[19:59 - 20:02]
needs to be fixed, also provide a

[20:00 - 20:03]
screenshot for the AI. The more details

[20:02 - 20:05]
and the more context that you give the

[20:03 - 20:07]
AI, the better it would be at figuring

[20:05 - 20:09]
out how to fix the problem. And speaking

[20:07 - 20:11]
of context, the final part of the

[20:09 - 20:13]
framework is context. Whenever you're in

[20:11 - 20:14]
doubt, add more context. Generally

[20:13 - 20:16]
speaking, the more context that you

[20:14 - 20:17]
provide to AI, whether you're building

[20:16 - 20:19]
or debugging or you're doing whatever

[20:17 - 20:20]
the better the results are going to be.

[20:19 - 20:22]
This includes providing the AI with

[20:20 - 20:24]
mockups, examples, and screenshots. The

[20:22 - 20:26]
pneummonic to remember for this

[20:24 - 20:28]
five-step framework is tiny ferrets

[20:26 - 20:30]
carry dangerous code. thinking

[20:28 - 20:31]
frameworks checkpoints debugging and

[20:30 - 20:33]
context. A helpful way of thinking about

[20:31 - 20:35]
how these principles of the framework

[20:33 - 20:36]
work well together in the process of

[20:35 - 20:38]
vibe coding is to realize that there's

[20:36 - 20:39]
only two modes that you're ever in.

[20:38 - 20:40]
You're either implementing a feature

[20:39 - 20:42]
where you're debugging your code. When

[20:40 - 20:43]
you're implementing features, you should

[20:42 - 20:45]
be thinking about how to provide more

[20:43 - 20:47]
context, mentioning frameworks, and

[20:45 - 20:48]
making incremental changes. You always

[20:47 - 20:50]
want to approach building new things one

[20:48 - 20:52]
step at a time. Implement one feature at

[20:50 - 20:53]
a time as you build your product. When

[20:52 - 20:55]
you're in debugging mode, you should be

[20:53 - 20:56]
thinking about the underlying structure

[20:55 - 20:58]
of your project, where it is that you

[20:56 - 20:59]
should be pointing the AI towards

[20:58 - 21:01]
changing as well as providing more

[20:59 - 21:02]
context like error messages and

[21:01 - 21:04]
screenshots. So, we now know the

[21:02 - 21:06]
fundamentals of what makes good vibe

[21:04 - 21:07]
coding. So, what are the actual tools

[21:06 - 21:09]
that we use? There are a full spectrum

[21:07 - 21:11]
of development tools available. On one

[21:09 - 21:13]
of the spectrum is for complete

[21:11 - 21:14]
beginners, people who have no

[21:13 - 21:15]
engineering background and no coding

[21:14 - 21:17]
background. Some popular

[21:15 - 21:20]
beginnerfriendly vibe coding tools

[21:17 - 21:21]
include lovable, vzero, and bolt. Then

[21:20 - 21:23]
slightly more intermediate, we have

[21:21 - 21:24]
something like Replet. This is still

[21:23 - 21:26]
very beginner friendly, but it also

[21:24 - 21:27]
showcases the codebase, so you can

[21:26 - 21:28]
actually dig into a little bit more and

[21:27 - 21:30]
understand the structures of the

[21:28 - 21:31]
projects. Then a little bit more

[21:30 - 21:33]
advanced, you have something like

[21:31 - 21:35]
Firebase Studio. Firebase Studio has two

[21:33 - 21:36]
modes to it. It has the very

[21:35 - 21:38]
user-friendly prompting mode as well as

[21:36 - 21:40]
a full ID experience, which stands for

[21:38 - 21:42]
integrated development environment, an

[21:40 - 21:44]
interface that is specifically designed

[21:42 - 21:45]
for writing and working with code. In

[21:44 - 21:47]
this case, it was built on top of VS

[21:45 - 21:48]
Code, which is a very popular ID. With

[21:47 - 21:50]
Firebase Studio, you can alternate

[21:48 - 21:52]
between the no code prompting view and

[21:50 - 21:54]
decoding mode. Firebase Studio also has

[21:52 - 21:56]
the benefit of being free. Now, moving

[21:54 - 21:58]
on to the more advanced vibe coding

[21:56 - 22:00]
tools. This will include AI code editors

[21:58 - 22:01]
and coding agents like Windsurf and

[22:00 - 22:03]
Cursor. Everything that we talked about

[22:01 - 22:04]
earlier was all web- based, so the setup

[22:03 - 22:06]
is really easy. The environment is

[22:04 - 22:07]
isolated and it takes care of a lot of

[22:06 - 22:09]
things for you. But if you really want

[22:07 - 22:11]
to produce productionready scalable

[22:09 - 22:12]
code, then you generally need to start

[22:11 - 22:14]
migrating to using something like

[22:12 - 22:15]
windsurf and cursor. Development is

[22:14 - 22:16]
going to be on your local machine. So

[22:15 - 22:18]
the setup is going to be a little bit

[22:16 - 22:20]
more complex but you also have access to

[22:18 - 22:21]
a full suite of development tools and

[22:20 - 22:23]
different features for Windsor and

[22:21 - 22:25]
cursor. You just directly have that

[22:23 - 22:27]
coding environment that IDE. Then on the

[22:25 - 22:29]
most advanced side of the spectrum you

[22:27 - 22:30]
have command line tools like cloud code

[22:29 - 22:32]
for example., These, are, tools, that, live

[22:30 - 22:34]
directly in your terminal in the root of

[22:32 - 22:35]
your computer. With these tools you need

[22:34 - 22:37]
to be comfortable working in the

[22:35 - 22:38]
terminal or the command line. But it

[22:37 - 22:40]
does give you so much more functionality

[22:38 - 22:42]
and you can use it with any type of ID

[22:40 - 22:43]
of your choosing. Something like cloud

[22:42 - 22:45]
code really begins to shine when you're

[22:43 - 22:46]
working on complex code bases. But the

[22:45 - 22:48]
expectation here is that you do really

[22:46 - 22:50]
need to know how to code and know your

[22:48 - 22:51]
way around a computer and have a deep

[22:50 - 22:54]
understanding, of, software., All right,

[22:51 - 22:56]
that is a crash course on vibe coding.

[22:54 - 22:57]
If you do want to dig into this more, I

[22:56 - 22:59]
made a full video called Vibe Coding

[22:57 - 23:01]
Fundamentals where I go into a lot more

[22:59 - 23:03]
detail. I also made a video specifically

[23:01 - 23:05]
about Firebase Studio which I'll link

[23:03 - 23:06]
over here and another one where I talk

[23:05 - 23:08]
about the cloud for models and cloud

[23:06 - 23:10]
code which I'll link over here too. Now

[23:08 - 23:12]
I will put on screen a little assessment

[23:10 - 23:14]
to see if we have retained information

[23:12 - 23:16]
about vibe coding. Final section out.

[23:14 - 23:20]
What are things looking like going into

[23:16 - 23:20]
the future?

[23:21 - 23:25]
In the AI world, we don't measure things

[23:23 - 23:28]
in terms of years or even months. We

[23:25 - 23:29]
measure things in terms of weeks. And

[23:28 - 23:31]
the timelines are just getting more and

[23:29 - 23:32]
more compressed. When I was at the code

[23:31 - 23:34]
with cloud conference, Daario, the CEO

[23:32 - 23:36]
of Enthropic, made a really good

[23:34 - 23:38]
analogy. He says that it's basically

[23:36 - 23:40]
like being strapped on a rocket that is

[23:38 - 23:41]
going through time and time and space

[23:40 - 23:43]
are warping so that everything is

[23:41 - 23:45]
speeding up faster and faster and

[23:43 - 23:46]
faster. And especially because of this

[23:45 - 23:47]
if you're just trying to keep up with

[23:46 - 23:49]
all the AI news, all the things that are

[23:47 - 23:51]
coming out, all the new models, all the

[23:49 - 23:52]
new tools, all the new technologies, you

[23:51 - 23:54]
will never be able to catch up with

[23:52 - 23:55]
everything and probably get really

[23:54 - 23:57]
stressed along the way, too. So that's

[23:55 - 23:58]
why my advice is to not pay too much

[23:57 - 24:00]
attention to all the new things that are

[23:58 - 24:02]
coming out, but instead focus on the

[24:00 - 24:03]
underlying trends that are happening.

[24:02 - 24:05]
And I think there are three major

[24:03 - 24:07]
underlying trends. The first one is

[24:05 - 24:10]
integration into workflows and existing

[24:07 - 24:11]
products. 2025 is definitely the year in

[24:10 - 24:13]
which people are taking the AI and

[24:11 - 24:14]
actually integrating it into their

[24:13 - 24:16]
existing workflows. Prime example of

[24:14 - 24:18]
this is Google itself. I was at their

[24:16 - 24:20]
Google IO conference and they are

[24:18 - 24:22]
putting a lot of effort into just making

[24:20 - 24:23]
Google products better by integrating AI

[24:22 - 24:25]
throughout. And I think this should be a

[24:23 - 24:27]
model for all companies. Think about how

[24:25 - 24:29]
do you improve your processes by

[24:27 - 24:31]
incorporating AI to have a better user

[24:29 - 24:32]
experience and also to reduce your cost.

[24:31 - 24:34]
And when it comes to implementation of

[24:32 - 24:36]
this, there's massive productivity boost

[24:34 - 24:37]
if you learn how to do AI assisted

[24:36 - 24:39]
coding or vibe coding. With this full

[24:37 - 24:40]
spectrum of coding tools, there's a

[24:39 - 24:42]
dramatic decrease in barrier of entry

[24:40 - 24:44]
for people who want to build things and

[24:42 - 24:45]
who may not know how to code. But

[24:44 - 24:46]
there's also a big push towards

[24:45 - 24:49]
increasing the productivity of

[24:46 - 24:50]
developers. After experiencing command

[24:49 - 24:52]
line tools like cloud code, I can

[24:50 - 24:53]
absolutely see the massive benefits of

[24:52 - 24:55]
tools like this. And I think there's

[24:53 - 24:57]
going to be massive focus of developing

[24:55 - 24:58]
and improving command line tools. So I

[24:57 - 25:00]
think if you are technical or if you're

[24:58 - 25:01]
someone who's willing to learn technical

[25:00 - 25:04]
things, learning command line tools like

[25:01 - 25:05]
cloud code is going to be where it's at.

[25:04 - 25:07]
And finally, the focus on AI agents is

[25:05 - 25:09]
not going away at all. In fact, there's

[25:07 - 25:11]
more and more interest in building AI

[25:09 - 25:13]
agents because AI agents have so much

[25:11 - 25:15]
potential in improving existing products

[25:13 - 25:17]
and for building new products as well.

[25:15 - 25:20]
AI agents allow experiences to be

[25:17 - 25:22]
personalized, available 24/7, and at

[25:20 - 25:24]
much much lower cost. Like Weissy said

[25:22 - 25:26]
for every SAS unicorn company, there

[25:24 - 25:27]
will probably be an equivalent AI agent

[25:26 - 25:28]
company. I'm sure in the coming few

[25:27 - 25:30]
months, there's going to be more and

[25:28 - 25:31]
more tools that will allow you to

[25:30 - 25:32]
implement and build agents even more

[25:31 - 25:33]
easily. So, if you want to build

[25:32 - 25:35]
something, build a business, do a

[25:33 - 25:37]
startup, whatever, I would recommend

[25:35 - 25:39]
looking into AI agents. All right, that

[25:37 - 25:40]
is all I have for you today. Here is a

[25:39 - 25:42]
final little assessment. Please answer

[25:40 - 25:44]
these questions in the comments. Thank

[25:42 - 25:45]
you so much for watching till the end of

[25:44 - 25:47]
this video. I'm so excited to see all

[25:45 - 25:49]
the things that you guys are going to do

[25:47 - 25:50]
and build using AI. I really hope this

[25:49 - 25:52]
is helpful and good luck on your AI

[25:50 - 25:56]
journey. I will see you guys in next

[25:52 - 25:56]
video or live stream.

## „Ç≥„É°„É≥„Éà

### 1. @TinaHuang1 (üëç 38)
Check out retool.com/tina to build enterprise-grade AI agents!

> **@KeywordManagement** (üëç 1): Have not tried this one, but will check it out.

> **@thatotherguy1037** (üëç 1): This is awesome work,  thank you so much ‚ù§

> **@silentwater79** (üëç 2): In what way is it different to n8n?

> **@VirtousStoic** (üëç 1): Tysm happy father days

> **@nicoleandalfonso6355** (üëç 0): What is the definition of of an enterprise-grade AI agent vs a non enterprise AI agent?

### 2. @mentalEscape (üëç 3)
Thanks for sharing knowledge. Love your channel.

### 3. @FelipeNakayama-Burattini (üëç 31)
As usual, amazing content! Thank you SO MUCH for all the time you dedicate on these videos! Love them!

> **@ItsNicolau** (üëç 7): Holy damn, 200‚Ç¨ ü§Øü§Ø

> **@VincenzoJimmie** (üëç 4): Wow‚Äî25 minutes is barely enough time to skim the surface. AI mastery takes months of hands-on learning, not a condensed highlight reel

> **@DBrown-vh5yy** (üëç 3): @@VincenzoJimmie This is how people 'get their feet wet' with the subject matter...

> **@adolf2186** (üëç 3): could have fed myself for a month with that money

> **@hahaahahahahahahah** (üëç 1): Thank mom and dad!

### 4. @ydmoskow (üëç 67)
I find that for prompting, the first thing I tell the llm is that is a prompt engineer and I ask it to write the prompt using best practices.  Them I take that prompt and feed it into a new chat.  No need to remember iguanas or strange sentences.

> **@ItsNicolau** (üëç 2): This! I've been doing a similar strategy for more than 6 months already, and it works super well üëåüèª

> **@Jelly-sk2ry** (üëç 1): I do this

> **@GeyzerSoze** (üëç 5): I do this, and also invite the prompt engineer to develop a Q&A to fill in any info or context that may be useful & I may have missed, which will help it create the best prompt. It loves that shit

> **@chantalfillon7590** (üëç 1): Yep. In the legal field we say ‚Äúyou‚Äôre an experienced paralegal, legal assistant, etc.‚Äù

> **@king_xena** (üëç 0): Sorry i dont follow how does this help, when we have different tasks? E.g we want to learn X, whats the point of telling it about prompt engineering and then pasting prompt eng to a new chat when it has nothing to do with X task/goal

### 5. @KymDee (üëç 1)
Exceptional content. Your video helped to gain a overview understanding of the major areas/flow/aspects of AI. Thank you.

### 6. @fabionano13 (üëç 33)
First Quiz:
1.They are subsets of Artifficial Inteligence.
2.Gemini, GPT, Claude.
3.Multimodality means that LLMs can input and output data in various types (like text, video, sound, code).

Second Quiz:
1.
Model
Tools
Knowledge & memory
Audio & speech
Guardrails
Orchestration
2. It requires memory so it can remember previous conversations and get the context allowing it to provide relevant answers.
3. MCP is a protocol that allows our developed agents to connect to others services like gmail, github, etc. After the sucessfull connection they can complete tasks on our behalf like answering emails, etc.

Third Quiz:
1.Vibe coding is using AI to help users with no coding experience to build a code with a specific purpose.
2. Thinking, Framework,  Checkpoints, Debugging and  Context
3. Lovable

Fourth Quiz:
1.
Implement AI in workflows and business processes
Vibe coding w/ help of AI
Build AI Agents
2. Personalized, lower cost, always available

### 7. @Againtogaia (üëç 1)
I appreciate the pneumonics. As someone who gets distracted easily, they remind me to do all of the steps. Thank you for explaining this in a way people like me can use it more efficiently

### 8. @annayeung5587 (üëç 4)
Thank you so so so much for this wonderful and informative video! You are a great teacher!

### 9. @isalutfi (üëç 11)
This is what I need to watch. Thank yo so much Tina for talking about essential AI skills.

### 10. @DrCedrickInTech (üëç 11)
This is the AI crash course executives didn‚Äôt know they needed. Love how you balance tech fluency with strategic clarity. In 2025, it's not just about knowing what AI can do, it's about knowing how to apply it meaningfully across roles and functions.

> **@timyan4692** (üëç 0): üòäüòäüòäüòäüòäüòäüòä

### 11. @danielolvera6225 (üëç 1)
This is the best teacher I‚Äôve seen so far. Holy crap. Great work girl.

### 12. @ChristianLira-o2d (üëç 0)
Hi Tina, 
I am thrilled to watch your productions which are real, insightful, and knowledge-able on deep AI. Thank you for putting these together.

### 13. @rav7727 (üëç 0)
Honestly I could listen to you talk and explain things all day, thank you so much for all your effort <3

### 14. @AilulMubarokah (üëç 18)
I'm from Indonesia and no one here teaches anything like this, thanks miss 

> **@maccomplex** (üëç 3): you can change that.

### 15. @Seriously-w3h (üëç 12)
Tina's first full flex teacher ever  in AI era‚ù§‚ù§

### 16. @jjones7837 (üëç 1)
Thanks Tina! Just found your channel and you're awesome. Blessings for all you're doing.

### 17. @sandro-nigris (üëç 2)
Thanks Tina: you are so knowledgeable and bring real value!

### 18. @–ü–æ—Ä—Ñ–∏—Ä—ñ–π–ë—É—Ç—Ä–∏–º—á—É–∫ (üëç 0)
I love how your video is easy to understand for me

### 19. @nakara4086 (üëç 0)
Excellent job, so smooth and fun absolutely easy to understand! Thank you

### 20. @kyrojohn (üëç 0)
YT hidden gem here, the whole channel really <3

