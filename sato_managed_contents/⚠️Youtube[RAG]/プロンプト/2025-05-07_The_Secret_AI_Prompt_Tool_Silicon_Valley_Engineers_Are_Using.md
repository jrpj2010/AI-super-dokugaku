# The Secret AI Prompt Tool Silicon Valley Engineers Are Using

**„ÉÅ„É£„É≥„Éç„É´:** The Next Wave - AI and the Future of Technology
**ÂÖ¨ÈñãÊó•:** 2025-05-06
**URL:** https://www.youtube.com/watch?v=M9LeemqrwGI

## Ë™¨Êòé

*Want Matt's favorite Coding AI tools? Get em' here:* üîó https://clickhubspot.com/13c950

Episode 57: Can simply "Vibe coding" with AI really replace the need for deep code context when building real applications?

Nathan Lands (https://x.com/NathanLands) is joined by Eric Provencher (https://x.com/pvncher), founder of Repo Prompt and an XR engineer at Unity, to reveal the secret AI prompt tool quietly powering Silicon Valley‚Äôs top engineers.
This episode dives deep into why the current trend of "Vibe coding" with tools like Cursor often falls apart for complex tasks ‚Äî and how Repo Prompt closes the gap by letting you build effective, highly targeted context for AI coding. Eric breaks down the philosophy behind contextual prompting, gives a live demo, and shares how Repo Prompt‚Äôs unique features like the context builder and codemaps give power-users real control over LLMs like Gemini and Claude. Beyond coding, they discuss implications for the future of engineering, learning, and the evolution of dev tools in the age of AI.

Check out The Next Wave Podcast if you‚Äôre on the go and want to listen to the show: https://lnk.to/thenextwaveyt

‚Äî
Show Notes:
(00:00) Vibe Coding Myths Unveiled
(03:15) Repo Navigation for Flutter Devs
(06:37) Gemini 2.5 Extends Model Context
(11:18) Automating File Rewrites with AI
(15:33) The Next AI Wave
(20:58) MCP: User-Customizable Tool Integration
(23:53) Efficient AI Tool Integration
(28:32) XR Interaction Toolkit Developer
(31:01) AI's Impact on Coding Learning

‚Äî
Mentions:
Eric Provencher: https://www.linkedin.com/in/provencher/
Repo Prompt: https://repoprompt.com/
Unity: https://unity.com/ai
Cursor: https://www.cursor.com/en
Gemini: https://gemini.google.com/
Claude: https://claude.ai/

Get the guide to build your own Custom GPT: https://clickhubspot.com/tnw

Subscribe to The Next Wave:
üîó https://clickhubspot.com/847e

üìô FREE Certification Courses
Digital Marketing Certification:
üëâ https://clickhubspot.com/od6
Social Media Marketing Course:
üëâ https://clickhubspot.com/Social-Media-Certification
SEO Training Course:
üëâ https://clickhubspot.com/SEO-Training-Course
Email Marketing Course:
üëâ https://clickhubspot.com/Email-Marketing-Certification

Find The Next Wave on your favorite podcast platform
üéôÔ∏è https://lnk.to/thenextwaveyt

Join Matt Wolfe and Nathan Lands, as they democratize the expertise often reserved for the boardrooms of the biggest corporations. From groundbreaking technologies to practical applications, Matt and Nathan will cover everything you need to stay informed and prepared. Whether you're seeking to adapt your company to the AI era or simply curious about the future, this podcast will equip you with the knowledge to thrive in the forthcoming wave of change.

More From Matt
üîó Future Tools https://futuretools.beehiiv.com/
üîó Blog https://www.mattwolfe.com/
üîó YouTube https://www.youtube.com/@mreflow

More From Nathan
üîó Newsletter https://news.lore.com/
üîó Blog https://lore.com/

The Next Wave is a HubSpot Original Podcast // Brought to you by The HubSpot Podcast Network // Production by Darren Clarke // Editing by Ezra Bakker Trupiano

#TheNextWave #Podcast

## Â≠óÂπï

[00:01 - 00:05]
Welcome to the Next Wave podcast. I'm

[00:03 - 00:07]
your host, Nathan Lans, and today we

[00:05 - 00:08]
have a killer episode. Everyone's

[00:07 - 00:10]
talking about vibe coding, but the

[00:08 - 00:12]
reality is for most things, vibe coding

[00:10 - 00:14]
doesn't work right now. And even the guy

[00:12 - 00:16]
who coined the term, Andre Carpathi, he

[00:14 - 00:18]
recently posted that he's now trying to

[00:16 - 00:20]
provide more context to models cuz he's

[00:18 - 00:21]
realized that's what you have to do to

[00:20 - 00:23]
get good results back. The reality is if

[00:21 - 00:24]
you want to get the most out of AI

[00:23 - 00:27]
coding, you've got to be showing the

[00:24 - 00:28]
models exactly the context they need.

[00:27 - 00:31]
Today I'm going to show you the secret

[00:28 - 00:32]
weapon that all the top AI coders are

[00:31 - 00:34]
using that you probably haven't even

[00:32 - 00:36]
heard of yet. Today I've got the founder

[00:34 - 00:38]
of Repoprompt, Eric Provener, on here

[00:36 - 00:40]
and he's going to show you exactly how

[00:38 - 00:41]
it works and how you can use Repomprompt

[00:40 - 00:44]
to take your AI coding to the next

[00:41 - 00:46]
level. So let's just jump right

[00:44 - 00:47]
in. Yeah. I've been telling people about

[00:46 - 00:49]
Reprompt for like the last, you know

[00:47 - 00:50]
probably 6 months or so. I've kind of

[00:49 - 00:53]
felt like it's been almost like my like

[00:50 - 00:55]
AI coding secret weapon. Yeah. Yeah.

[00:53 - 00:57]
Yeah, I mean I I really want to bring

[00:55 - 00:58]
you on after I saw that tweet from Andre

[00:57 - 01:01]
Carpathi the other day. He used to be a

[00:58 - 01:03]
Tesla AI now. He's like one of the best

[01:01 - 01:04]
educators about how LLMs work and things

[01:03 - 01:06]
like that. He had this uh this tweet

[01:04 - 01:10]
saying, "Noticing myself adopting a

[01:06 - 01:11]
certain rhythm in AI assisted coding. I

[01:10 - 01:13]
code I actually and professionally care

[01:11 - 01:15]
about contrast to vibe code." You know

[01:13 - 01:17]
he coined the term vibe code which

[01:15 - 01:18]
everyone's been using. And he he then he

[01:17 - 01:20]
basically goes on to talk about like

[01:18 - 01:21]
stuffing everything relevant into

[01:20 - 01:23]
context. All this I was like he doesn't

[01:21 - 01:26]
know about repop. I'm like, how did this

[01:23 - 01:28]
like top AI educator in the world, top

[01:26 - 01:30]
expert, everything totally has no idea

[01:28 - 01:32]
about reprompt? I was like, okay, so I

[01:30 - 01:33]
need to get Eric on the podcast at least

[01:32 - 01:34]
try to help with that. Yeah, I

[01:33 - 01:36]
appreciate that. Yeah, I mean, yeah

[01:34 - 01:37]
looking at that that tweet, you see

[01:36 - 01:39]
exactly like that that flow that like

[01:37 - 01:40]
got me started. Like you when when you

[01:39 - 01:42]
start getting serious about coding with

[01:40 - 01:43]
AI, like you start thinking like, okay

[01:42 - 01:45]
well, how do I get the information to

[01:43 - 01:46]
the AI model and like the UX on all

[01:45 - 01:48]
these other tools is just not cutting it

[01:46 - 01:49]
for it. Like you need a tool to just be

[01:48 - 01:50]
able to quickly select, search for your

[01:49 - 01:51]
files, like find things. There's

[01:50 - 01:53]
probably some people listening right now

[01:51 - 01:55]
like, "Oh, it's okay. Cool. Nathan's

[01:53 - 01:57]
excited. What is it?" So, like maybe

[01:55 - 01:58]
you know, if you could explain like try

[01:57 - 02:00]
to simplify it and I think we should

[01:58 - 02:02]
then just jump into a demo and we can

[02:00 - 02:03]
kind of just thing. Sure thing. Yeah.

[02:02 - 02:04]
It'll probably be easier if I just show

[02:03 - 02:05]
the screen here. I mean, the first thing

[02:04 - 02:07]
you're going to do when you're you're

[02:05 - 02:08]
going to open up repo prompt is pick a

[02:07 - 02:10]
folder. So, this here is like the entire

[02:08 - 02:11]
folder. But generally, when you're

[02:10 - 02:13]
working in a repo like this, you want to

[02:11 - 02:14]
think through like what are the files

[02:13 - 02:15]
that are are going through. And if

[02:14 - 02:17]
you're using a coding agent like with

[02:15 - 02:18]
cursor or whatever, what the first thing

[02:17 - 02:20]
they're going to do when you ask a

[02:18 - 02:22]
question is, okay, well, let me go find

[02:20 - 02:24]
what the user is trying to do. Let me

[02:22 - 02:27]
search for files and pick those out. And

[02:24 - 02:28]
with repo prompt, that's like on you as

[02:27 - 02:30]
a user to to do that work. There are

[02:28 - 02:32]
tools to help with it. So yeah, like

[02:30 - 02:34]
I'll just show uh here with the website.

[02:32 - 02:35]
This is my my other project. This is

[02:34 - 02:37]
this is the website for repo prompt

[02:35 - 02:39]
site. I was just iterating on some

[02:37 - 02:43]
pricing. I just lowered the annual price

[02:39 - 02:45]
there. And uh yeah, so help me update

[02:43 - 02:49]
all the docs pages. So if I do that and

[02:45 - 02:50]
then I just do gemini flash quickly to

[02:49 - 02:52]
show what that looks like. So it will

[02:50 - 02:54]
actually search for for files using an

[02:52 - 02:56]
LLM based on the prompt that you've

[02:54 - 02:58]
typed out. So like you know a big part

[02:56 - 02:59]
of using repo prompt is that you have to

[02:58 - 03:01]
know you know what it is that you're

[02:59 - 03:03]
trying to select here, right? And you

[03:01 - 03:04]
know what I noticed a lot of users they

[03:03 - 03:05]
were just putting everything in. They

[03:04 - 03:07]
would just say like okay just select all

[03:05 - 03:09]
and and that would be it. And you'd be

[03:07 - 03:11]
like okay well I was doing it at first.

[03:09 - 03:12]
Yeah. Yeah. I mean I mean yeah I mean

[03:11 - 03:14]
that's the easy thing to do. You're like

[03:12 - 03:15]
okay well there's the codebase. Perfect.

[03:14 - 03:16]
But you know there's plenty of tools

[03:15 - 03:17]
that can just zip up your codebase and

[03:16 - 03:19]
that's that's easy. But like the power

[03:17 - 03:21]
prompt is you can be selective. You

[03:19 - 03:22]
don't have to select everything. So I

[03:21 - 03:24]
can just hit replace here and then okay

[03:22 - 03:25]
well what did that do? Okay. Well that

[03:24 - 03:27]
actually found all these files here that

[03:25 - 03:29]
are related to my query. Put them in

[03:27 - 03:31]
order of like priority of like

[03:29 - 03:32]
importance based on what the LM judgment

[03:31 - 03:33]
is. And of course if you use Gemini

[03:32 - 03:35]
Flash you're not going to get the best

[03:33 - 03:37]
results compared to like using you know

[03:35 - 03:39]
like a bigger model like Gemini 2.5 Pro.

[03:37 - 03:40]
it'll be a lot more thorough, but you

[03:39 - 03:42]
know, like that that's the thing. So

[03:40 - 03:44]
it'll it'll pick those out. It'll use

[03:42 - 03:45]
something called code maps to help with

[03:44 - 03:47]
that. You can see that the actual token

[03:45 - 03:50]
file selection queries is just 6k

[03:47 - 03:51]
tokens. If I were to switch over to the

[03:50 - 03:53]
repo prompt codebase, which is a Swift

[03:51 - 03:56]
codebase, you can see that the code the

[03:53 - 03:58]
the codebase is a lot larger, you know

[03:56 - 04:00]
so the map is 85,000 tokens. So, I I'll

[03:58 - 04:01]
explain a little bit like what is a map

[04:00 - 04:02]
really? Like what what do you do with

[04:01 - 04:05]
that and and how does this help and what

[04:02 - 04:07]
does it do? working with a codebase, you

[04:05 - 04:08]
know, if if you've uh spent some time

[04:07 - 04:10]
you know, programming in the past. I

[04:08 - 04:11]
know a lot of folks are are getting into

[04:10 - 04:12]
just using AI and they're they're

[04:11 - 04:14]
they're not super familiar with all the

[04:12 - 04:16]
technicals there, but like vibe coding

[04:14 - 04:17]
and Yeah. Exactly. Exactly. Well, you

[04:16 - 04:18]
know, there's a lot of files and you

[04:17 - 04:20]
want to like try and get an

[04:18 - 04:21]
understanding of the files. So, Rever

[04:20 - 04:23]
has this code map feature and what this

[04:21 - 04:26]
will do is it will basically as you add

[04:23 - 04:27]
files, it'll index them and extract

[04:26 - 04:30]
what's called like it's it's a map, but

[04:27 - 04:32]
so it's like a highle extracted view

[04:30 - 04:34]
like an index of your codebase. Exactly.

[04:32 - 04:35]
Yeah. And this works for a bunch of

[04:34 - 04:37]
languages, Swift being one of them. And

[04:35 - 04:38]
if I were to select everything and kind

[04:37 - 04:41]
of show you, you know, what that looks

[04:38 - 04:42]
like. So these are the the map files. So

[04:41 - 04:44]
I'll open that up. I'll open VS Code

[04:42 - 04:46]
just to show you. So you've got like all

[04:44 - 04:48]
the the file definitions and uh you

[04:46 - 04:50]
know, just just the you what's private

[04:48 - 04:51]
what's what's public, and you can scroll

[04:50 - 04:53]
through and and so you give this

[04:51 - 04:56]
information to the AI model, and it's

[04:53 - 04:57]
able to be very selective. Uh well, it's

[04:56 - 04:59]
able to have a lot more information in a

[04:57 - 05:02]
lot less context. And so the context

[04:59 - 05:03]
builder uses that data to help you find

[05:02 - 05:04]
what what the relevant files are based

[05:03 - 05:06]
on your query. So it has like a kind of

[05:04 - 05:08]
peak inside the files without having all

[05:06 - 05:10]
of the details and it's able to kind of

[05:08 - 05:11]
surface that relevant information for

[05:10 - 05:13]
you so that you can use that in a

[05:11 - 05:14]
prompt. So yeah, I've I've gone and I've

[05:13 - 05:16]
explained a little bit about like all

[05:14 - 05:17]
this ways of picking files and stuff

[05:16 - 05:18]
like what what do you do with that, you

[05:17 - 05:20]
know, right? Like that's like the the

[05:18 - 05:21]
big question, right? I think it's also

[05:20 - 05:23]
good to explain like why it's important

[05:21 - 05:24]
because I I think one thing like one

[05:23 - 05:26]
thing I love about repo prompts so when

[05:24 - 05:28]
I first started using it I had been like

[05:26 - 05:30]
using just like a custom script I had

[05:28 - 05:31]
created to like take my codebase and

[05:30 - 05:33]
like and then like put you know the

[05:31 - 05:34]
relevant you know context in there which

[05:33 - 05:36]
a lot of times I was just doing all of

[05:34 - 05:38]
it. I was literally putting all into a

[05:36 - 05:40]
single file and I' I'd copy and paste

[05:38 - 05:42]
that into chat GBT when pro when it

[05:40 - 05:44]
first came out and I think I tweet about

[05:42 - 05:45]
this someone and someone told me like oh

[05:44 - 05:47]
you got to try repo prompt. So when I

[05:45 - 05:49]
tried repo prompt, the fact that I could

[05:47 - 05:52]
like see how much context I was sharing

[05:49 - 05:53]
with the model was amazing. And and it

[05:52 - 05:54]
seems like that's super relevant too

[05:53 - 05:56]
because you know at least from the

[05:54 - 05:58]
benchmarks I've seen you know everyone

[05:56 - 05:59]
talking about how much context you can

[05:58 - 06:01]
put into their LLM and now you're

[05:59 - 06:03]
getting like 1 million and I think Llama

[06:01 - 06:05]
was four or something crazy high like

[06:03 - 06:08]
that. I think it's 1 million. Oh no it

[06:05 - 06:09]
goes to 10 I think. Llama. Oh yeah 10.

[06:08 - 06:10]
But I think on the benchmarks though

[06:09 - 06:13]
right? I think on the benchmarks for

[06:10 - 06:15]
Llama 4, as soon as you went over like

[06:13 - 06:17]
128k context, like nowhere near the 10

[06:15 - 06:20]
million, like the quality just like

[06:17 - 06:22]
dropped like like a rock. Yeah. Well

[06:20 - 06:24]
you know, like until Gemini 2.5 came

[06:22 - 06:26]
out, like pretty much all the models and

[06:24 - 06:28]
I would say even including like 01 Pro

[06:26 - 06:30]
like you would really want to stay below

[06:28 - 06:31]
32k tokens in general. I just I find

[06:30 - 06:33]
like over that you're you're you're

[06:31 - 06:34]
you're just losing a lot of

[06:33 - 06:36]
intelligence. Like so there's this

[06:34 - 06:38]
concept of like effective context, you

[06:36 - 06:40]
know, the effective context window. Like

[06:38 - 06:42]
at what point does the intelligence stop

[06:40 - 06:43]
being like as relevant for that model

[06:42 - 06:44]
and for a lot of smaller models and

[06:43 - 06:46]
local models, it's a lot lower and you

[06:44 - 06:48]
probably want to stay around 8K tokens.

[06:46 - 06:50]
But like for for bigger models, 32K is a

[06:48 - 06:51]
good number. And it's only now with

[06:50 - 06:53]
Gemini that you're able to kind of use

[06:51 - 06:55]
the full the full package, the full

[06:53 - 06:57]
context window. And 03 is quite good too

[06:55 - 06:59]
on these benchmarks too for context

[06:57 - 07:01]
utilization. But but yeah, so you're

[06:59 - 07:02]
using this context, you've picked out

[07:01 - 07:04]
your files. Say you you want to use as

[07:02 - 07:05]
many as you want 100K like what do you

[07:04 - 07:07]
do with that? Uh and so I have a

[07:05 - 07:09]
question here. I'm just going to paste

[07:07 - 07:10]
it to 03 and you'll see like what what

[07:09 - 07:12]
is 03 getting out of this out of this?

[07:10 - 07:13]
So it's getting basically this file

[07:12 - 07:16]
tree. So it's getting a directory

[07:13 - 07:19]
structure of of this uh of this project.

[07:16 - 07:20]
It's getting basically the highle code

[07:19 - 07:22]
maps of the of the files that I haven't

[07:20 - 07:23]
selected. So basically when it's set to

[07:22 - 07:25]
complete everything that I haven't

[07:23 - 07:26]
selected gets kind of shipped in and

[07:25 - 07:27]
then you have the files that I did

[07:26 - 07:29]
select and and so then the context is

[07:27 - 07:31]
able to to go ahead and all right there

[07:29 - 07:34]
you Oh, so 03 gave like a really nice

[07:31 - 07:36]
response here with tables. Loves tables

[07:34 - 07:37]
on how to do that. And so this is like a

[07:36 - 07:39]
great way to kind of just get this

[07:37 - 07:41]
information into 03, get the most out of

[07:39 - 07:42]
this model. And 03 is an expensive

[07:41 - 07:44]
model. If you're trying to use it a lot

[07:42 - 07:46]
like this is a great way to kind of get

[07:44 - 07:48]
more value out of it, move fast, and and

[07:46 - 07:50]
get good responses. Hey, if you take a

[07:48 - 07:53]
look at my web presence online, it's

[07:50 - 07:55]
safe to say that I'm a bit AI obsessed.

[07:53 - 07:57]
I even have a podcast all about AI that

[07:55 - 07:59]
you're watching right now. I've gone

[07:57 - 08:01]
down multiple rabbit holes with AI and

[07:59 - 08:05]
done countless hours of research on the

[08:01 - 08:06]
newest AI tools every single week. Well

[08:05 - 08:08]
I've done it again and I just dropped my

[08:06 - 08:10]
list of my favorite AI tools. I've done

[08:08 - 08:13]
all the research on what's been working

[08:10 - 08:14]
for me, my favorite use cases, and more.

[08:13 - 08:17]
So, if you want to steal my favorite

[08:14 - 08:19]
tools and use them for yourself, now you

[08:17 - 08:20]
can. You can get it at the link in the

[08:19 - 08:22]
description below. Now, back to the

[08:20 - 08:24]
show. When I demoed this to Matt like

[08:22 - 08:25]
many months back, the thing that like

[08:24 - 08:27]
blew him away. He was like, "It can do

[08:25 - 08:29]
that." Was just showing him that like

[08:27 - 08:30]
you're you're putting all that context

[08:29 - 08:32]
together and then you're giving it

[08:30 - 08:35]
instructions and now you got one button

[08:32 - 08:36]
to copy that and then you go to your LLM

[08:35 - 08:39]
and then you paste that and you're

[08:36 - 08:41]
pasting so much information and I think

[08:39 - 08:42]
the average person doesn't like people

[08:41 - 08:44]
who are just using chat GPT or even

[08:42 - 08:45]
people who are coding with cursor like

[08:44 - 08:47]
people are just testing it out they

[08:45 - 08:48]
don't realize that you can do that that

[08:47 - 08:50]
you can literally just copy and paste

[08:48 - 08:52]
all of that context in there and that

[08:50 - 08:55]
the LM gets that and it understands what

[08:52 - 08:57]
to do. So I've just gone ahead and and

[08:55 - 08:59]
I've got Claude open. So Claude, you

[08:57 - 09:00]
know, in contrast to Chad PT, Claude is

[08:59 - 09:02]
very good at following instructions.

[09:00 - 09:04]
Like it's the best model at following

[09:02 - 09:06]
instructions, I find. And and so I've

[09:04 - 09:08]
gone ahead and just asked me help change

[09:06 - 09:10]
the dark mode color scheme and I pasted

[09:08 - 09:12]
that in to Claude like this with this

[09:10 - 09:14]
XML copy button. And I think this is

[09:12 - 09:16]
another thing that that like repop does

[09:14 - 09:18]
quite well is that it's got tools to

[09:16 - 09:20]
kind of send information into the LM

[09:18 - 09:22]
but it's also got tools to to go ahead

[09:20 - 09:24]
and write an XML plan and and it's going

[09:22 - 09:26]
to create this theme selector and it's

[09:24 - 09:28]
going to add these files and and change

[09:26 - 09:29]
files for me. And what's cool with this

[09:28 - 09:31]
is that I can just go ahead and use

[09:29 - 09:34]
Claude with my subscription and then

[09:31 - 09:35]
have it modify all these files. So, it's

[09:34 - 09:37]
based creating all these files and it

[09:35 - 09:39]
can search and replace parts of files

[09:37 - 09:40]
too. So I don't have to re-update and

[09:39 - 09:42]
re-upload the the whole thing, have it

[09:40 - 09:44]
output the complete code. So a lot of

[09:42 - 09:45]
models struggle with, you know, people

[09:44 - 09:46]
noticing like, oh, this model's really

[09:45 - 09:48]
lazy. It's not giving me the whole code.

[09:46 - 09:49]
But like this kind of circumvents that

[09:48 - 09:51]
issue because it lets the AI just kind

[09:49 - 09:53]
of get an escape hatch and just do what

[09:51 - 09:55]
it needs to do here. You know, LLMs are

[09:53 - 09:57]
are are beasts that seem to really like

[09:55 - 09:59]
XML. So if I were to hit copy on this

[09:57 - 10:00]
prompt here, I can open that up here.

[09:59 - 10:02]
Uh, you'll see like, oh, first of all

[10:00 - 10:04]
all of the parts of the prompt are

[10:02 - 10:06]
formatted with these XML tags. So

[10:04 - 10:08]
there's the user instructions, you've

[10:06 - 10:09]
got the file contents. If I scroll up

[10:08 - 10:10]
you know, they're all kind of separated

[10:09 - 10:12]
out. So everything's structured in like

[10:10 - 10:14]
an optimal way for the model. And you

[10:12 - 10:16]
know, when you're outputting XML, the

[10:14 - 10:18]
thing that about XML is that it's like

[10:16 - 10:20]
it's it's not like a a nested structure.

[10:18 - 10:22]
JSON is, you know, what people are used

[10:20 - 10:23]
to looking at, but it's it's something

[10:22 - 10:25]
that like models struggle to output

[10:23 - 10:27]
reliably. If you're using like chatpt

[10:25 - 10:29]
or, you know, cloud on the web, like

[10:27 - 10:31]
it's just not it's just not going to

[10:29 - 10:32]
like respect that formatting the way you

[10:31 - 10:34]
want it to. I think of it as like a

[10:32 - 10:36]
information language. It's like that AIS

[10:34 - 10:37]
can really simply understand. It's

[10:36 - 10:39]
probably the best one, right? And it's

[10:37 - 10:40]
it's easy for them to follow. So I have

[10:39 - 10:42]
like examples in my prompt on how to

[10:40 - 10:44]
output this kind of stuff. Do you do you

[10:42 - 10:46]
check everything or I try to if I if I

[10:44 - 10:47]
care about what I'm doing like that's

[10:46 - 10:49]
the you know what Karthy's saying is

[10:47 - 10:50]
like you got to you when you're when you

[10:49 - 10:52]
care about what you're coding uh it's

[10:50 - 10:53]
very important to to to go back and

[10:52 - 10:55]
forth. And you know sometimes when I'm

[10:53 - 10:57]
when I'm coding like this like I'll I'll

[10:55 - 10:59]
iterate like so I pasted this question

[10:57 - 11:01]
right uh with 03 and often what I'll do

[10:59 - 11:03]
is I'll read through the answer and then

[11:01 - 11:06]
I'll I'll change my prompt and then

[11:03 - 11:07]
paste again into a new chat and and try

[11:06 - 11:08]
and like see where the result is

[11:07 - 11:10]
different because basically I look at

[11:08 - 11:12]
like here's the output. Okay, I I

[11:10 - 11:14]
actually don't care maybe about this

[11:12 - 11:15]
copy link button. Okay, then I'll put

[11:14 - 11:17]
specifically put a mention in my prompt

[11:15 - 11:18]
to say like let's let's kind of just

[11:17 - 11:20]
focus on this part of the question and

[11:18 - 11:21]
kind of reorient it. And and that's the

[11:20 - 11:23]
nice thing with this is that I can just

[11:21 - 11:24]
hit copy as many times I want. If you're

[11:23 - 11:25]
if you're paying for a pro sub, like

[11:24 - 11:27]
there's no cost to trying things.

[11:25 - 11:29]
There's no cost to to hitting paste

[11:27 - 11:32]
again. And except for that, you know

[11:29 - 11:34]
the paste limit uh with with 03 here.

[11:32 - 11:35]
Yeah. So, you know, you just try again.

[11:34 - 11:37]
You just paste again. Let the model

[11:35 - 11:38]
think again and try things and and I

[11:37 - 11:39]
think that's that's like a really

[11:38 - 11:41]
important way of working with these

[11:39 - 11:43]
models is to experiment and try things

[11:41 - 11:45]
and and see, you know, where how how

[11:43 - 11:46]
does changing the context, what files

[11:45 - 11:48]
you have selected, your prompt. I use

[11:46 - 11:49]
these these also these stored prompts

[11:48 - 11:51]
that come built in the app. So there's

[11:49 - 11:53]
the architect and engineer and and these

[11:51 - 11:55]
these kind of help focus the model. They

[11:53 - 11:57]
give them roles. So like if I'm working

[11:55 - 11:59]
on something complicated, the architect

[11:57 - 12:02]
prompt will kind of focus the model on

[11:59 - 12:04]
on just the design and have it kind of

[12:02 - 12:05]
not think about the code itself. Uh

[12:04 - 12:07]
whereas the engineer is just the code

[12:05 - 12:08]
like don't worry about the design just

[12:07 - 12:09]
kind of give me the code. Maybe you

[12:08 - 12:11]
should explain like when you say

[12:09 - 12:13]
engineer prompt it's literally you're

[12:11 - 12:15]
just adding stuff that you copy and

[12:13 - 12:17]
paste into the LM saying like you're an

[12:15 - 12:18]
expert engineer and this is what I

[12:17 - 12:20]
expect from you. expect for you to give

[12:18 - 12:22]
me XML. That's your job. Do it. And

[12:20 - 12:24]
that's that's literally how the LM's

[12:22 - 12:25]
work. Like, okay, I'll do it.

[12:24 - 12:27]
Absolutely. Yeah. Giving them roles is

[12:25 - 12:28]
is crucial. Telling them who they are

[12:27 - 12:30]
what their what their job is, you know

[12:28 - 12:32]
what their job description, you know

[12:30 - 12:34]
what do I look for, like giving them a

[12:32 - 12:36]
performance review, evaluation, uh, all

[12:34 - 12:37]
that stuff. Like I I find like the more

[12:36 - 12:38]
detailed you are with your prompts, the

[12:37 - 12:40]
more you can help. Like they kind of

[12:38 - 12:41]
color the responses interesting way. So

[12:40 - 12:43]
just adding the engineer prop you see

[12:41 - 12:45]
like it spent more time thinking about

[12:43 - 12:46]
it but actually that's partly because

[12:45 - 12:48]
it's trying to add call tools that are

[12:46 - 12:50]
invalid but um just as a contrast to the

[12:48 - 12:52]
old response. So here this time it kind

[12:50 - 12:54]
of said okay this is the file tailwind

[12:52 - 12:55]
here's the change and this is this is

[12:54 - 12:57]
the change that I'm going to do in a

[12:55 - 12:58]
code block and so like just having it

[12:57 - 13:00]
kind of answer. So you know for the

[12:58 - 13:02]
longest time before I had any of these

[13:00 - 13:04]
XML features I was just kind of using

[13:02 - 13:06]
repo prompt and like getting these kinds

[13:04 - 13:08]
of outputs and then just copying them

[13:06 - 13:09]
back into my codebase manually and kind

[13:08 - 13:11]
of reviewing them. That was like really

[13:09 - 13:13]
the antithesis of vibe coding where

[13:11 - 13:14]
everything's kind of automated. Yeah.

[13:13 - 13:16]
So, you know, like I I think you know

[13:14 - 13:18]
the the working on repo prompt it's it's

[13:16 - 13:19]
really like there's there's building

[13:18 - 13:21]
your context. That's like the biggest

[13:19 - 13:22]
thing. Just picking what you want. You

[13:21 - 13:25]
want to frontload that work and and you

[13:22 - 13:26]
know in contrast to using agents you're

[13:25 - 13:28]
going to have those agents kind of run

[13:26 - 13:30]
off, do a lot of work, call a bunch of

[13:28 - 13:31]
tool calls. You see like 03 kind of

[13:30 - 13:33]
thought for 15 seconds, thought through

[13:31 - 13:35]
some tools to call. Um it didn't really

[13:33 - 13:37]
make sense. It just kind of kept going

[13:35 - 13:38]
and and ended up doing this. And if

[13:37 - 13:40]
you've used, you know, cursor a lot, you

[13:38 - 13:41]
know, you'll see like often using 03

[13:40 - 13:43]
it'll call tools. It'll like read this

[13:41 - 13:44]
file, read that file, read this file.

[13:43 - 13:47]
But if you just give it the files up

[13:44 - 13:49]
front and and you just kind of send it

[13:47 - 13:50]
off to work with your prompts, you right

[13:49 - 13:51]
away you get a response and you're like

[13:50 - 13:54]
okay, well, does this make sense to me?

[13:51 - 13:55]
Am I able to use this? Um, instead of a

[13:54 - 13:56]
little bit more work, at least right

[13:55 - 13:58]
now, but it's yeah, I think you get a

[13:56 - 14:00]
lot better results. So it's Yeah. Yeah.

[13:58 - 14:01]
Yeah, just frontloading that context

[14:00 - 14:03]
being able to to think through and

[14:01 - 14:05]
iterate on that and and that's the whole

[14:03 - 14:07]
philosophy around it is just like

[14:05 - 14:08]
thinking through like making this easy.

[14:07 - 14:10]
The context builder helps you find that

[14:08 - 14:12]
context. Uh you know eventually I'm

[14:10 - 14:14]
going to add uh MCP support so you can

[14:12 - 14:16]
query documentation find find things

[14:14 - 14:18]
related to to to your query as well and

[14:16 - 14:21]
just spend time as an engineer sitting

[14:18 - 14:23]
through what do I want the LLM to know

[14:21 - 14:24]
and then what do I want it to do and

[14:23 - 14:26]
then make that flow as quick and as

[14:24 - 14:27]
painless as possible and like that's

[14:26 - 14:28]
kind of everything and I think you know

[14:27 - 14:29]
going forward and you know as you get

[14:28 - 14:32]
serious coding with AI like that's

[14:29 - 14:34]
that's what the human's job is in this

[14:32 - 14:35]
loop this engineer's job is figuring out

[14:34 - 14:37]
the context. I think that's that's the

[14:35 - 14:39]
new software engineering job. I'm just

[14:37 - 14:40]
like I said before, I'm just so

[14:39 - 14:42]
surprised that a lot of people haven't

[14:40 - 14:43]
talked about this cuz like for me like

[14:42 - 14:45]
right now cursor is good for like

[14:43 - 14:47]
something very simple like okay change

[14:45 - 14:49]
some buttons or change some links or

[14:47 - 14:51]
change whatever you know but anything

[14:49 - 14:54]
complicated repo prompt I got like way

[14:51 - 14:56]
way better results. So I I'm curious

[14:54 - 14:58]
like you know have you ever thought

[14:56 - 14:59]
about like this being used for things

[14:58 - 15:01]
outside of coding and do you think it

[14:59 - 15:02]
would be useful for anything outside of

[15:01 - 15:04]
coding? I I think of like a in a

[15:02 - 15:06]
corporate context of they have all this

[15:04 - 15:07]
information they're trying to use like

[15:06 - 15:09]
let's say it's for um you know and

[15:07 - 15:10]
there's different divisions of a company

[15:09 - 15:12]
like let's say one's like a marketing

[15:10 - 15:13]
division uh it feels like there's

[15:12 - 15:15]
probably like you know instead of like

[15:13 - 15:17]
just trying to feed all the company's

[15:15 - 15:18]
information to the LLM probably they

[15:17 - 15:20]
should have like the relevant like okay

[15:18 - 15:21]
for this kind of content what is my

[15:20 - 15:23]
relevant marketing guidelines brand

[15:21 - 15:25]
guidelines other information that's

[15:23 - 15:27]
useful to the LM to tell it like writing

[15:25 - 15:28]
style and things like that. Yeah. I mean

[15:27 - 15:30]
I've I've gotten academics reach out to

[15:28 - 15:31]
me tell me they're using it for their

[15:30 - 15:33]
work. Uh there's folks in different

[15:31 - 15:34]
fields for sure. Like in general, like

[15:33 - 15:37]
you know, if you're working with plain

[15:34 - 15:39]
text files, you know, repop can service

[15:37 - 15:41]
those use cases for sure. Like it's it's

[15:39 - 15:43]
all set up to to kind of read any kind

[15:41 - 15:44]
of file and then apply edits to any kind

[15:43 - 15:46]
of file too. Like I don't differentiate

[15:44 - 15:48]
you know, distinguish or anything like

[15:46 - 15:50]
if it's if I can read it, then I'll I'll

[15:48 - 15:51]
I'll apply edits for you. And and and

[15:50 - 15:53]
yeah, like I think a whole bunch of work

[15:51 - 15:55]
is around just like gathering context

[15:53 - 15:57]
and kind of iterating on stuff like even

[15:55 - 15:59]
you know in in doing legal work. I I do

[15:57 - 16:00]
think though like there is, you know, a

[15:59 - 16:02]
flow that is still missing from this

[16:00 - 16:04]
app. It's just that like kind of

[16:02 - 16:05]
collaborative nature. I think there's

[16:04 - 16:07]
still some some work that needs to kind

[16:05 - 16:09]
of be done to kind of make this a more

[16:07 - 16:10]
collaborative tool, make this a tool

[16:09 - 16:11]
that that kind of syncs a little bit

[16:10 - 16:13]
better with different things like for

[16:11 - 16:14]
now like developers use Git and like

[16:13 - 16:16]
that's that kind of collaboration

[16:14 - 16:18]
bedrock. Yeah. I mean, yeah, that's

[16:16 - 16:20]
something I think too is like yeah, repo

[16:18 - 16:21]
prompts super useful but you kind of you

[16:20 - 16:23]
have to be a little bit more advanced

[16:21 - 16:26]
like than the average vibe coder, the

[16:23 - 16:28]
average person using an LLM. And uh I'm

[16:26 - 16:30]
kind of curious like why did you not go

[16:28 - 16:31]
the VC route or like and where's Rainbow

[16:30 - 16:33]
Prompt at right now? Like what is your

[16:31 - 16:35]
kind of you know where is it now and

[16:33 - 16:37]
what's your plan for it? Yeah. I mean I

[16:35 - 16:39]
you know I I've had a lot of folks you

[16:37 - 16:40]
know kind of you know bring that up to

[16:39 - 16:42]
me and they're kind of thinking through

[16:40 - 16:45]
like you know why not VC or whatever.

[16:42 - 16:46]
And I think like I mean it's it's not

[16:45 - 16:48]
something that the door's closed on

[16:46 - 16:51]
forever. It's just I think right now

[16:48 - 16:53]
it's it's working. I'm able to build and

[16:51 - 16:55]
you know I I'm able to to kind of listen

[16:53 - 16:57]
to my users and you know pay attention

[16:55 - 17:00]
to what they need. And I think you know

[16:57 - 17:02]
it's it's it's just not super clear to

[17:00 - 17:04]
me like where where this where this all

[17:02 - 17:06]
goes you know like this is an app that

[17:04 - 17:08]
is like super useful and it's it's like

[17:06 - 17:10]
helping me and I'm I'm able to build it.

[17:08 - 17:12]
But like is it is it something that

[17:10 - 17:14]
necessarily makes sense to like have

[17:12 - 17:16]
like you know $100 million invested into

[17:14 - 17:17]
it to grow a huge team to like build

[17:16 - 17:19]
maybe. I I don't know. So right now

[17:17 - 17:20]
you're still working at Unity. Is that

[17:19 - 17:22]
right? So you're like working at Unity

[17:20 - 17:24]
and then building this at night while

[17:22 - 17:26]
like having a family. So I was in

[17:24 - 17:28]
paternity leave for a huge part of the

[17:26 - 17:30]
last year. Um and that's kind of like a

[17:28 - 17:33]
lot of the time I was able to build this

[17:30 - 17:34]
out on. So I so you know raising a child

[17:33 - 17:36]
you know that's that's a lot of work

[17:34 - 17:38]
too. So yeah a lot of it right now is

[17:36 - 17:39]
still just like you know nights and

[17:38 - 17:41]
weekends spending a lot of time on it.

[17:39 - 17:43]
You know I feel like everything with AI

[17:41 - 17:45]
right now is like who knows what's going

[17:43 - 17:47]
to happen like yeah in a year everything

[17:45 - 17:50]
could be different in 5 years.

[17:47 - 17:52]
who the who the hell knows right and in

[17:50 - 17:54]
that kind of situation um I think

[17:52 - 17:57]
there's a lot of risk like right now

[17:54 - 17:58]
because AI is such a big wave that's why

[17:57 - 18:00]
we call the show the next wave right

[17:58 - 18:02]
it's such a large wave of transformation

[18:00 - 18:05]
happening that you are going to see the

[18:02 - 18:07]
largest investments ever I think in

[18:05 - 18:09]
history as well as the largest

[18:07 - 18:10]
acquisitions ever and I think these are

[18:09 - 18:12]
have yet to come like people think oh

[18:10 - 18:14]
it's already these have yet to come

[18:12 - 18:17]
we're like like in the early part of

[18:14 - 18:19]
this this transition and so with

[18:17 - 18:21]
in that situation I I I think you

[18:19 - 18:23]
definitely should consider it because

[18:21 - 18:26]
there's going to be so much money thrown

[18:23 - 18:28]
at the competitors and I think people

[18:26 - 18:32]
like Kerser Windsurf maybe Windsurf's

[18:28 - 18:34]
getting bought by OpenAI possibly uh for

[18:32 - 18:36]
crazy amount of money. Um they'll

[18:34 - 18:37]
probably try to integrate Wind they'll

[18:36 - 18:39]
try to probably integrate repo prompt

[18:37 - 18:41]
type features into their products at

[18:39 - 18:42]
some point. I would assume the best two

[18:41 - 18:45]
routes for you in my opinion would be

[18:42 - 18:47]
either to go really big and go the VC

[18:45 - 18:48]
route or to go more like, hey, who knows

[18:47 - 18:50]
what's going to happen with it. I just

[18:48 - 18:51]
want to like get my name out there and I

[18:50 - 18:53]
can leverage my name for something else

[18:51 - 18:55]
in the future and like open source it.

[18:53 - 18:57]
That's like my my kind of thought on

[18:55 - 18:59]
strategically what I would do is like

[18:57 - 19:00]
either go really big or open source it

[18:59 - 19:02]
and make it free and just put it out

[19:00 - 19:04]
there and say, you know, and get some

[19:02 - 19:06]
reputation benefit from it. Yeah. I

[19:04 - 19:07]
mean, well, there there is a free tier.

[19:06 - 19:09]
It's not open source. Um, but but there

[19:07 - 19:10]
is a free tier. You know, the thing the

[19:09 - 19:11]
thing about open source actually is

[19:10 - 19:14]
something I've thought about a lot and

[19:11 - 19:15]
and the the big issue with it right now

[19:14 - 19:18]
especially as people are are building AI

[19:15 - 19:20]
tools is that like it's never been

[19:18 - 19:22]
easier to fork a project and kind of go

[19:20 - 19:23]
off and just build it as a competitor.

[19:22 - 19:25]
Like if you That's what I'm saying

[19:23 - 19:26]
though. Like like I think that you're

[19:25 - 19:27]
like kind of getting up on like the

[19:26 - 19:29]
monetary side of it. You're literally

[19:27 - 19:31]
just making the reputation like hoping

[19:29 - 19:32]
to get a reputational benefit from it

[19:31 - 19:34]
versus monetary. But even that

[19:32 - 19:36]
reputational side like like if you if

[19:34 - 19:37]
you've looked at Klein like Klein's a

[19:36 - 19:39]
big tool you know that that that came

[19:37 - 19:41]
around actually started around a similar

[19:39 - 19:42]
time as me working in repop. Um and if

[19:41 - 19:44]
you're not familiar the folks listening

[19:42 - 19:47]
on the call like Klein is a is a is a AI

[19:44 - 19:48]
agent that sits in VS Code and and and

[19:47 - 19:50]
it's pretty cool but the thing that is

[19:48 - 19:52]
not so cool about it is that it eats

[19:50 - 19:54]
your tokens for lunch. Like that thing

[19:52 - 19:55]
will churn through your your your wallet

[19:54 - 19:58]
like faster than any other tool that

[19:55 - 20:00]
exists just cuz it goes off and reads

[19:58 - 20:02]
files stuffs the context as big as

[20:00 - 20:03]
possible. You know, if if like cursor is

[20:02 - 20:05]
trying to like be conservative with with

[20:03 - 20:07]
what it serves the model to save on

[20:05 - 20:09]
their inference costs, like client's the

[20:07 - 20:11]
complete opposite and will maximize your

[20:09 - 20:13]
costs as much as possible. Sounds like

[20:11 - 20:14]
an open source project. Yeah. Yeah. But

[20:13 - 20:16]
it's really powerful. It's very

[20:14 - 20:17]
powerful. It does a lot of good. A lot

[20:16 - 20:19]
of people really enjoy using it because

[20:17 - 20:20]
it has good results for certain things.

[20:19 - 20:21]
But yeah, that cost is very high. And

[20:20 - 20:23]
but the thing that I was trying to bring

[20:21 - 20:25]
up with this is that like so client was

[20:23 - 20:26]
actually forked a few months ago by by

[20:25 - 20:28]
another team of developers and it's

[20:26 - 20:30]
called brew. Brew is the the uh the

[20:28 - 20:31]
alternative. And a lot of folks, you

[20:30 - 20:32]
know, and if you look at open router and

[20:31 - 20:34]
some stats, like RU is actually

[20:32 - 20:36]
surpassing Klein. And so the the you

[20:34 - 20:38]
know, that fork is is now overtaken the

[20:36 - 20:39]
original. And you know, that's the kind

[20:38 - 20:40]
of space that we're in where like

[20:39 - 20:42]
different teams will kind of take your

[20:40 - 20:43]
code, take it in their direction, and

[20:42 - 20:45]
then all of a sudden they've overtaken

[20:43 - 20:46]
you and you know, you kind of lose track

[20:45 - 20:49]
of, you know, where things are going

[20:46 - 20:50]
there. So like it's it's it's a crazy

[20:49 - 20:52]
space and and it's never been easier to

[20:50 - 20:54]
open pull requests with AI. You don't

[20:52 - 20:55]
you don't understand you don't need to

[20:54 - 20:56]
understand the code. You're like, "Oh, I

[20:55 - 20:58]
have this open source project. I'm just

[20:56 - 21:00]
going to fork it and add my features and

[20:58 - 21:01]
kind of go and and you know and like

[21:00 - 21:03]
it's a tricky thing but like you know

[21:01 - 21:05]
having a free version and and kind of

[21:03 - 21:07]
trying to ship and and try and grow a

[21:05 - 21:09]
community of users who are passionate

[21:07 - 21:10]
who like can talk back to you and and

[21:09 - 21:12]
you know I mean that's kind of the route

[21:10 - 21:13]
I've taken right now and it's it's it's

[21:12 - 21:15]
kind of been working so far. I was in

[21:13 - 21:17]
beta for a long time but yeah you know

[21:15 - 21:18]
leaving beta I you know it's it's like

[21:17 - 21:20]
you know it's still new figuring out

[21:18 - 21:21]
where to go next with and and it's Mac

[21:20 - 21:23]
only right now. Is that is that correct?

[21:21 - 21:25]
Yeah that's that's true. it is only Mac

[21:23 - 21:26]
is Mac only and and I think a part of

[21:25 - 21:28]
that is is that I started off you know

[21:26 - 21:29]
just kind of trying to think about like

[21:28 - 21:30]
you know how do I build this in a good

[21:29 - 21:32]
way like I started with Electron

[21:30 - 21:34]
actually building this out and and and

[21:32 - 21:35]
the problem is like I immediately ran

[21:34 - 21:38]
into issues trying to build for

[21:35 - 21:40]
different platforms and like I spent a

[21:38 - 21:41]
bunch of time debugging just getting SVG

[21:40 - 21:43]
icon rendering you know all these little

[21:41 - 21:44]
things that are just like rabbit holes

[21:43 - 21:46]
and you're like okay well you're so

[21:44 - 21:47]
abstracted from the base of like what's

[21:46 - 21:50]
happening and you spend a lot of time

[21:47 - 21:51]
just solving build issues that it's like

[21:50 - 21:53]
I'm just going to go ahead and do build

[21:51 - 21:55]
native and just just run with it and and

[21:53 - 21:57]
have better performance doing so. Like

[21:55 - 21:59]
you know, if you open an IDE like VS

[21:57 - 22:01]
Code, you you you you open up like a

[21:59 - 22:04]
huge repo. What actually happens is is

[22:01 - 22:05]
it'll load the file tree and it will it

[22:04 - 22:07]
will just kind of lazy load everything.

[22:05 - 22:08]
Like not everything needs to load

[22:07 - 22:10]
because if you're opening an IDE, you

[22:08 - 22:11]
know, as a coder, traditionally, you

[22:10 - 22:14]
only have a couple files open at a time.

[22:11 - 22:15]
Maybe maybe you have a dozen, but like

[22:14 - 22:17]
that's that's at the most like you're

[22:15 - 22:20]
not you're not going to be processing

[22:17 - 22:21]
50,000 files at the same time. But an AI

[22:20 - 22:24]
model can you know if you give it to

[22:21 - 22:25]
Gemini like an Gemini will will want all

[22:24 - 22:27]
those files. It will want as much as you

[22:25 - 22:29]
can give it because it can read all of

[22:27 - 22:30]
it. And and so like you need a tool that

[22:29 - 22:33]
is built different that is kind of

[22:30 - 22:35]
organized in a way where it's kind of

[22:33 - 22:38]
thinking first through that performance

[22:35 - 22:40]
of mass data processing um that you need

[22:38 - 22:41]
to kind of do. So yeah, it's it's it's a

[22:40 - 22:43]
whole different way. That's why that's

[22:41 - 22:45]
why it's native because like I I want

[22:43 - 22:46]
that performance processing all these

[22:45 - 22:49]
files. there's all this concurrency

[22:46 - 22:50]
happening where you're like in parallel

[22:49 - 22:52]
editing these files like processing them

[22:50 - 22:54]
and doing all this stuff like you know

[22:52 - 22:56]
it's it's very hard to do if you're just

[22:54 - 22:57]
you using JavaScript or TypeScript.

[22:56 - 22:59]
Yeah, I assume so. I had thought about

[22:57 - 23:00]
that like when I use repo prompt it

[22:59 - 23:02]
seems like you've done a really great

[23:00 - 23:04]
job of building it. It works really well

[23:02 - 23:06]
and it it is all just you like right

[23:04 - 23:08]
now. Yeah, it is just me. Yeah, I've

[23:06 - 23:09]
been working on it a lot. That's crazy.

[23:08 - 23:11]
That's crazy. Yeah, it's it's come a

[23:09 - 23:12]
long way. I iterated a lot on it, you

[23:11 - 23:14]
know, but like you know the first

[23:12 - 23:15]
version was super jank. It's just, you

[23:14 - 23:17]
know, I But that but that's the power of

[23:15 - 23:18]
dog fooding, too. Like if you're not

[23:17 - 23:20]
feel like folks listening, dog fooding

[23:18 - 23:21]
is when you like kind of use your own

[23:20 - 23:24]
product to iterate on it and build with

[23:21 - 23:26]
it and you kind of like make it make it

[23:24 - 23:27]
a habit of making sure that you're a

[23:26 - 23:28]
number one user of your app, you know

[23:27 - 23:30]
your own product to make sure that you

[23:28 - 23:32]
see all the stuff that sucks about it.

[23:30 - 23:35]
And for the longest time, like, you

[23:32 - 23:37]
know, it's really sucked and and and

[23:35 - 23:39]
just that struggle and that that pain of

[23:37 - 23:40]
of using it and forcing yourself to to

[23:39 - 23:42]
feel that pain, like that's what makes

[23:40 - 23:43]
it good. That's that's where you're able

[23:42 - 23:45]
to kind of feel feel those things that

[23:43 - 23:46]
the other the users using the app will

[23:45 - 23:48]
feel and and that's when you end up with

[23:46 - 23:50]
something that that is great in the end.

[23:48 - 23:52]
So where do you think reput prompt is

[23:50 - 23:54]
going like long term? It it's weird you

[23:52 - 23:55]
know like in December like OpenAI

[23:54 - 23:57]
announces 03 and they're like oh it

[23:55 - 23:59]
beats all the ARC AGI tests and you're

[23:57 - 24:01]
like well is this AGI like what is this

[23:59 - 24:02]
like? And then and then it shifts and

[24:01 - 24:05]
it's like okay I mean like it's a better

[24:02 - 24:07]
model. It it lies to you. It's it's it's

[24:05 - 24:09]
not like the messiah you know. So, so

[24:07 - 24:11]
it's hard to say like I I don't I don't

[24:09 - 24:13]
know like where we go. Like I have ideas

[24:11 - 24:15]
on like you know using LLM so much. I

[24:13 - 24:17]
have ideas on like where the future is.

[24:15 - 24:19]
One year from now I I think like you

[24:17 - 24:21]
know I I'll have to adapt this product

[24:19 - 24:22]
and and keep iterating on it to kind of

[24:21 - 24:24]
stay relevant. So it's going to keep

[24:22 - 24:26]
changing but like I think that the flow

[24:24 - 24:28]
of kind of I'm kind of pushing towards

[24:26 - 24:30]
of that context building I think that

[24:28 - 24:32]
remains relevant for a while longer and

[24:30 - 24:34]
and what improves is the layers of

[24:32 - 24:36]
automation around that work. So I think

[24:34 - 24:38]
like long term I still think that is

[24:36 - 24:39]
kind of the vibe that I want to go

[24:38 - 24:42]
towards. Though of course like you

[24:39 - 24:44]
you'll have to you know have an agent

[24:42 - 24:45]
that kind of like does more research for

[24:44 - 24:47]
you on your behalf that will kind of run

[24:45 - 24:49]
off a little bit. Being better at doing

[24:47 - 24:51]
parallel work you know some folks using

[24:49 - 24:52]
cloud cloud code like they they have

[24:51 - 24:54]
this thing called the git work trees

[24:52 - 24:55]
that they use where basically they they

[24:54 - 24:58]
split the repo into like parallel

[24:55 - 24:59]
universes and they let like cloud code

[24:58 - 25:01]
kind of try things on these parallel

[24:59 - 25:02]
universes and iterate and go off in the

[25:01 - 25:03]
distance. That kind of workflow is going

[25:02 - 25:05]
to be important. very expensive

[25:03 - 25:07]
workflow

[25:05 - 25:09]
but it's it's one that that like I think

[25:07 - 25:11]
is is one of those ideas that you'll

[25:09 - 25:13]
kind of want to explore and go down that

[25:11 - 25:15]
rabbit hole. So, I think just like

[25:13 - 25:17]
integrating MCP, just like embracing

[25:15 - 25:18]
that like universality of all of these

[25:17 - 25:20]
different tools. So, for folks listening

[25:18 - 25:22]
if they're not sure what is MCP is

[25:20 - 25:24]
another acronym.

[25:22 - 25:27]
Yeah. Yeah. I mean so so the idea the

[25:24 - 25:28]
idea there is is like basically

[25:27 - 25:30]
traditionally if you use like claude or

[25:28 - 25:31]
open AI they have tools and those tools

[25:30 - 25:32]
you know one of them could be like

[25:31 - 25:34]
search the web or one of them could be

[25:32 - 25:36]
like read the files on your thing or

[25:34 - 25:39]
look up documentation or these kinds of

[25:36 - 25:40]
things and and there's this protocol MCP

[25:39 - 25:43]
that like creates like an abstraction

[25:40 - 25:45]
layer so that like any client app can

[25:43 - 25:48]
can implement this protocol and then

[25:45 - 25:49]
users can bring their own tools. So if a

[25:48 - 25:50]
user comes in and says like oh I want to

[25:49 - 25:52]
use and there's this new one that's

[25:50 - 25:53]
really cool it's called context 7 where

[25:52 - 25:55]
basically they've gone ahead built a

[25:53 - 25:56]
server that fetches the latest

[25:55 - 25:58]
documentation for whatever programming

[25:56 - 25:59]
language you're using and and we'll kind

[25:58 - 26:01]
of pull that in as context. So you can

[25:59 - 26:03]
say okay great fetch fetch the latest

[26:01 - 26:04]
you know angular docs or whatever docs

[26:03 - 26:05]
you you care about and then you can

[26:04 - 26:07]
bring that in. So that kind of work

[26:05 - 26:09]
where you're like doing that that that

[26:07 - 26:11]
context retrieval that's super important

[26:09 - 26:12]
or like you Stripe has one too where

[26:11 - 26:14]
basically all the docs for their their

[26:12 - 26:16]
tool is set up and you know you just

[26:14 - 26:17]
plug in the Stripe MCP and then all of a

[26:16 - 26:19]
sudden if you're trying to vibe code

[26:17 - 26:21]
your way through integrating Stripe like

[26:19 - 26:22]
that's super easy that the work is kind

[26:21 - 26:24]
of handled. You can plug in your API

[26:22 - 26:26]
keys onto it so it can even talk to the

[26:24 - 26:28]
back end for you. Uh that whole work is

[26:26 - 26:30]
kind of automated. So it's really like

[26:28 - 26:32]
it's all about like having tools for for

[26:30 - 26:33]
folks using these models to kind of

[26:32 - 26:35]
automate connecting to different

[26:33 - 26:36]
services in this like universe of all

[26:35 - 26:37]
these different you know services. Yeah.

[26:36 - 26:39]
I kind of think of it most I mean it's

[26:37 - 26:41]
different than XML but for me the way I

[26:39 - 26:42]
think of it cuz I'm like I'm a person

[26:41 - 26:45]
who's like more on the business side

[26:42 - 26:46]
slightly technical code a bit uh you

[26:45 - 26:48]
know I think of it as almost more just

[26:46 - 26:51]
how XML is like the information language

[26:48 - 26:52]
that AI can understand. The MCP is like

[26:51 - 26:54]
the same thing with any any service you

[26:52 - 26:56]
want to use or tool. It's a way that you

[26:54 - 26:58]
for the AI to know how to work with

[26:56 - 26:59]
those things uh very clearly. Yeah. And

[26:58 - 27:00]
and funny enough you mentioned XML

[26:59 - 27:02]
because that's actually one of the

[27:00 - 27:04]
things that I do a lot with reform is is

[27:02 - 27:06]
like parsing XML and like I've show I've

[27:04 - 27:08]
shown with that and I think one strength

[27:06 - 27:10]
there that I have that like a lot of

[27:08 - 27:11]
other tools are kind of ignoring. So

[27:10 - 27:12]
traditionally when you're when you're

[27:11 - 27:14]
working with these language models as a

[27:12 - 27:15]
developer uh and you say like and you

[27:14 - 27:17]
see and you can see this if you use

[27:15 - 27:19]
chattv you be like hey like search the

[27:17 - 27:21]
web it's going to use the the search

[27:19 - 27:23]
tool and you'll see it say tool search

[27:21 - 27:25]
and it'll go through. But what happens

[27:23 - 27:27]
when it's doing that is that basically

[27:25 - 27:30]
it it calls that tool, it stops, waits

[27:27 - 27:32]
for the result, and then continues. And

[27:30 - 27:33]
when it continues, it's basically like

[27:32 - 27:35]
if you think about it like in in this

[27:33 - 27:37]
weird way like how tool calling is it's

[27:35 - 27:39]
it's funny like I think of it like you

[27:37 - 27:41]
have this the the the robot is is kind

[27:39 - 27:43]
of being reboot as a new session uh with

[27:41 - 27:45]
that new context because basically every

[27:43 - 27:47]
tool call is a new query. So you're

[27:45 - 27:49]
giving back the old information but

[27:47 - 27:50]
you're not necessarily talking to that

[27:49 - 27:52]
same instance. It's like it's like a

[27:50 - 27:54]
different AI instance that is answering

[27:52 - 27:56]
your question from the new checkpoint.

[27:54 - 27:57]
So like that's like a weird thing. So

[27:56 - 27:59]
you know as you're as you're making all

[27:57 - 28:00]
of these tool calls if you if you use

[27:59 - 28:02]
cursor you know it'll make like a 100

[28:00 - 28:03]
tool calls in in in I think they have a

[28:02 - 28:05]
limit of 25 actually but by the end of

[28:03 - 28:06]
it you know you've gone through 25

[28:05 - 28:08]
different instances of these models and

[28:06 - 28:09]
then you get a result at the end and

[28:08 - 28:10]
you're like well you know it's like

[28:09 - 28:12]
weird like what actually happened you

[28:10 - 28:13]
there's some data loss like weird stuff

[28:12 - 28:14]
you know we don't know how all of this

[28:13 - 28:16]
yeah it does seem like that could create

[28:14 - 28:18]
like reliability issues right because

[28:16 - 28:19]
like you know with LM like sometimes

[28:18 - 28:22]
they give you amazing results and other

[28:19 - 28:23]
times it's like what is what is this and

[28:22 - 28:24]
so every time you're doing a new tool it

[28:23 - 28:26]
sounds like you're you're almost

[28:24 - 28:28]
recreating that the chance of it going

[28:26 - 28:30]
wrong in a way. Exactly. Yeah. You're

[28:28 - 28:31]
you're aggregating this issues but you

[28:30 - 28:32]
don't even know where that information

[28:31 - 28:33]
it could be different servers that are

[28:32 - 28:35]
actually processing all these different

[28:33 - 28:37]
tool calls and you know maybe you know

[28:35 - 28:39]
it's weird sometimes you'll have like oh

[28:37 - 28:40]
that server has some like chip issue on

[28:39 - 28:42]
its memory and like that actually causes

[28:40 - 28:43]
some weird issues where cloud is

[28:42 - 28:45]
actually really dumb today. Um but on

[28:43 - 28:46]
the other one it's it's a lot smarter

[28:45 - 28:48]
cuz their chip the memory chip is

[28:46 - 28:49]
working fine you know you don't know. So

[28:48 - 28:51]
that kind of thing. So the the way that

[28:49 - 28:53]
I've kind of gone about this is that

[28:51 - 28:55]
like the way I call tools is you you

[28:53 - 28:58]
have your XML and the AI will just

[28:55 - 28:59]
answer in one instance and and and it'll

[28:58 - 29:01]
just give you the whole thing and then I

[28:59 - 29:02]
parse it and it can call a bunch of

[29:01 - 29:03]
tools in there. It can be like hey like

[29:02 - 29:06]
I want to call this this do this and

[29:03 - 29:08]
this and then I just parse that and then

[29:06 - 29:09]
bulk call the tools and then get the

[29:08 - 29:11]
results and then we go another instance

[29:09 - 29:12]
with the results and you can kind of go

[29:11 - 29:14]
back and forth like that. So like not

[29:12 - 29:16]
have to wait on each single one. You're

[29:14 - 29:17]
actually just bulk sending them out

[29:16 - 29:19]
getting that data. It's a lot more

[29:17 - 29:21]
efficient. you're able to to process say

[29:19 - 29:22]
like 25 queries you know get read 25

[29:21 - 29:24]
we'll bring them all in you know let's

[29:22 - 29:25]
work from there and see how it goes and

[29:24 - 29:27]
so that kind of thinking so I think

[29:25 - 29:29]
there's there's a lot to to kind of play

[29:27 - 29:31]
with in in terms of you know how you're

[29:29 - 29:32]
even getting this data back and forth

[29:31 - 29:33]
from the LLM because at the end of the

[29:32 - 29:35]
day it's all text you know text and

[29:33 - 29:37]
images maybe um some video in some cases

[29:35 - 29:38]
but like really text just for your

[29:37 - 29:40]
coding like that's that's the thing that

[29:38 - 29:41]
that you're working with and you can do

[29:40 - 29:43]
a lot with text manipulating it and

[29:41 - 29:45]
playing with it to to kind of get good

[29:43 - 29:46]
route what do you think uh like I've

[29:45 - 29:48]
heard you know YC and others I think

[29:46 - 29:50]
Gary Tan said that I can't remember if

[29:48 - 29:52]
it was 80% but I think he said like 80%

[29:50 - 29:54]
of the code for the the startups going

[29:52 - 29:56]
through YC right now is AI generated

[29:54 - 29:59]
that number could be wrong like where

[29:56 - 30:01]
like do you think in 3 years from now

[29:59 - 30:02]
like like in 3 years from now do we

[30:01 - 30:04]
still have like normal engineers who

[30:02 - 30:06]
don't use AI at all is that a real thing

[30:04 - 30:08]
well first of all like I think saying a

[30:06 - 30:10]
percent like that of how much of it is

[30:08 - 30:13]
AI generated it's a bit misleading

[30:10 - 30:15]
because like I can basically like no but

[30:13 - 30:17]
like I can go ahead and like every line

[30:15 - 30:20]
of code I could basically like type it

[30:17 - 30:22]
like in pseudo code to the AI model and

[30:20 - 30:24]
like have it paste it back in uh as like

[30:22 - 30:26]
a fleshed out JavaScript function and

[30:24 - 30:28]
say 100% of my code's written by AI, you

[30:26 - 30:30]
know, and you know, if you're it really

[30:28 - 30:32]
depends on how how your workflow is

[30:30 - 30:33]
what your pipeline looks like. I do

[30:32 - 30:35]
think fundamentally the job of an

[30:33 - 30:36]
engineer has changed. It's already done.

[30:35 - 30:38]
It's already completely different. Like

[30:36 - 30:39]
you can't you can't work the same way

[30:38 - 30:41]
but it depends what point in the stack

[30:39 - 30:42]
you're working on. Like I I work for

[30:41 - 30:44]
some folks who do some like really

[30:42 - 30:46]
low-level, you know, graphics, you know

[30:44 - 30:48]
work and and I I talked to someone about

[30:46 - 30:49]
like how they they can't really use AI

[30:48 - 30:51]
models because the AI models just

[30:49 - 30:52]
hallucinates everything like they it is

[30:51 - 30:55]
just not trained on anything that they

[30:52 - 30:56]
work on. So it's just useless for them.

[30:55 - 30:57]
But then if you work on work with

[30:56 - 31:00]
someone who's a you know web developer

[30:57 - 31:01]
well 100% of the code like like 98% of

[31:00 - 31:03]
the training code is web web code and

[31:01 - 31:06]
web framework code. And so it's like

[31:03 - 31:07]
okay well yeah 100% of that work can be

[31:06 - 31:09]
done by AI. It's really easy. But I

[31:07 - 31:11]
think like as we move forward more and

[31:09 - 31:12]
more, you're going to want to have AI

[31:11 - 31:14]
models thinking through hard problems

[31:12 - 31:15]
for you because it just happens much

[31:14 - 31:17]
faster as they get better at math at

[31:15 - 31:18]
solving like you know connectivity

[31:17 - 31:21]
architecture. Like architecture is

[31:18 - 31:23]
something that like these 03 and 01 Pro

[31:21 - 31:24]
and hopefully 03 Pro is just excel at.

[31:23 - 31:27]
They're they're very good at finding

[31:24 - 31:28]
good ways of like organizing your code

[31:27 - 31:31]
and helping you plan how you connect

[31:28 - 31:33]
things together. The the job is is fully

[31:31 - 31:34]
changed. Like I think from from today on

[31:33 - 31:36]
like if you're not using these tools

[31:34 - 31:37]
you're not learning how they work. Like

[31:36 - 31:39]
I think that that's like an issue cuz

[31:37 - 31:40]
like I don't think you know a

[31:39 - 31:41]
traditional engineer who spends his

[31:40 - 31:43]
whole career just typing code up like

[31:41 - 31:45]
that doesn't exist anymore. But what

[31:43 - 31:47]
does exist is someone who understands

[31:45 - 31:48]
code and who can read it and and who

[31:47 - 31:50]
understands you know what questions to

[31:48 - 31:52]
ask. And if you're if you're prompting

[31:50 - 31:54]
like you know about the code if you're

[31:52 - 31:56]
if you if you understand you know the

[31:54 - 31:57]
connections that that's where you're

[31:56 - 31:58]
going to get the best results. And

[31:57 - 31:59]
that's why like a tool like ruber prompt

[31:58 - 32:01]
is so helpful because you're able to do

[31:59 - 32:03]
that and feed the right context in. But

[32:01 - 32:05]
if you're just saying like make the

[32:03 - 32:07]
button blue or like move it over here, I

[32:05 - 32:08]
mean that works for to some extent. You

[32:07 - 32:09]
know, if as long as your your your

[32:08 - 32:11]
instructions are simple enough and you

[32:09 - 32:12]
know what you want, you can get there.

[32:11 - 32:14]
But like at a certain point, you fall

[32:12 - 32:16]
off and and and you know, that's when it

[32:14 - 32:17]
stops working. And and maybe that point

[32:16 - 32:19]
where you fall off gets further and

[32:17 - 32:20]
further as the models improve. But I

[32:19 - 32:22]
don't think that like in the next 10

[32:20 - 32:24]
years, we get to a point where that

[32:22 - 32:25]
point stops existing. One thing that we

[32:24 - 32:27]
didn't talk about that I was kind of

[32:25 - 32:29]
curious to talk about was like what what

[32:27 - 32:32]
do you do at Unity? like and and like a

[32:29 - 32:33]
crazy backstory like so I used to be

[32:32 - 32:35]
involved in the game industry and I used

[32:33 - 32:37]
to see David Helguson the creator of uh

[32:35 - 32:40]
you know Unity around parties all the

[32:37 - 32:41]
time. He's super nice, super nice guy.

[32:40 - 32:43]
Uh like I had some kind of like image in

[32:41 - 32:46]
my mind of him was like this uh

[32:43 - 32:47]
eccentric uh you know uh European

[32:46 - 32:49]
billionaire who had like the popped up

[32:47 - 32:51]
cuffs and he'd always like you know uh

[32:49 - 32:53]
he'd go around with like loud techno

[32:51 - 32:56]
music with his you know the roof down

[32:53 - 32:57]
and uh it's crazy how big Unity is now.

[32:56 - 32:59]
Yeah. Yeah. I mean, so what I do there

[32:57 - 33:02]
is like I so I've been I've been doing

[32:59 - 33:03]
uh kind of XR XR research and XR

[33:02 - 33:05]
engineering and uh so I work on a

[33:03 - 33:08]
toolkit called the XR interaction

[33:05 - 33:10]
toolkit and and basically what what that

[33:08 - 33:12]
is for is it's a it's a framework for

[33:10 - 33:13]
developers to to build interactions with

[33:12 - 33:16]
XR. If you're if you're putting on an

[33:13 - 33:19]
Oculus Quest or your a hollow lens or or

[33:16 - 33:20]
you know like uh whatever you know Apple

[33:19 - 33:22]
Vision Pro you want to basically

[33:20 - 33:23]
interact with objects in your in your

[33:22 - 33:25]
scene in your world you know like in in

[33:23 - 33:27]
AR if you're walking up and you want to

[33:25 - 33:28]
pick up a virtual cube like how do you

[33:27 - 33:30]
how do you process that interaction of

[33:28 - 33:31]
you grabbing the cube and picking it up

[33:30 - 33:32]
and looking at it so that's like I've

[33:31 - 33:35]
done a lot of research on that that

[33:32 - 33:37]
interaction of like playing input hand

[33:35 - 33:39]
like I I've written specs that are

[33:37 - 33:41]
adopted for for like the industry in

[33:39 - 33:42]
terms of hand interaction. So like you

[33:41 - 33:43]
know just tracking your hands. How do

[33:42 - 33:44]
you how do you grasp something? What

[33:43 - 33:46]
what should you be doing if you want to

[33:44 - 33:47]
poke a button that's like not there?

[33:46 - 33:48]
Like what does that look like? Uh so

[33:47 - 33:49]
that kind of stuff. That's that's what I

[33:48 - 33:51]
do there. That's amazing. That's like

[33:49 - 33:53]
that's a really complicated engineering

[33:51 - 33:54]
work. I'm I'm constantly this actually

[33:53 - 33:56]
one reason I even like you know helped

[33:54 - 33:58]
start this podcast was I'm constantly

[33:56 - 34:00]
thinking about where AI is going and

[33:58 - 34:01]
wanting to stay ahead and also think

[34:00 - 34:04]
about what does it mean for me and my

[34:01 - 34:05]
family like with your child. Uh you know

[34:04 - 34:07]
have you thought about that yet? Like

[34:05 - 34:09]
what course what what do you think they

[34:07 - 34:12]
should learn and like and and how I have

[34:09 - 34:15]
no idea. Okay.

[34:12 - 34:16]
Okay. This this everyone right like like

[34:15 - 34:17]
what do you even teach your children?

[34:16 - 34:19]
Like is it is it important to learn to

[34:17 - 34:21]
code? Is it do we teach them logic

[34:19 - 34:24]
morals probably all of this and more and

[34:21 - 34:26]
being flexible and super fluid and Yeah.

[34:24 - 34:28]
Yeah. You know it but it is funny on

[34:26 - 34:30]
that topic like I I look at engineers

[34:28 - 34:32]
coming out and learning to code with AI

[34:30 - 34:33]
around and I think they're at a

[34:32 - 34:35]
disadvantage. You know, it's it's

[34:33 - 34:36]
unfortunate that like, you know, if

[34:35 - 34:39]
you're starting to code today and you

[34:36 - 34:40]
have AI to lean on, you just don't have

[34:39 - 34:41]
that struggle. You just don't have the

[34:40 - 34:43]
pain that like I had to go through when

[34:41 - 34:44]
I started to code when, you know, when

[34:43 - 34:46]
engineers who've been in the field for

[34:44 - 34:48]
so long that had to struggle and and and

[34:46 - 34:50]
and not get the the dopamine hit of a

[34:48 - 34:52]
fixed problem right away, like to to

[34:50 - 34:54]
study it and understand how it works

[34:52 - 34:55]
like that just doesn't exist anymore

[34:54 - 34:57]
because the AI just solves it for you.

[34:55 - 34:58]
And I think that's true in code, but

[34:57 - 35:00]
it's going to be more and more true in

[34:58 - 35:02]
every field. And so I think like you

[35:00 - 35:04]
know if you know if you want to really

[35:02 - 35:06]
get into something I think like there's

[35:04 - 35:07]
going to be a need for people to have

[35:06 - 35:09]
the restraint to kind of put aside these

[35:07 - 35:11]
tools to struggle a little bit. I think

[35:09 - 35:13]
there's a ton of value in kind of using

[35:11 - 35:15]
them to learn and grow. But there's also

[35:13 - 35:17]
like that restraint that you need to

[35:15 - 35:19]
form to kind of have the struggle

[35:17 - 35:21]
because that's where the learning is and

[35:19 - 35:22]
and it's it's really tricky and I and I

[35:21 - 35:24]
don't know how you you solve that now

[35:22 - 35:25]
because it's it's too easy not to

[35:24 - 35:28]
struggle now which which is a big

[35:25 - 35:30]
problem. Yeah. I've heard uh Jonathan

[35:28 - 35:33]
Blow I don't if you know know of him the

[35:30 - 35:35]
game designer he he talks about exactly

[35:33 - 35:37]
what you're saying that you know it's in

[35:35 - 35:38]
the future like yeah sure AI could get

[35:37 - 35:40]
amazing at coding in the future but it's

[35:38 - 35:41]
also going to create an issue where like

[35:40 - 35:43]
just like you said people are not going

[35:41 - 35:46]
to learn to properly code he was already

[35:43 - 35:48]
complaining about before AI the code was

[35:46 - 35:50]
shitty and then now with AI it's like

[35:48 - 35:51]
okay now we're kind of screwed I guess

[35:50 - 35:53]
because like we're in a situation where

[35:51 - 35:54]
like no one knows what's going on and

[35:53 - 35:58]
like you're you're entirely dependent on

[35:54 - 36:00]
the AI for for everything. Yeah. I But

[35:58 - 36:01]
that that's the thing like I think maybe

[36:00 - 36:02]
that's the middle part, you know, where

[36:01 - 36:04]
where we're at this point where it's

[36:02 - 36:05]
like the AI is just not quite good

[36:04 - 36:07]
enough to kind of solve all the problems

[36:05 - 36:08]
and you still have problems to solve and

[36:07 - 36:09]
you still have people that need to kind

[36:08 - 36:11]
of work with the machines to kind of

[36:09 - 36:13]
figure out how to go. Maybe at some

[36:11 - 36:15]
point in the future it all of it is moot

[36:13 - 36:17]
and I I know some folks think that and

[36:15 - 36:19]
and maybe it doesn't matter. But like I

[36:17 - 36:20]
think you know there's going to be some

[36:19 - 36:22]
discomfort in the middle where where you

[36:20 - 36:23]
know the machines are not quite good

[36:22 - 36:25]
enough to solve every problem. we lean

[36:23 - 36:27]
on them as if they are and then, you

[36:25 - 36:28]
know, we're kind of atrophying a lot of

[36:27 - 36:30]
skills that we we've heard. I' i've he

[36:28 - 36:31]
folks, you know, I haven't driven in in

[36:30 - 36:33]
a Tesla with FSD, but I've heard folks

[36:31 - 36:34]
say the same thing there where like if

[36:33 - 36:36]
they're using it all the time, they

[36:34 - 36:38]
actually like suck at driving without

[36:36 - 36:39]
it. And it's like, you know, like more

[36:38 - 36:41]
and more that's going to kind of be a

[36:39 - 36:42]
thing where where like, you know, that's

[36:41 - 36:44]
that that is the thing where we start to

[36:42 - 36:45]
go like to like where like almost living

[36:44 - 36:47]
in like one of those sci-fi novels

[36:45 - 36:48]
right? Like everything being super super

[36:47 - 36:49]
safe. You know, I live in Japan.

[36:48 - 36:51]
Everything, you know, I used to live in

[36:49 - 36:52]
San Francisco. Everything's super safe

[36:51 - 36:55]
in Japan. There's one reason I like it

[36:52 - 36:58]
but you do lose some some freedom in in

[36:55 - 37:00]
that. Uh, so Eric, it's been awesome.

[36:58 - 37:02]
And, uh, maybe we should, you know, tell

[37:00 - 37:04]
people where they can find you and, uh

[37:02 - 37:07]
where they can find repo prompt and

[37:04 - 37:11]
Yeah. So, so I'm, uh, you know, puncher

[37:07 - 37:12]
with a V on X. So, it's like PVN C on X.

[37:11 - 37:14]
Uh, and most most social, my handle all

[37:12 - 37:16]
over. Um, so you can reach out there. My

[37:14 - 37:18]
DMs are open if you have questions. And

[37:16 - 37:21]
repop.com. So, you can just head over

[37:18 - 37:23]
there and, uh, find the app. free to

[37:21 - 37:24]
download and uh nice Discord community

[37:23 - 37:26]
too. If you want to hop over there and

[37:24 - 37:28]
send me some messages and tell me what

[37:26 - 37:29]
you think like please do. Cool. I think

[37:28 - 37:31]
I'm on there too which I'm not very

[37:29 - 37:32]
active but yeah. No, it's all good.

[37:31 - 37:33]
Yeah. Well, thank thanks for having me

[37:32 - 37:36]
on Nathan. It's been great chatting with

[37:33 - 37:39]
you and it's been great. Yeah. Yeah.

[37:36 - 37:42]
Appreciate it.

[37:39 - 37:42]
[Music]

[37:47 - 37:54]
[Music]

## „Ç≥„É°„É≥„Éà

### 1. @TheNextWavePod (üëç 3)
*Want Matt's favorite Coding AI tools? Get em' here:* üîó https://clickhubspot.com/d0b5b5

> **@IgorGanapolsky** (üëç 0): There can only be one: WindSurf. Period

> **@MarceloLopezJr** (üëç 0): Where are these tools again?

All I got was a marketing download PDF. No list of tools. No tips on using said tools.

17 pages of marketing.

What did I miss?

> **@tj6512** (üëç 0): Agreed - just checked - where are the tools ? Kindly advise. Thanks

### 2. @tomasprieto9746 (üëç 45)
Misleading title, you can absolutely set the context in Cursor, better support, longer bake time in use by pros. Be honest you‚Äôll get a better response.

> **@pvncher** (üëç 8): Cursor is definitely a great tool, and I don‚Äôt want to convince people not use it, but while they do provide controls for selecting context, their prompt building process is opaque and you don‚Äôt know what the model actually knows about by the time their agent sends a request.

You can evaluate this by seeing every time an agent makes a tool call to read a file that‚Äôs already been manually assigned as context.

Repo Prompt gives you all the prompts, and lets you even export them to use with any chat app. 
There‚Äôs no hidden optimization step, which cursor does to save costs.

> **@brinkoo7** (üëç 0): I agree 100%. Repo prompt is a handy cool tool which I have trialed for a month, but the title and the opening are very misleading. My favorite part of RepoPrompt was copying the codebase to clipboard, but there is an open source cli tool called code2prompt that covers this.

> **@pvncher** (üëç 0): @ if all you‚Äôre doing is copying the entire codebase then sure, but I think if that‚Äôs all you used the app for you were missing out on a lot of what if could do for you. 

Not to mention that if your repo grows beyond the scale of a hobby project you have to start being selective about what files to include.

I‚Äôm not sure when you tried it, but the biggest strengths are codemaps and the context builder for honing in on relevant context. 

Then for applying edits there‚Äôs a powerful parsing and diff engine that can let you apply responses from Claude or ChatGPT straight back into file changes.

> **@unom8** (üëç 0): ‚Äã@@pvncherI used cursor a bit back when it was a plugin, now using github copilot, and it feels like most of what is mentioned is already baked in, you can explicitly mark files or folders to use for context, it may not have presets for arch vs eng - but you could just have separate chats set up.
if codemaps are just local indexing then copilot has that as well.
Am I missing something?

> **@pvncher** (üëç 0): ‚Å†@@unom8Yes you can tag files and folders as context in those apps, but you don‚Äôt know what will actually get included in your prompts when they‚Äôre sent to the llm. Cursor especially has some sort of prompt prioritization system that truncates parts of your context to fit into their target context window. Also, have you tried manually tagging a dozen files in cursor or vscode? The ux is quite poor, whereas here it‚Äôs trivial, and you can then take the prompt and context either into the chat, or externally to any external one.

Regarding codemaps, it‚Äôs not just indexing your files - that‚Äôs the file tree which is a trivial feature to implement. Codemaps extract the definitions of your classes, methods and functions, and for strongly typed languages, it will even auto detect references in your files and auto include maps to those files to give the llm a working understanding of the files referenced in your selected files.

Repo Prompt also uses codemaps with the context builder, to automatically find relevant files based on a prompt, which is a very handy feature. It also has very advanced file editing capabilities   , for rapidly and precisely editing many files at once. You can for example paste a prompt into ai studio, and it will output a formatted xml response that you can paste back into the app explaining exactly what to change in all required files, even letting you make complex refactors.

Give this a watch when you get the chance
https://youtu.be/yQBoPfYqta4

### 3. @wmarple (üëç 33)
EDIT:  I'm a senior dev who uses Cursor everyday, and I do think this project has value.  It's just TBD whether it has value for me personally (it may).  I'd like to apologize and revise my initial comment.  As a dev myself, I can certainly empathize and appreciate the amount of sweat equity that the developer has placed into this project.  I DO think the product has potential.  For me, I just need to see if it is an improvement over what I'm already doing to keep context size down and get models to focus on the right things in my codebase.  

My original review was negative, but that was no fault of the developer.  I abhor click bait "silicon valley secret weapon" sales pitches.  It's one thing to use it as a click bait title, it's another to sell it that way in the video.  Products that are driving real value (which I think this is) don't need slimy sales pitches.

> **@pvncher** (üëç 3): In the dev, and this secret thing isn‚Äôt my sales pitch - what I‚Äôm offering is a scalpel for building prompts from your project context, and what I believe is the most precise set of tools for editing large files in bulk.

I‚Äôm assuming you imagine context windows will just keep growing to encompass entire codebases; but even if that happens, the cost will be such that you‚Äôll always want to build with precision, especially as models continue to scale at the frontier.

Before deciding that this has nothing to offer you, why not give it a try? It has a high learning curve, and I‚Äôm working to address that, but I genuinely believe that you can get far better results with it than with cursor today.

> **@wmarple** (üëç 0): @@pvncher I will try anything once.  I'd be down to pair with you sometime, and you can show me how this thing can improve my current workflow.  I'm currently using a workflow using task master ai for my planning phase.  That looks like creating a PRD (markdown instructions), then taskmaster uses claude to create a tasks json, as well as individual task text files.  This has all of the task specific context including specific files, and seeks to do some of what your tool is doing.  If RepoPrompt can do this better, or perhaps help me create better PRD/task files more quickly, I could see it being useful.  The ONLY thing that is a struggle at times, is that if I'm working in a workspace (laravel backend and next.js frontend for example), the model will get confused and needs to be redirected to the correct root.  Claude 3.7 got good at mitigating this, but as I've played with Gemini 2.5 pro I've come to realize they're not equal.  I have also spent a little time trying RepoMix to see if it can help it stay on track.  It's helped a little, but isn't a silver bullet to be sure.  Anyway, hit me up if you want to show me the ropes talk@willmarple.com

### 4. @chris7c0 (üëç 1)
The ideas behind Repo Prompt are amazing.  I will love to try if it comes to Linux or Windows.

### 5. @bennyflint (üëç 0)
I‚Äôve written scripts to do some of this (which is nice for things like CI), but I can‚Äôt wait to try it out. I, for some reason, like the back and forth of a web-based chat. This should work perfectly.

### 6. @dashmasterful (üëç 0)
So basically cursor but with extra steps? I'm trying to understand why this tool is better than Windsurf or Cursor

### 7. @cptechno (üëç 1)
Excellent product!   I haven't downloaded yet, but waiting for the Linux or Windows version to be available.

### 8. @mikestaub (üëç 1)
It's a great tool, but I'm sure Cursor and Windsurf with copy it soon. In the meantime I love using it!

### 9. @crstdr1 (üëç 1)
This tool makes sense to me. Started testing it today and love how it works.

### 10. @JamesWiley-r5b (üëç 1)
RepoPrompt could be used for technical writing databases, website, technical documentation, inside of codebases and repositories,branding, locale separation 
Makes me think of some of the the,Palantir products AIP and Foundry. One of the tool groups would be a good way to think of it.

### 11. @syberkitten1 (üëç 0)
My 2 cents...
Been working with LLms fir the past 2.5 years, and i stopped coding by myself almost completely, because AI does it better, but if and only if you know how to break and define the task like an Architect and understand the underlying intricacies well enough to navigate through the dumbness that is inherent unless you ground your models with the proper documentation.

It is still a hard work to articulate and define and iterate and always you have to speak out clearly and challenge yourself in your approach.   

Sometimes i felt like you guys, even more so i got anxiety and started to lose confidence only to realize that it's not practical for me to code again since I'm few times faster now with AI

Without proper tooling/workflow/documentation/high level design/examples you might waster more time then coding it yourself especially if it's not too complicated.

### 12. @universebecomingltd (üëç 2)
-Crickets- You didn't bother to think about 65% of computer users.  By the time you do, we'll be long gone. üôÇ

### 13. @DanielBurkett-n4n (üëç 1)
This Repo Prompt is not available on Linux yet, which I find surprising.

> **@pvncher** (üëç 2): Unfortunately the issue with making a native app. There are no good solutions for cross platform apps that need the full performance of the machine.

### 14. @hatonafox5170 (üëç 10)
I'm not normally negative about this kind of thing but this really is not compelling at all. There's just not enough here. It's missing too many features and if you're "advanced" enough to use this (the host's lanugage) then you can configure things like Cline or Roo to work in expanisve codebases really well. Additionally, no single dev is going to on nights and weekends put together a tool that's going to be relevant even 3 months from now. There is too much money being invested and development being allocated to making vibe coding tools better and better. These companies see a future where anyone can chat their way to a production ready app. Which means your customer base expands dramatically and so does your revenue potential. By the time this has MCP we'll have 2 or 3 other mission critical frameworks that need integrating. If this really proved game-changing to devs at windsurf or cursor they'd copy it in a single sprint. I wish the dev well obviously but your title and thumbnail really set this up to be underwhelming. I also think a lack of multi-OS support limits value as well. I almost exclusively develop on Linux these days unless I'm traveling which is when I use my Mac to get work done. Almost everything AI related just works on Linux.

> **@spreadkit** (üëç 0): Shallow. That's what 90% of content has been for a while now with all this ai thing

> **@youngop** (üëç 1): Yeah, if it‚Äôs not competing with Cline or Roo Cline features and all‚Ä¶ then I‚Äôm gonna be uninterested until it catches up.

> **@pvncher** (üëç 0): I totally get where you're coming from, but Repo Prompt does a lot of things that other tools don't, and it's a heck of lot more context efficient than cline and roo, which are very expensive to operate with your api out of pocket.

I don't disagree with you that it's a competitive space, but I've been iterating on this app for a long while and have moved fast to offer unique features that aren't standard elsewhere. I believe the codemaps in here, particularly with strongly typed langauges, is much more advanced than what other tools are doing, and the diff editing in here is a lot more reliable than cline or roo.

Use what tool works best for you, but you're judging it to be irrelevant without having spent any time with it. I do hear you on Linux support - it's unfortunately painful to port native apps, and tools outside of the mac ecosystem for that are abysmal.

> **@pvncher** (üëç 3): @@youngop Competing with cline or roo is relative. This thing can carry out large multi-file edits a lot more reliably than either of those, while using a fraction of the tokens, and costing an order of magnitude less. Yes it doesn't do all the agentic stuff those do, but for 90% of work iterating on code, all you need is the right context, and then a quick turnaround on a file edit.

### 15. @solifugus (üëç 0)
Repo Prompt is only available for Mac.  I using Linux and am not going to buy  Mac.  Cursor works for some simpler things with common languages but Anthropic console is actually pretty good.  The problem is that is eats money fast, also.  I have to say, it is worth sinking some money into.  I agree about Claude being better at following instructions.  It also generates much better code and fiction that OpenAI.

### 16. @jesseburstrom5920 (üëç 0)
I just built same with gemini 2.5 pro but it is totally web based need no installation at all works all systems directly thinking open sourcing it maybe in near future. Interesting the idea with checkbox to files (already have treeview of repomix packing) i thought yesterday is probably one simple prompt and maybe just 5-10 minutes to get in. Today little while get model selector and token plus price count. Next to  update local repo for testing and possible automation there. Great I get lots of ideas to include in my project. I have thought about actually letting AI to decide what files are important for specific prompts maybe...

### 17. @tinycircle14 (üëç 2)
I‚Äôll check it out. Curious if repoprompt paid you for any of this?

> **@pvncher** (üëç 0): I did not. Nathan has been a fan of the app for a few months and reached out to do the podcast.

### 18. @truthontech (üëç 0)
This is aaaaaamaaaaazing! Frikken well done!

### 19. @I3Programming-rh4nw (üëç 0)
Hi, I'm currently learning programming. Would it be better to use this tool (not relying on it 100%), or would it be better not to use it at all? I'd really appreciate your answer

> **@pvncher** (üëç 1): I think it's good to find a balance. This tool does keep you engaged with your codebase a lot more than other tools. I'd recommend trying to struggle with problems before letting the ai solve them for you, but do use this to feed context to ai to help you understand the problem space when you run into issues. Ask it things like - without telling me the answer, help me understand where I'm going wrong.

### 20. @bensavage6389 (üëç 1)
I'm pretty sure co-pilot released this ability to query your entire repo. And early May of 2025

> **@pvncher** (üëç 0): If you have a sufficiently large repo, no model will be able to see the whole thing, so what they do is use techniques to search for relevant files, and while they‚Äôll usually do a decent job, it‚Äôs slow and it might miss out on some.

Repo prompt tries to make that process as easy as possible, between a good ui for it, search and filtering. 

There‚Äôs also codemaps which have broad project awareness a context builder that studies the map for relevant files.

