# STOP prompt engineering, "Stop Killing Games" and STOP self-sabotage

**チャンネル:** Goju Tech Talk
**公開日:** 2025-06-28
**URL:** https://www.youtube.com/watch?v=-Nik0rQhquA

## 説明

Deep Dives: https://www.youtube.com/playlist?list=PLmcRM-yaxDR8C3_m-EDGXlo9mhAO4pZSQ

Topics:

  1 - Don't be the agent of your own destruction
  2 - Stop Killing Games
  3 - Stop killing the word engineering with "prompt engineering"

      how - Google, Microsoft, IBM, Anthropic, Amazon, Meta, X (Grok), etc. 

      what - (build this thing)

  4 - code analysis =- syntax
      code understanding =- semantics
      code comprehension =- intention
  
      meta-reasoning or temporal analysis / time dilation

  5 - Natural Languages != Programming Languages
  
--------------------------------------------------

Approach:

  Think deeply on complex problems and iterate
  
  Goju judge others less; judge himself more :)
  
  Less instant gratification
  ...
  More delayed gratification
  
--------------------------------------------------
  
Technology that annoys Goju:
  
  Poor quality code and software (machine programming can help)
  Code that is overly complex (for no good reason)

--------------------------------------------------

Song:

  Anthem of Tzeentch (Dream Clinic)

## 字幕

[00:00 - 00:05]
So, I have a couple things I want to

[00:03 - 00:09]
talk about,

[00:05 - 00:12]
but I'm real curious if any of you have

[00:09 - 00:15]
things that we should talk about. So,

[00:12 - 00:18]
would love to hear

[00:15 - 00:20]
and have us just meander.

[00:18 - 00:24]
So, if you have thoughts, let's drop

[00:20 - 00:29]
this in here. If you have things you

[00:24 - 00:35]
would like us to talk about tonight,

[00:29 - 00:37]
please drop some ideas in the chat.

[00:35 - 00:40]
Excellent.

[00:37 - 00:42]
Uh what I'm going to do

[00:40 - 00:45]
is

[00:42 - 00:48]
I'm going to list some of the things

[00:45 - 00:51]
that I was curious to talk about and

[00:48 - 00:54]
we'll go from there.

[00:51 - 00:59]
All right. Now, the first thing that I

[00:54 - 00:59]
wanted to talk about, there's a few.

[01:00 - 01:07]
Don't be the agent of your own

[01:04 - 01:07]
destruction.

[01:08 - 01:16]
That's one of the first things. Another

[01:12 - 01:16]
thing that

[01:19 - 01:24]
so I have a couple things on my mind

[01:21 - 01:25]
that I'm real I'm I'm quite curious

[01:24 - 01:30]
about but I'm trying to figure out how I

[01:25 - 01:34]
can breach these topics

[01:30 - 01:36]
and keep things on the up and up

[01:34 - 01:38]
so I don't out people because I don't

[01:36 - 01:41]
want to say anything negative about

[01:38 - 01:44]
people because this is a uh principle of

[01:41 - 01:47]
mine, but I want to talk about some

[01:44 - 01:50]
things that are going on that I'm not

[01:47 - 01:54]
super thrilled about.

[01:50 - 01:57]
So, maybe I'll just list

[01:54 - 02:01]
the second one as

[01:57 - 02:03]
stop killing games.

[02:01 - 02:07]
All right.

[02:03 - 02:07]
Stop killing games.

[02:08 - 02:15]
It's this movement. It's this stop

[02:12 - 02:18]
killing games uh movement

[02:15 - 02:20]
and

[02:18 - 02:24]
I like games where in particular we're

[02:20 - 02:25]
talking about video games I think here.

[02:24 - 02:30]
Yep.

[02:25 - 02:33]
And as someone who not only really likes

[02:30 - 02:37]
video games, I've also

[02:33 - 02:39]
created a video game that I've been

[02:37 - 02:42]
running for

[02:39 - 02:48]
25 years. So I have a little bit to

[02:42 - 02:49]
share about this. And my take may be

[02:48 - 02:52]
it may be controversial, but I mean I

[02:49 - 02:58]
don't think it is. But this is a topic

[02:52 - 03:02]
that's going around on the the intwebs

[02:58 - 03:03]
and I've seen a few

[03:02 - 03:06]
videos

[03:03 - 03:09]
from some of the people that I like

[03:06 - 03:12]
quite a bit. Asmin Gold, love you,

[03:09 - 03:15]
brother. Techone,

[03:12 - 03:18]
uh, Moist Critical, all these guys I'm a

[03:15 - 03:22]
huge fan of. And

[03:18 - 03:22]
the take that they have is

[03:23 - 03:29]
It's similarish

[03:25 - 03:31]
to to my take, which is

[03:29 - 03:34]
I think this thing that they're doing

[03:31 - 03:38]
here is

[03:34 - 03:41]
really meant to help uh gamers not lose

[03:38 - 03:45]
access to their favorite games. And

[03:41 - 03:48]
I love that.

[03:45 - 03:52]
And I'm coming at it from both sides.

[03:48 - 03:55]
So, I'm coming at it from the side of

[03:52 - 03:58]
I've created a video game and I've run

[03:55 - 04:00]
it for 25 years. I mean, it's just a

[03:58 - 04:04]
small thing, but

[04:00 - 04:06]
it's my game and I spent a lot of time

[04:04 - 04:11]
on it.

[04:06 - 04:14]
So, I have that aspect to it. But

[04:11 - 04:16]
I think I also running that game I think

[04:14 - 04:20]
about

[04:16 - 04:23]
the players and part of the reason why I

[04:20 - 04:25]
continue to run Nautica even today is

[04:23 - 04:29]
that

[04:25 - 04:32]
people still play it and

[04:29 - 04:38]
they seem to enjoy it. So why would I

[04:32 - 04:41]
take this away from them? I I I

[04:38 - 04:44]
that feels a little bit maybe

[04:41 - 04:49]
soulless or

[04:44 - 04:51]
maybe not in favor of the

[04:49 - 04:54]
players who who've dedicated a huge

[04:51 - 04:56]
portion of their their life or whatever

[04:54 - 05:00]
to these games. So, for example, in

[04:56 - 05:02]
Nica, it's Nica is this online role

[05:00 - 05:04]
playing game I created. I would bring up

[05:02 - 05:06]
the website, but the website is like

[05:04 - 05:10]
perpetually down. It has something to do

[05:06 - 05:12]
this. I I I'm I'm a bad maintainer. So,

[05:10 - 05:14]
I I haven't done a super great job.

[05:12 - 05:16]
Sorry, I'm fiddling with the mic here.

[05:14 - 05:18]
trying to my my perpetual problem with

[05:16 - 05:20]
the mic is I'm trying to figure out how

[05:18 - 05:24]
to have the mic close to me, but I can

[05:20 - 05:26]
still use my hands and and I don't I

[05:24 - 05:28]
don't bump the mic

[05:26 - 05:31]
cuz when this arm comes up, I'm I'm I

[05:28 - 05:33]
I'll figure it out by the year 2030. I'm

[05:31 - 05:35]
going to get this sorted. We're going to

[05:33 - 05:38]
try this. I'm going to go up like this a

[05:35 - 05:42]
little bit more. Maybe this gets us

[05:38 - 05:45]
there. Okay, so here's the thing.

[05:42 - 05:45]
Uh

[05:47 - 05:53]
where this gets really tricky I think

[05:50 - 05:54]
the stop killing games is in the cases

[05:53 - 05:58]
where

[05:54 - 06:00]
it's an online game and

[05:58 - 06:03]
for those games that you you like

[06:00 - 06:05]
download it and you do single player. I

[06:03 - 06:08]
don't know that there's too much of a

[06:05 - 06:11]
concern. I mean, maybe there is if you

[06:08 - 06:14]
DRM it and then you like revoke licenses

[06:11 - 06:16]
or something. And if if gaming companies

[06:14 - 06:18]
are doing that,

[06:16 - 06:21]
that's not great. Like maybe stop doing

[06:18 - 06:23]
that gaming companies. That's not the

[06:21 - 06:27]
greatest behavior.

[06:23 - 06:29]
But the challenge I think with this stop

[06:27 - 06:32]
killing games

[06:29 - 06:35]
comes into

[06:32 - 06:38]
the issue that you have when you have an

[06:35 - 06:41]
online game and you have to maintain the

[06:38 - 06:45]
online presence

[06:41 - 06:49]
even after you've reached exceeded and

[06:45 - 06:51]
are on the decline of a user base. So we

[06:49 - 06:53]
can look at this from multiple

[06:51 - 06:55]
perspectives. We can look at it from the

[06:53 - 06:56]
perspective of the player and say you

[06:55 - 06:59]
dedicated your life to this. You can

[06:56 - 07:03]
also look at this perspective of the

[06:59 - 07:07]
company that created the game and the

[07:03 - 07:11]
issues that they might uh deal with

[07:07 - 07:12]
trying to keep the online presence when

[07:11 - 07:14]
they don't necessarily like have the

[07:12 - 07:18]
budget for it. And and and again this is

[07:14 - 07:20]
complicated. I don't know exactly.

[07:18 - 07:25]
I don't know that there's a universal

[07:20 - 07:29]
answer, but what I do think is

[07:25 - 07:32]
for people that have paid money

[07:29 - 07:34]
for a game,

[07:32 - 07:39]
we should be incentivized

[07:34 - 07:42]
as game developers to support that game

[07:39 - 07:44]
as long as we possibly can. I'm not

[07:42 - 07:46]
talking about games that are free. If

[07:44 - 07:49]
it's a free-to-play game and people have

[07:46 - 07:51]
paid no money and it's just purely based

[07:49 - 07:53]
on ads or something and then you want to

[07:51 - 07:55]
shut you want to sunset that game, well,

[07:53 - 07:58]
the players haven't paid any money and

[07:55 - 08:01]
so I don't know that they necessarily

[07:58 - 08:04]
have a say. Uh your advertisers might

[08:01 - 08:07]
not be super happy, but you know, it's

[08:04 - 08:09]
uh you do you. But when the players have

[08:07 - 08:11]
paid money

[08:09 - 08:13]
and you're taking away something that

[08:11 - 08:15]
they paid money for, I think this is

[08:13 - 08:18]
where things can start to get like a

[08:15 - 08:20]
little murky.

[08:18 - 08:22]
And I know that it's this whole virtual

[08:20 - 08:27]
thing. I I understand this that NOCA is

[08:22 - 08:27]
an online game and

[08:27 - 08:33]
I suspect I may be well within my rights

[08:31 - 08:37]
to shut it down, but I don't want to do

[08:33 - 08:39]
that. like why would I do that?

[08:37 - 08:42]
First of all, as a game developer, it

[08:39 - 08:44]
was a lot of work to create that game.

[08:42 - 08:47]
Uh,

[08:44 - 08:50]
and if people enjoy the game, even if

[08:47 - 08:52]
it's not making money,

[08:50 - 08:54]
wouldn't I just want them to keep

[08:52 - 08:56]
playing? Like, that was sort of one of

[08:54 - 08:59]
the whole points of creating the game.

[08:56 - 09:01]
So obviously I think that that's part of

[08:59 - 09:04]
my agenda,

[09:01 - 09:06]
but I figured out a way to minimize the

[09:04 - 09:08]
overhead costs

[09:06 - 09:13]
of keeping the game online. And so it's

[09:08 - 09:15]
not I'm not dropping millions of dollars

[09:13 - 09:17]
uh per month

[09:15 - 09:19]
that maybe some of these big studios are

[09:17 - 09:25]
like I don't I don't know what their

[09:19 - 09:25]
parameters are and so I don't know

[09:25 - 09:30]
how to potentially course correct.

[09:28 - 09:35]
But what I do think is it's generally a

[09:30 - 09:35]
really bad idea to

[09:36 - 09:42]
try to create a business model where the

[09:38 - 09:44]
players pay money.

[09:42 - 09:46]
Even if it's just a membership thing,

[09:44 - 09:48]
like if you set set it up to where they

[09:46 - 09:51]
don't actually buy the game, they just

[09:48 - 09:52]
pay a monthly membership

[09:51 - 09:54]
and then you're like, "Well, you just

[09:52 - 09:56]
pay a membership and so we can shut it

[09:54 - 10:00]
down anytime we want." Like, you could

[09:56 - 10:02]
do that. You could, but

[10:00 - 10:06]
I don't know that you're necessarily

[10:02 - 10:09]
building all the goodwill that

[10:06 - 10:10]
maybe you should be trying to build with

[10:09 - 10:14]
the players, especially if you think

[10:10 - 10:18]
about game number two or game number

[10:14 - 10:21]
three because I think

[10:18 - 10:26]
gamers

[10:21 - 10:29]
keep track of which studios I think are

[10:26 - 10:31]
doing good things and which are not. I

[10:29 - 10:36]
won't mention some of the studio names

[10:31 - 10:39]
or the publisher names or whatever. But

[10:36 - 10:45]
I suspect if gamers watch this either

[10:39 - 10:49]
live or in the VOD, I think they'll

[10:45 - 10:54]
maybe they they can chime in and say.

[10:49 - 10:55]
So anyway, this stop killing games. I

[10:54 - 10:58]
haven't read through this entire thing.

[10:55 - 10:59]
I've only seen snippets

[10:58 - 11:02]
and

[10:59 - 11:05]
we might take some time tonight to uh

[11:02 - 11:07]
read through this, but this is becoming

[11:05 - 11:09]
this has become like a hot button topic

[11:07 - 11:12]
in the gaming community right now

[11:09 - 11:17]
because there's at least one person in

[11:12 - 11:21]
the gaming community that is sort of I'm

[11:17 - 11:24]
just trying to reach my remote here so I

[11:21 - 11:26]
can zoom in or zoom. I'm going to try to

[11:24 - 11:27]
zoom

[11:26 - 11:30]
No, I'm just going to do this. Just

[11:27 - 11:33]
point down a little bit. Okay. So,

[11:30 - 11:35]
there's at least one big

[11:33 - 11:40]
uh

[11:35 - 11:40]
online content creator,

[11:41 - 11:44]
uh,

[11:42 - 11:46]
internet

[11:44 - 11:50]
voice, internet personality, this kind

[11:46 - 11:52]
of thing that has kind of taken issue

[11:50 - 11:54]
with this.

[11:52 - 11:55]
Not going to mention names because I'm

[11:54 - 11:57]
not trying to throw anyone under the

[11:55 - 11:59]
bus.

[11:57 - 12:01]
I'll generally only try to focus

[11:59 - 12:07]
principally on the names of the people

[12:01 - 12:12]
that I'm given solid uh praise to.

[12:07 - 12:17]
And I generally uh agree with uh Asmin

[12:12 - 12:19]
Gold uh Techone and uh Moist Critical on

[12:17 - 12:22]
the the views that they have on this

[12:19 - 12:24]
which is this is inspired I think

[12:22 - 12:28]
generally

[12:24 - 12:30]
as a good thing that for gamers that pay

[12:28 - 12:33]
for games

[12:30 - 12:35]
they should be able to keep their their

[12:33 - 12:39]
goods.

[12:35 - 12:39]
I think that makes sense

[12:40 - 12:44]
and maybe there's a way forward here. I

[12:42 - 12:46]
don't actually know exactly what the way

[12:44 - 12:48]
forward is, but just simply turning off

[12:46 - 12:54]
a service,

[12:48 - 12:57]
turning off the the game because it's

[12:54 - 12:59]
costing too much for your servers,

[12:57 - 13:00]
whatever. after you've already built

[12:59 - 13:02]
this player base and you have people

[13:00 - 13:05]
that are like deeply committed to it,

[13:02 - 13:08]
this doesn't feel like it's super fair

[13:05 - 13:10]
for the consumers.

[13:08 - 13:12]
And I think we should put something in

[13:10 - 13:16]
place to

[13:12 - 13:16]
ensure that minimally

[13:17 - 13:24]
people that pay for games

[13:20 - 13:26]
can still play those games in some

[13:24 - 13:28]
capacity.

[13:26 - 13:30]
And I I again I don't know exactly all

[13:28 - 13:34]
the ramifications of this. This this is

[13:30 - 13:36]
potentially a complicated uh topic, but

[13:34 - 13:39]
in some cases it's I think it's not that

[13:36 - 13:41]
hard. Like if it's a single player game

[13:39 - 13:44]
and it has an online component like

[13:41 - 13:47]
let's take for example

[13:44 - 13:50]
uh Path of Exile, Path of Exile 2, which

[13:47 - 13:54]
is a game that uh I like

[13:50 - 13:57]
and it's a game I've played. All right.

[13:54 - 14:00]
Well, bring up uh Steam for me. And

[13:57 - 14:03]
we're talking about Path of Exile.

[14:00 - 14:05]
What is this game? It looks like I've

[14:03 - 14:07]
played this for just an hour. I'll have

[14:05 - 14:09]
to check this out. This looks like my

[14:07 - 14:12]
kind of game.

[14:09 - 14:14]
Uh so, Path of Exile.

[14:12 - 14:17]
I I haven't actually played this 600

[14:14 - 14:20]
hours. I wish I wish I had or could

[14:17 - 14:21]
have. I'm just super busy. But as I

[14:20 - 14:24]
think many of you know, what I'll often

[14:21 - 14:25]
do is I'll launch the game and then I'll

[14:24 - 14:27]
pause. I'll put it on the side and I'll

[14:25 - 14:29]
come back. By the way, I I try to do

[14:27 - 14:32]
that with Cyberpunk

[14:29 - 14:34]
and I was doing it and then it was like

[14:32 - 14:37]
jacking with my computer because with

[14:34 - 14:41]
Cyberpunk also I haven't played 410

[14:37 - 14:43]
hours but um

[14:41 - 14:47]
when you pause it, it somehow doesn't

[14:43 - 14:51]
throttle the CPU. So, cyber f cyberpunk

[14:47 - 14:54]
developers, cyberpunk folks, guys, if

[14:51 - 14:58]
people are at the pause screen, can you

[14:54 - 15:01]
please put in a frame limiter? So, it

[14:58 - 15:03]
limits the frames or it checks to see if

[15:01 - 15:06]
it's a background process and then

[15:03 - 15:08]
throttles down the CPU and GPU usage. I

[15:06 - 15:09]
know you're super smart. I know you can

[15:08 - 15:12]
do this. Almost every other game that

[15:09 - 15:14]
I've ever played does this. I don't know

[15:12 - 15:16]
why you're not doing it and I would

[15:14 - 15:18]
really like you to do it because what I

[15:16 - 15:22]
don't want to do and maybe this is by

[15:18 - 15:24]
design. I really hope it isn't. What I

[15:22 - 15:26]
don't want to do is I'm in the middle of

[15:24 - 15:29]
something but then I've got to work work

[15:26 - 15:30]
to do and then I have to shut the game

[15:29 - 15:32]
down and then I have to launch it and I

[15:30 - 15:34]
have to go through your launcher and

[15:32 - 15:36]
your launcher wants me to create an

[15:34 - 15:38]
account on your thing. It reminds me

[15:36 - 15:41]
every 10 times or something, which by

[15:38 - 15:43]
the way is also kind of annoying.

[15:41 - 15:47]
Again, I'm really enjoying the game, but

[15:43 - 15:50]
guys, I've said no 10 times that I don't

[15:47 - 15:51]
want to create an account by you

[15:50 - 15:55]
continuing to ask me. It's more just

[15:51 - 15:58]
sort of annoying. Like, just have proper

[15:55 - 16:00]
etiquette or minimally have it set up to

[15:58 - 16:01]
where it shows it and says, "I don't

[16:00 - 16:03]
want to do this." And then there can be

[16:01 - 16:07]
a box that says, "Remember this and so

[16:03 - 16:08]
don't ask me again. Uh it it's it's not

[16:07 - 16:11]
a super good look. It makes you guys

[16:08 - 16:13]
look a little desperate and you

[16:11 - 16:15]
shouldn't be cuz you're making great

[16:13 - 16:18]
games. Like your Witcher stuff I hear is

[16:15 - 16:21]
pretty good. Cyberpunk is phenomenal. So

[16:18 - 16:23]
just stop with the bottom of the barrel

[16:21 - 16:26]
uh behavior.

[16:23 - 16:28]
uh behave more like worldclass uh

[16:26 - 16:30]
developers, which you guys clearly are,

[16:28 - 16:34]
and stop trying to force us to have

[16:30 - 16:36]
accounts for I mean, look, um

[16:34 - 16:38]
I'll try to show you live.

[16:36 - 16:41]
I think it takes some number of tries

[16:38 - 16:44]
before it pops this up. Maybe we'll get

[16:41 - 16:44]
lucky.

[16:44 - 16:53]
Okay, so this time it's not showing me.

[16:48 - 16:53]
Let's actually run this experiment.

[16:53 - 17:00]
You may actually have to enter the game

[16:54 - 17:04]
to trigger it, but let's just uh

[17:00 - 17:06]
let's just launch this and see if we can

[17:04 - 17:09]
figure out

[17:06 - 17:11]
how many times it takes for it to pop

[17:09 - 17:13]
up. So, what we're doing is we're

[17:11 - 17:15]
waiting to get the screen that's going

[17:13 - 17:18]
to ask us specifically for this create

[17:15 - 17:19]
an account

[17:18 - 17:21]
because it's going to and notice that

[17:19 - 17:22]
it's going to ask you to do the sign in.

[17:21 - 17:26]
You can skip this and just go straight

[17:22 - 17:30]
to the play. Sorry, my thing's blocking

[17:26 - 17:33]
this. So you'll hopefully see it

[17:30 - 17:34]
uh the the different screen if we launch

[17:33 - 17:35]
this enough times cuz I think it's

[17:34 - 17:38]
probably just a counter in the

[17:35 - 17:40]
background and that every 10th time it

[17:38 - 17:45]
might be date driven as well. I'm not

[17:40 - 17:47]
sure but we'll see if we can trigger it.

[17:45 - 17:49]
And this is basically what I'm saying I

[17:47 - 17:51]
would like them to just stop doing. What

[17:49 - 17:53]
would be really great actually to add to

[17:51 - 17:56]
this and I think Asmin talks about this.

[17:53 - 17:58]
I would like to launch Steam and

[17:56 - 18:00]
actually hit the play button and have it

[17:58 - 18:03]
just play the game. That would actually

[18:00 - 18:06]
be super great if I could do that. A lot

[18:03 - 18:07]
of other games do that. I know that

[18:06 - 18:10]
you're trying to circumvent the Steam

[18:07 - 18:11]
ecosystem and all this stuff and you've

[18:10 - 18:14]
got your own stuff cuz now you can sell

[18:11 - 18:17]
your games and not have to pay the Steam

[18:14 - 18:22]
overhead and all this other mumbo jumbo.

[18:17 - 18:22]
But like it really feels like it's um

[18:23 - 18:27]
it's sort of like

[18:27 - 18:33]
focusing on the minutia and not focusing

[18:30 - 18:35]
enough on the higher order bits. Like

[18:33 - 18:38]
you're getting distracted, I think, by

[18:35 - 18:40]
the the small amount of table scraps and

[18:38 - 18:44]
you're not focusing on the fact that you

[18:40 - 18:46]
just got yourself like a $1,000 stake.

[18:44 - 18:49]
Just focus on the steak.

[18:46 - 18:51]
Don't focus about like all the sides

[18:49 - 18:53]
aren't exactly perfect and the stake is

[18:51 - 18:57]
that you guys created this brilliant

[18:53 - 18:59]
game and people love it and so that

[18:57 - 19:02]
should be sufficient. Like I don't I

[18:59 - 19:05]
don't know if Elden Ring is doing this.

[19:02 - 19:08]
Uh Path of Exile, I don't know, maybe

[19:05 - 19:10]
they are. I can't remember. But I know

[19:08 - 19:13]
Blizzard is doing this. And I think part

[19:10 - 19:14]
of the reason why I have leaned away

[19:13 - 19:17]
from playing Blizzard games, I used to

[19:14 - 19:20]
be a huge Diablo fan.

[19:17 - 19:22]
And I think that Blizzard kind of, you

[19:20 - 19:24]
know, I I I'll probably always love

[19:22 - 19:28]
Blizzard in some capacity. I have a

[19:24 - 19:32]
special place in my heart for them, but

[19:28 - 19:34]
I think they kind of self-sabotaged.

[19:32 - 19:36]
Uh and that goes back to this point here

[19:34 - 19:38]
is this whole don't be the agent of your

[19:36 - 19:40]
own destruction

[19:38 - 19:42]
which is sort of an interesting topic I

[19:40 - 19:44]
think in the sense that it doesn't take

[19:42 - 19:47]
much

[19:44 - 19:48]
and often times I think we sort of know

[19:47 - 19:51]
when we're being the agent of our own

[19:48 - 19:54]
destruction

[19:51 - 19:59]
and we know because there's something in

[19:54 - 20:02]
us that's saying you shouldn't do this.

[19:59 - 20:05]
this is not great behavior.

[20:02 - 20:08]
Let's maybe not do this. But then you do

[20:05 - 20:10]
it anyway and then bad things happen and

[20:08 - 20:11]
you kind of knew that bad things were

[20:10 - 20:13]
going to happen, but you thought maybe I

[20:11 - 20:18]
can get away with this and you didn't

[20:13 - 20:20]
get away with it. And now Yeah.

[20:18 - 20:24]
Okay. We'll try this a couple more times

[20:20 - 20:27]
and see if we get the popup.

[20:24 - 20:28]
But needless to say, I I have a game

[20:27 - 20:30]
launcher

[20:28 - 20:32]
uh

[20:30 - 20:35]
whatever the studio is that made this. I

[20:32 - 20:38]
I have a game launcher, guys. It's

[20:35 - 20:40]
called Steam. Steam launches my games.

[20:38 - 20:43]
And so when I hit the play button from

[20:40 - 20:46]
Steam, that means I know this is going

[20:43 - 20:50]
to be shocking. It means I want to play

[20:46 - 20:55]
this game. It means uh it doesn't mean

[20:50 - 20:57]
that I'm I want to play Witcher 4.

[20:55 - 20:59]
I mean, I I get that that's your game as

[20:57 - 21:02]
well. And on top of that, I would likely

[20:59 - 21:06]
figure this out. It also doesn't mean

[21:02 - 21:08]
that I want to buy a Switch, too. Like,

[21:06 - 21:10]
again, what exactly are we doing here?

[21:08 - 21:13]
Right? We're This is turning into this

[21:10 - 21:16]
going to be like a full ad uh type

[21:13 - 21:17]
framing thing.

[21:16 - 21:19]
And I know you can do it. Like it's

[21:17 - 21:21]
great that you have the technical

[21:19 - 21:23]
sophistication to pull this off. Like

[21:21 - 21:25]
again, if we pull this over, I think

[21:23 - 21:29]
it's set up so you can click on these

[21:25 - 21:32]
and these are some of their games. And

[21:29 - 21:35]
yeah, okay, cool. But

[21:32 - 21:39]
yeah, just I don't know. It feels like

[21:35 - 21:43]
you're trying a little too hard. And

[21:39 - 21:46]
if you focus more on building just great

[21:43 - 21:50]
games, I don't even know if you need

[21:46 - 21:52]
this because people will just go and

[21:50 - 21:56]
play your great games. And the beauty of

[21:52 - 22:00]
gamers, gamers talk to each other. I

[21:56 - 22:02]
knew that you guys created Witcher.

[22:00 - 22:05]
Where's Witcher? This one. I knew that

[22:02 - 22:08]
you Is this Witcher 2?

[22:05 - 22:10]
I don't know what this is.

[22:08 - 22:14]
This is Cyberpunk.

[22:10 - 22:14]
It's Witcher. That's Witcher.

[22:14 - 22:18]
Gwent. I I'm actually doing myself a

[22:16 - 22:19]
disservice because now I've learned

[22:18 - 22:22]
something new. I didn't know about this

[22:19 - 22:25]
this Gwent game. But here's the thing. I

[22:22 - 22:27]
knew about Witcher before I ever played

[22:25 - 22:30]
Cyberpunk. And I knew that the people

[22:27 - 22:32]
who created uh Cyberpunk also created

[22:30 - 22:34]
Witcher. How did I know? Because I

[22:32 - 22:36]
listen to the musings of the gaming

[22:34 - 22:38]
community and they talk about it. So,

[22:36 - 22:40]
you don't even really need to do this.

[22:38 - 22:44]
Is that the gamers, more likely than

[22:40 - 22:46]
not, anybody that's playing cyberpunk,

[22:44 - 22:49]
they're likely to figure out by

[22:46 - 22:54]
following the breadcrumbs that you also

[22:49 - 22:57]
created Witcher and that they'll likely

[22:54 - 22:59]
play those and find out about those. For

[22:57 - 23:02]
example,

[22:59 - 23:05]
I've only ever played

[23:02 - 23:08]
uh Elden Ring, but I'm well aware that

[23:05 - 23:10]
the folks that or I think anyway the

[23:08 - 23:12]
folks uh from software that created

[23:10 - 23:14]
Elden Ring are also the ones that

[23:12 - 23:17]
created the Dark Soul games. I've never

[23:14 - 23:19]
played a Dark Soul game or at least I

[23:17 - 23:21]
have maybe my younger brother tricked me

[23:19 - 23:27]
into playing it once and I just died a

[23:21 - 23:33]
lot. that I I had no connection with

[23:27 - 23:35]
Elder Ring and the Souls games,

[23:33 - 23:37]
but I do now because I I fell in love

[23:35 - 23:39]
with Elder Ring and then I started to do

[23:37 - 23:42]
some digging and everybody talks about

[23:39 - 23:45]
this is that it's like this is like Dark

[23:42 - 23:47]
Souls leveled up and open world and this

[23:45 - 23:49]
kind of thing. So, the point is that you

[23:47 - 23:51]
don't have to I don't think Elden Ring

[23:49 - 23:52]
had a launcher. Maybe it did, but I

[23:51 - 23:55]
didn't find out about the other games

[23:52 - 23:57]
that way. So, you don't have to have a a

[23:55 - 23:59]
walking billboard that's creating all

[23:57 - 24:01]
this advertisement.

[23:59 - 24:03]
You may think that you're doing yourself

[24:01 - 24:07]
a service, but you're probably actually

[24:03 - 24:10]
doing yourself a disservice truthfully

[24:07 - 24:12]
because one, you're slowing us down that

[24:10 - 24:14]
we click the play and then now we have

[24:12 - 24:17]
to navigate this and do this. And the

[24:14 - 24:20]
whole time is I think many of us are

[24:17 - 24:23]
getting annoyed because I didn't launch

[24:20 - 24:24]
I didn't click that button so I could

[24:23 - 24:27]
jump into your launcher and do this

[24:24 - 24:29]
stuff. I clicked this button here. Let

[24:27 - 24:30]
me let me show you what this button

[24:29 - 24:33]
says.

[24:30 - 24:34]
Okay. So, I'll show you why I clicked

[24:33 - 24:36]
this button. This should hopefully make

[24:34 - 24:38]
sense to any of the folks that are back

[24:36 - 24:40]
here. I'm not even being snarky. I'm

[24:38 - 24:42]
just trying to be as straightforward and

[24:40 - 24:45]
honest as possible. there's this button

[24:42 - 24:46]
called play.

[24:45 - 24:49]
And when you click it for the game that

[24:46 - 24:51]
you're interested in, it allows you to

[24:49 - 24:53]
play that game.

[24:51 - 24:55]
So when I click the button that says

[24:53 - 24:57]
play,

[24:55 - 24:59]
it means I want to play that game. It

[24:57 - 25:00]
doesn't mean I want to launch your game

[24:59 - 25:04]
launcher.

[25:00 - 25:06]
So if we go to like I don't actually

[25:04 - 25:10]
know

[25:06 - 25:10]
if I

[25:11 - 25:17]
if I click play if it has a launcher.

[25:14 - 25:20]
But I don't recall uh a launcher. I do

[25:17 - 25:22]
recall a launcher for some of the other

[25:20 - 25:24]
ones. The point is is that like this

[25:22 - 25:28]
this button clearly is indicating what

[25:24 - 25:30]
the user wants to do. And look, if you

[25:28 - 25:31]
want to have a game launcher or

[25:30 - 25:35]
something like your own little console

[25:31 - 25:37]
thing, talk to Steam and then have a

[25:35 - 25:38]
second button or something that can

[25:37 - 25:40]
launch your thing. And so people who

[25:38 - 25:43]
want to launch your thing and look at it

[25:40 - 25:44]
can do that or whatever.

[25:43 - 25:47]
Uh and the people that just want to play

[25:44 - 25:51]
the game don't have to be slowed down by

[25:47 - 25:53]
like another walking uh advert

[25:51 - 25:55]
because I think that what you may not

[25:53 - 26:00]
realize is this may actually be doing

[25:55 - 26:02]
you more harm than good. It's like uh

[26:00 - 26:04]
well there's actually many analogies I

[26:02 - 26:06]
could come up with, many stories, but

[26:04 - 26:08]
I'll

[26:06 - 26:10]
I don't know. I'll I'll refrain because

[26:08 - 26:13]
I may have to say some names if I do

[26:10 - 26:16]
this.

[26:13 - 26:16]
Uh,

[26:16 - 26:20]
man, there's there's two that are at the

[26:19 - 26:22]
tip of my tongue that I want to share,

[26:20 - 26:26]
but it probably wouldn't be a super

[26:22 - 26:28]
great idea to do this,

[26:26 - 26:30]
so I won't.

[26:28 - 26:32]
All right. So, we we'll talk about maybe

[26:30 - 26:34]
stop killing games. We'll talk maybe

[26:32 - 26:36]
about this. Don't be the agent of your

[26:34 - 26:39]
own destruction. I want to also share as

[26:36 - 26:42]
far as music goes,

[26:39 - 26:46]
I skipped way past it, but in typical

[26:42 - 26:49]
form, I'm loving Dream Clinic and

[26:46 - 26:52]
listening to Psycher

[26:49 - 26:56]
right here, right there.

[26:52 - 26:58]
And I think that

[26:56 - 27:02]
fingers crossed. I was talking with the

[26:58 - 27:04]
Dream Clinic folks and I think that they

[27:02 - 27:06]
said they're going to create I'll have

[27:04 - 27:08]
to double check. I think I have it

[27:06 - 27:11]
somewhere that they are going to

[27:08 - 27:14]
potentially create a psycher too.

[27:11 - 27:16]
And I'll probably be listening to that.

[27:14 - 27:19]
I mean I just as an aside I like a lot

[27:16 - 27:20]
of the Dream Clinic stuff. I've been

[27:19 - 27:23]
actually trying to listen to almost all

[27:20 - 27:27]
of them, but there's two that I listen

[27:23 - 27:30]
to in particular when I'm recording VODs

[27:27 - 27:31]
or doing stream and it's basically the

[27:30 - 27:34]
Automata

[27:31 - 27:39]
and

[27:34 - 27:43]
Automata and Psyker, I think. Yeah. So,

[27:39 - 27:46]
it's those first two.

[27:43 - 27:48]
And I I I like many of them, but the

[27:46 - 27:50]
first two I think are really are kind of

[27:48 - 27:53]
more,

[27:50 - 27:54]
at least for me, this this kind of jivey

[27:53 - 27:57]
thing.

[27:54 - 28:02]
Okay,

[27:57 - 28:05]
I I brought us up to speed now on our

[28:02 - 28:08]
on our music collection.

[28:05 - 28:11]
Okay, so the topics we had written down,

[28:08 - 28:13]
stop killing games and don't be the

[28:11 - 28:16]
agent of your own destruction. And I

[28:13 - 28:20]
feel like there was

[28:16 - 28:22]
Oh, yeah. I was talking with Taylor

[28:20 - 28:25]
earlier tonight

[28:22 - 28:25]
and

[28:30 - 28:34]
this came up. This whole prompt

[28:32 - 28:37]
engineering

[28:34 - 28:42]
Now I have videos on this. I think you

[28:37 - 28:42]
can s Can you search? Yeah.

[28:46 - 28:51]
Okay.

[28:48 - 28:54]
So here for those of you that are just

[28:51 - 28:57]
listening that aren't watching this

[28:54 - 29:01]
in the search on goju tech talk I have a

[28:57 - 29:04]
few videos on prompt engineering. First

[29:01 - 29:07]
one prompt engineering. Nah, just more

[29:04 - 29:10]
hype. But is prompt engineering actually

[29:07 - 29:12]
engineering is a second one.

[29:10 - 29:14]
Uh

[29:12 - 29:16]
I guess there's only two, but then

[29:14 - 29:17]
there's a few more that popped up that I

[29:16 - 29:18]
think also talk about prompt

[29:17 - 29:21]
engineering.

[29:18 - 29:24]
In a nutshell, if I can explain this

[29:21 - 29:26]
maybe the fastest that I possibly can.

[29:24 - 29:31]
Uh let let's actually have Google fact

[29:26 - 29:33]
check me. What is engineering?

[29:31 - 29:36]
All right, let's see what Google says

[29:33 - 29:39]
and let's try to compare and contrast

[29:36 - 29:42]
this with the universe of prompt

[29:39 - 29:45]
engineering. All right, what Google says

[29:42 - 29:47]
engineering is is this. It is the

[29:45 - 29:50]
application of scientific and

[29:47 - 29:51]
mathematical principles to design, build

[29:50 - 29:53]
and maintain structures, machines,

[29:51 - 29:55]
processes and systems. It involves

[29:53 - 29:57]
problem solving, innovation and

[29:55 - 29:59]
practical application of knowledge to

[29:57 - 30:01]
create solutions that benefit society.

[29:59 - 30:02]
engineers use their understanding of

[30:01 - 30:05]
science and mathematics to develop new

[30:02 - 30:07]
technologies, improve existing ones and

[30:05 - 30:11]
solve real world challenges. And then it

[30:07 - 30:14]
explains some more of this stuff. Notice

[30:11 - 30:17]
if you will

[30:14 - 30:19]
two key important properties of

[30:17 - 30:22]
engineering

[30:19 - 30:24]
are that is it is the application of

[30:22 - 30:27]
scientific

[30:24 - 30:30]
and mathematical principles. I pretty

[30:27 - 30:33]
much agree with this. The way that I

[30:30 - 30:38]
describe it is that engineering is

[30:33 - 30:41]
the process of applying

[30:38 - 30:44]
knowledge that we have

[30:41 - 30:47]
a priori that we've learned through

[30:44 - 30:51]
scientific experimentation and logical

[30:47 - 30:55]
reasoning that will give us predictable

[30:51 - 30:57]
outcomes that we are going to rely on to

[30:55 - 31:00]
build things.

[30:57 - 31:02]
The predictable outcome part is super

[31:00 - 31:07]
important. This is what makes it

[31:02 - 31:10]
engineering and not uh astrology or

[31:07 - 31:13]
tarot card readings. Okay, those things

[31:10 - 31:15]
are less predictable. And if you like

[31:13 - 31:17]
those things, that's cool. I like those

[31:15 - 31:19]
things, too. But I don't want those

[31:17 - 31:23]
things. I don't want tarot cards and

[31:19 - 31:26]
astrology to be the foundations of a

[31:23 - 31:29]
bridge between

[31:26 - 31:32]
uh the west uh coast

[31:29 - 31:34]
uh peninsula and the east coast

[31:32 - 31:36]
peninsula in Northern California. I

[31:34 - 31:38]
think that would not be super great

[31:36 - 31:41]
because I don't think that bridge is

[31:38 - 31:43]
going to stand. I think it's going to be

[31:41 - 31:46]
imaginary and lots of people are not

[31:43 - 31:48]
going to survive driving on that bridge.

[31:46 - 31:51]
For bridges or other things related to

[31:48 - 31:54]
engineering like airplanes and computers

[31:51 - 31:58]
and all this stuff, I would like to have

[31:54 - 32:01]
them be based off of science that is far

[31:58 - 32:04]
more deterministic. It is reliable and

[32:01 - 32:06]
that we can trust. So then when we get a

[32:04 - 32:08]
result, we can rely on that result. At

[32:06 - 32:11]
least for the most part, it becomes a

[32:08 - 32:13]
nines's problem. Like it's 99 how many

[32:11 - 32:16]
nines reliable?

[32:13 - 32:18]
For the most part, airplanes are very

[32:16 - 32:20]
reliable. I realized recently there's

[32:18 - 32:23]
been a tragic accident with a plane

[32:20 - 32:25]
going down. Uh my my heart goes out to

[32:23 - 32:28]
all the people that were involved in

[32:25 - 32:30]
that either on the plane or the

[32:28 - 32:33]
families. It's heartbreaking when these

[32:30 - 32:36]
things happen and we need to do a better

[32:33 - 32:38]
job uh trying to make these airplanes

[32:36 - 32:41]
and other modes of transportation safer

[32:38 - 32:43]
and that is going to go to engineering.

[32:41 - 32:48]
It's going to do more with us getting

[32:43 - 32:49]
things more reliable right

[32:48 - 32:52]
now.

[32:49 - 32:54]
Is creating a prompt

[32:52 - 32:57]
for a machine learning system, a large

[32:54 - 32:59]
language model,

[32:57 - 33:01]
the application of scientific and

[32:59 - 33:05]
mathematical principles?

[33:01 - 33:06]
It is not. And the reason why it is not,

[33:05 - 33:09]
and I don't mean to sound like I'm the

[33:06 - 33:11]
knower of all things.

[33:09 - 33:13]
I just mean that based on this

[33:11 - 33:15]
definition which I think is a pretty

[33:13 - 33:17]
good definition. The reason why prompt

[33:15 - 33:20]
engineering is not this is that it is

[33:17 - 33:22]
not the application of scientific or

[33:20 - 33:25]
mathematical principles.

[33:22 - 33:27]
That's not what writing a prompt is for

[33:25 - 33:30]
a language model. Writing a prompt in

[33:27 - 33:33]
for a language model is putting in a

[33:30 - 33:37]
whole bunch of written pros in a variety

[33:33 - 33:40]
of ways and hoping crossing your fingers

[33:37 - 33:43]
that you'll get a specific outcome.

[33:40 - 33:45]
That's what it is.

[33:43 - 33:47]
It it doesn't really go much further

[33:45 - 33:49]
than that.

[33:47 - 33:52]
And the reason for this is manyfold.

[33:49 - 33:54]
First, that language models are

[33:52 - 33:59]
stochastic machines. So even if you had

[33:54 - 33:59]
a perfect prompt

[33:59 - 34:04]
because the language model is stochastic

[34:02 - 34:06]
in nature, there's no guarantee that

[34:04 - 34:08]
that

[34:06 - 34:11]
uh prompt that you give it is going to

[34:08 - 34:14]
give you a deterministic answer. But

[34:11 - 34:16]
that's what you need with engineering.

[34:14 - 34:19]
With engineering, you need things that

[34:16 - 34:21]
are reliable,

[34:19 - 34:23]
right? that like like chemical

[34:21 - 34:28]
engineering is a really good one.

[34:23 - 34:32]
Imagine if you had two chemicals that

[34:28 - 34:35]
you're going to work with and

[34:32 - 34:37]
once in a while when you combine them

[34:35 - 34:39]
things just blew up.

[34:37 - 34:41]
Do you think people would fiddle around

[34:39 - 34:43]
with those chemicals?

[34:41 - 34:45]
I mean the likelihood I think is pretty

[34:43 - 34:48]
slim. It it might happen but it's

[34:45 - 34:51]
probably going to be less safe. The

[34:48 - 34:54]
beauty though about chemical engineering

[34:51 - 34:57]
is that the chemicals have very reliable

[34:54 - 34:58]
properties when they're combined for the

[34:57 - 35:00]
most part. It's not perfectly

[34:58 - 35:02]
deterministic. And a lot of that I think

[35:00 - 35:04]
probably has more to do with limitations

[35:02 - 35:08]
in the knowledge that we possess as a

[35:04 - 35:12]
species on how the chemicals work than

[35:08 - 35:15]
sort of random magical events that

[35:12 - 35:18]
happen that cause bad behaviors.

[35:15 - 35:21]
Uh what I mean by that is there may be

[35:18 - 35:22]
cases where unexpected results happen

[35:21 - 35:24]
when you combine chemicals that are

[35:22 - 35:27]
supposed to play well with them with

[35:24 - 35:30]
each other, but they might not. And but

[35:27 - 35:32]
that outcome may have less to do with

[35:30 - 35:36]
the chemicals behaving in an

[35:32 - 35:40]
unpredictable way and more to do with an

[35:36 - 35:43]
environmental change that was uh

[35:40 - 35:45]
undetected. people were unaware of and

[35:43 - 35:47]
then that caused things to go in like a

[35:45 - 35:50]
bad direction. I'm not saying that this

[35:47 - 35:52]
is always the case. There are cases of

[35:50 - 35:54]
uncertainty in like the knowledge that

[35:52 - 35:56]
we possess with these properties, but

[35:54 - 35:58]
there's other factors is I guess what

[35:56 - 36:00]
I'm talking about.

[35:58 - 36:03]
For the most part, we can largely depend

[36:00 - 36:07]
on these things doing the things that

[36:03 - 36:07]
they're supposed to do.

[36:08 - 36:14]
Like for example, if I have some code,

[36:12 - 36:17]
let's use let's use uh software

[36:14 - 36:21]
engineering since software engineering

[36:17 - 36:23]
is I think a really good

[36:21 - 36:27]
a vehicle for this.

[36:23 - 36:29]
We'll just look at

[36:27 - 36:30]
uh

[36:29 - 36:33]
C++

[36:30 - 36:36]
since

[36:33 - 36:38]
it's my favorite language.

[36:36 - 36:40]
I'm sorry if people don't fully love

[36:38 - 36:42]
that.

[36:40 - 36:46]
And this is our staging server. So if

[36:42 - 36:49]
things are going uh in a wrong way. Oh,

[36:46 - 36:49]
there we go. Okay.

[36:50 - 36:54]
Now, let me show you what I mean about

[36:51 - 36:56]
engineering and what engineering kind of

[36:54 - 36:59]
looks like

[36:56 - 37:02]
and why you want it to be deterministic

[36:59 - 37:03]
and why this is engineering and what

[37:02 - 37:05]
you're doing with prompts is not

[37:03 - 37:08]
engineering. Okay.

[37:05 - 37:13]
This is engineering because this if

[37:08 - 37:17]
statement if right is false

[37:13 - 37:19]
it cannot enter this control expression.

[37:17 - 37:23]
It cannot

[37:19 - 37:26]
it cannot ever go this way. It must obey

[37:23 - 37:28]
this. It can never go inside of this. I

[37:26 - 37:30]
don't know, this probably sounds like

[37:28 - 37:33]
it's a broken record, but I'm trying to

[37:30 - 37:37]
be super clear is that the the laws of

[37:33 - 37:40]
the universe say that only when right is

[37:37 - 37:43]
true are you allowed to go in here. That

[37:40 - 37:47]
means that only when right is not true,

[37:43 - 37:50]
it will go in here. Now imagine if you

[37:47 - 37:54]
will that you're running this program

[37:50 - 37:58]
and then just one out of every let's say

[37:54 - 38:00]
million times it decides

[37:58 - 38:02]
I'm going to go in here when right's

[38:00 - 38:04]
false. No big deal. I'm just going to do

[38:02 - 38:08]
it once out of every 10 million tries.

[38:04 - 38:13]
Like largely it's doing the right thing.

[38:08 - 38:15]
What could happen in that one case?

[38:13 - 38:17]
What does that happen? What does that

[38:15 - 38:19]
result in with the user, the user

[38:17 - 38:21]
experience? What about the developers?

[38:19 - 38:23]
How do developers debug this? How do

[38:21 - 38:25]
they make this stop happening? So, you

[38:23 - 38:26]
get the user report that there's this

[38:25 - 38:29]
bug and it went in here and you're

[38:26 - 38:32]
looking at this code saying, well, the

[38:29 - 38:34]
code looks right.

[38:32 - 38:36]
And you're like, I don't think there's a

[38:34 - 38:40]
bug here. And then you say, oh, that's

[38:36 - 38:42]
right. It's stochastic, though. Shoot.

[38:40 - 38:45]
That's right. We use a stochastic

[38:42 - 38:49]
system. And so once in a while, right

[38:45 - 38:51]
could be false and go in here. Well,

[38:49 - 38:54]
Bob, I guess we ought to deal with that.

[38:51 - 38:56]
That's what you have. And that is not

[38:54 - 39:00]
engineering. That's not how engineering

[38:56 - 39:02]
works. Engineering works based off of

[39:00 - 39:05]
scientific and mathematical principles

[39:02 - 39:07]
that are going to give us deterministic

[39:05 - 39:09]
or largely deterministic behavior. I say

[39:07 - 39:11]
largely because there's some cases where

[39:09 - 39:14]
variables are outside of our control.

[39:11 - 39:17]
Like for example,

[39:14 - 39:21]
uh if this has some kind of weird uh

[39:17 - 39:24]
memory overwrite that

[39:21 - 39:28]
right comes in, where is where is right?

[39:24 - 39:30]
Right comes in here and write is uh true

[39:28 - 39:33]
and then somehow one of these operations

[39:30 - 39:35]
flips right to write is false. Now this

[39:33 - 39:38]
code will still do the right thing, but

[39:35 - 39:40]
if you looked at the value of write

[39:38 - 39:41]
here, you would see, oh, it's true. So,

[39:40 - 39:42]
it's going to go in here and you put a

[39:41 - 39:44]
break point and then it doesn't go in

[39:42 - 39:46]
here and you're like, "Wait, what

[39:44 - 39:49]
happened?" Well, it turns out this FD

[39:46 - 39:54]
set, this file descriptor set actually

[39:49 - 39:56]
overwrote into uh this write because uh

[39:54 - 40:01]
somebody uh mismanaged the size or

[39:56 - 40:03]
something of this FD. Okay, now that

[40:01 - 40:05]
could happen, but notice that that

[40:03 - 40:08]
doesn't change the fact that this has to

[40:05 - 40:09]
be uh correctly checked. What I'm

[40:08 - 40:12]
talking about with these stochastic

[40:09 - 40:14]
systems is that you can have this check

[40:12 - 40:17]
and it can say yes right must be true

[40:14 - 40:19]
here and then it can be like but this

[40:17 - 40:22]
time we're going to let it fly. So write

[40:19 - 40:25]
is false and it's like go ahead in

[40:22 - 40:29]
anyway. What the heck? That's not going

[40:25 - 40:31]
to work as far as foundation of

[40:29 - 40:33]
engineering. That's why prompt

[40:31 - 40:35]
engineering is not engineering. It's

[40:33 - 40:38]
something else.

[40:35 - 40:40]
I'm not sure exactly what it is,

[40:38 - 40:43]
but it's not engineering. And I think

[40:40 - 40:46]
that people are doing themselves a big

[40:43 - 40:47]
disservice by calling it engineering.

[40:46 - 40:49]
But check this out. We're going to go

[40:47 - 40:52]
down this rabbit hole a little bit more

[40:49 - 40:54]
hopefully until I'll just keep fighting

[40:52 - 40:56]
this battle. I I'll I I think I was

[40:54 - 40:59]
telling Taylor on the phone tonight,

[40:56 - 41:01]
I'll I'll probably fight this battle

[40:59 - 41:03]
until either people stop calling it

[41:01 - 41:09]
prompt engineering, which will probably

[41:03 - 41:09]
happen at some point, or I die.

[41:09 - 41:14]
One of those two things. And I've I've

[41:12 - 41:15]
gone down this path before. There was

[41:14 - 41:18]
there used to be this thing called

[41:15 - 41:22]
software 2.0. And thankfully we stopped

[41:18 - 41:24]
using this term and instead we started

[41:22 - 41:26]
using this term machine programming and

[41:24 - 41:29]
we wouldn't use software 2.0 cuz it just

[41:26 - 41:32]
was sort of nonsensical.

[41:29 - 41:35]
And at the time people were like this is

[41:32 - 41:37]
crazy. This is a interesting fight to

[41:35 - 41:39]
draw a line in the sand on. And I was

[41:37 - 41:43]
like well it has to do more with the

[41:39 - 41:46]
fact that software 2.0 one doesn't make

[41:43 - 41:49]
logical sense to me. There's no universe

[41:46 - 41:52]
where I see all software being written

[41:49 - 41:55]
all from a bunch of

[41:52 - 41:57]
uh numbers that can't be reasoned about

[41:55 - 42:00]
logically

[41:57 - 42:03]
and on top of that uh it's using systems

[42:00 - 42:06]
that are stoastic and I don't know how

[42:03 - 42:11]
that can give us reliable software and

[42:06 - 42:14]
then the third piece is that who says

[42:11 - 42:16]
we're just at version 2.0 You know, what

[42:14 - 42:19]
about all what about the 70 years of

[42:16 - 42:22]
work that people have done

[42:19 - 42:25]
in computer science and with software

[42:22 - 42:28]
uh prior to this? Does that all not

[42:25 - 42:30]
count? It's just like it's whether its

[42:28 - 42:33]
intention is or not, it it's really a

[42:30 - 42:35]
dubious thing to say because of sort of

[42:33 - 42:38]
the arrogance around it. It's very

[42:35 - 42:41]
arrogant to to suppose that a neural

[42:38 - 42:44]
network or neural networks will replace

[42:41 - 42:46]
all software code and then all the

[42:44 - 42:49]
software we had prior to neural networks

[42:46 - 42:52]
is uh 1.0 and everything moving forward

[42:49 - 42:54]
that's neural networks is 2.0. Keep in

[42:52 - 42:57]
mind I teach neural networks at the

[42:54 - 43:00]
universities. So I love neural nets. I'm

[42:57 - 43:03]
a I'm a huge fan of neural nets but

[43:00 - 43:06]
they're not a panacea.

[43:03 - 43:09]
So I would die on that hill and I

[43:06 - 43:11]
literally was dying on that hill and

[43:09 - 43:12]
thankfully we stopped using this term

[43:11 - 43:15]
mostly because I think people came to

[43:12 - 43:17]
their senses and they're like, "Oh yeah,

[43:15 - 43:20]
that makes a lot of sense. This is

[43:17 - 43:21]
actually a really bad term." Similar

[43:20 - 43:24]
thing with prompt engineering. It's not

[43:21 - 43:27]
engineering, guys. Stop calling it

[43:24 - 43:28]
engineering. you're

[43:27 - 43:31]
you're

[43:28 - 43:35]
making yourself look bad

[43:31 - 43:38]
and you're hurting your reputation. Let

[43:35 - 43:40]
me explain why. The the first is you're

[43:38 - 43:43]
making yourself look bad because people

[43:40 - 43:46]
are not stupid. They're gullible, but

[43:43 - 43:48]
they're not dumb. And they also know how

[43:46 - 43:50]
to use computers and Google things like

[43:48 - 43:52]
what engineering is. And so when you

[43:50 - 43:55]
call this prompt engineering and they're

[43:52 - 43:57]
like, "Huh, it's engineering." and then

[43:55 - 43:58]
their partner says, "Well, what is

[43:57 - 44:00]
engineering?" And then they get on their

[43:58 - 44:02]
computer and they look it up and they're

[44:00 - 44:04]
like, "Wait a second."

[44:02 - 44:08]
Um,

[44:04 - 44:11]
so prompts though don't necessarily

[44:08 - 44:14]
uh use knowledge that we have to create

[44:11 - 44:17]
solutions that benefit society. Like

[44:14 - 44:20]
that doesn't feel like that matches. Uh,

[44:17 - 44:21]
and also when I'm doing prompts, I'm not

[44:20 - 44:23]
necessarily using science and

[44:21 - 44:26]
mathematics. So this is weird. It

[44:23 - 44:28]
doesn't feel necessarily like this is

[44:26 - 44:30]
again going back to why this this is

[44:28 - 44:33]
engineering. It's using science and

[44:30 - 44:36]
mathematics. This is logic.

[44:33 - 44:39]
Logic is the foundation of mathematics.

[44:36 - 44:42]
Like mathematics sort of comes after

[44:39 - 44:45]
basic logic at least in my mind that you

[44:42 - 44:48]
have things like first order logic and

[44:45 - 44:50]
uh predicate calculus which is things

[44:48 - 44:54]
like

[44:50 - 44:57]
uh this and this

[44:54 - 45:02]
or you have something like this or this

[44:57 - 45:05]
and then you introduce things like for

[45:02 - 45:09]
excuse me for all x's

[45:05 - 45:12]
there is some x that exists

[45:09 - 45:14]
that is equal to some y something like

[45:12 - 45:16]
that right these are things that you

[45:14 - 45:22]
learn in first order logic

[45:16 - 45:24]
and this here is uh first order logic

[45:22 - 45:26]
because it's well I guess it's

[45:24 - 45:29]
mathematics but uh this is first order

[45:26 - 45:32]
logic because it's using the and that we

[45:29 - 45:34]
just talked about it's saying this thing

[45:32 - 45:38]
and this

[45:34 - 45:39]
so while these things are the

[45:38 - 45:41]
cornerstone of mathematics that we think

[45:39 - 45:44]
of I think in a traditional sense like

[45:41 - 45:46]
the less than operator or equality.

[45:44 - 45:48]
These

[45:46 - 45:51]
uh logical operators I think are

[45:48 - 45:53]
actually precursors to mathematics. For

[45:51 - 45:55]
some reason we we like learn these

[45:53 - 45:58]
things later like at the collegiate

[45:55 - 46:00]
level. I don't know why that is,

[45:58 - 46:05]
especially given that we do learn what

[46:00 - 46:08]
and and or are uh in grade school and

[46:05 - 46:10]
like before high school. But again, a

[46:08 - 46:13]
part that's going to make this even

[46:10 - 46:14]
stranger

[46:13 - 46:16]
is,

[46:14 - 46:20]
and this is maybe just in the United

[46:16 - 46:20]
States, but what we often learn

[46:21 - 46:28]
in school prior to collegate learning of

[46:25 - 46:31]
first order logic.

[46:28 - 46:35]
is this notion that

[46:31 - 46:35]
when we

[46:35 - 46:39]
use the word and

[46:37 - 46:44]
we mean

[46:39 - 46:49]
the logical and. So go to the grocery

[46:44 - 46:53]
store, get bread and milk.

[46:49 - 46:55]
Yes, that is what and means here. But

[46:53 - 46:59]
interestingly enough, when we get into

[46:55 - 47:01]
the or operator,

[46:59 - 47:03]
I'm I suspect I'll find an ore operator

[47:01 - 47:05]
in here somewhere.

[47:03 - 47:08]
I don't think I'd have to look too far.

[47:05 - 47:12]
Here's the ore operator.

[47:08 - 47:12]
We generally mean

[47:14 - 47:19]
exclusive or

[47:16 - 47:24]
not inclusive or.

[47:19 - 47:27]
This is inclusive or not exclusive or.

[47:24 - 47:30]
And the reason why

[47:27 - 47:35]
is that what we mean when we generally

[47:30 - 47:38]
say or in normal things is this or that.

[47:35 - 47:42]
Well, dear, you can have popcorn or you

[47:38 - 47:47]
can have ice cream. Which do you want?

[47:42 - 47:51]
What we mean in first order logic with

[47:47 - 47:53]
uh inclusive or is either can be true or

[47:51 - 47:56]
both can be true.

[47:53 - 47:58]
You can have the popcorn and the ice

[47:56 - 48:04]
cream.

[47:58 - 48:07]
The exclusive or is the or that we use

[48:04 - 48:09]
in normal conversation. It's one or the

[48:07 - 48:11]
other, not both.

[48:09 - 48:15]
So, do you want the popcorn or the ice

[48:11 - 48:20]
cream? You can only have one. And that

[48:15 - 48:22]
actually is why we call it exclusive or.

[48:20 - 48:24]
It's you can have one or you can have

[48:22 - 48:28]
the other, but you cannot have both.

[48:24 - 48:31]
It's exclusive to each one of these. And

[48:28 - 48:33]
inclusive or is basically saying that

[48:31 - 48:36]
no, it's inclusive. So, either of these

[48:33 - 48:40]
at the same time is fine. You can have

[48:36 - 48:46]
both. Uh so in code when we say or what

[48:40 - 48:47]
we actually mean and and this maybe uh

[48:46 - 48:49]
is

[48:47 - 48:50]
something that maybe you haven't learned

[48:49 - 48:53]
in school. I don't know if many people

[48:50 - 48:56]
talk about this is people will learn or

[48:53 - 48:59]
and then they'll learn exclusive or.

[48:56 - 49:00]
What we really mean here is inclusive or

[48:59 - 49:04]
and then when you think about it like

[49:00 - 49:08]
this there's two ores. There's the

[49:04 - 49:10]
inclusive ore which is that guy and then

[49:08 - 49:14]
there's exclusive ore which is a a

[49:10 - 49:14]
different guy.

[49:14 - 49:21]
And so when we say or in the traditional

[49:18 - 49:24]
sense of these operators, there's the

[49:21 - 49:27]
silent inclusive.

[49:24 - 49:29]
Excuse me, my throat's uh I guess I've

[49:27 - 49:31]
been talking too much today.

[49:29 - 49:36]
So

[49:31 - 49:36]
I I find this at least slightly amusing

[49:37 - 49:40]
in that

[49:41 - 49:47]
we don't say inclusive or we just say

[49:44 - 49:50]
or. It kind of makes sense. But I think

[49:47 - 49:51]
when people learn exclusive or they get

[49:50 - 49:53]
confused

[49:51 - 49:54]
or at least a lot of people that I've

[49:53 - 49:57]
taught exclusive or they get confused.

[49:54 - 49:59]
are like, "Oh, exor, you know, it's it's

[49:57 - 50:02]
exor." So, it's or or it's exor. And

[49:59 - 50:04]
then many people don't know the X stands

[50:02 - 50:07]
for exclusive.

[50:04 - 50:09]
And so, again, this is going back to the

[50:07 - 50:11]
whole mastering the basics. If you just

[50:09 - 50:14]
master the basics, a lot of this stuff

[50:11 - 50:16]
just illuminates itself. So, when you

[50:14 - 50:18]
understand X or the X stands for

[50:16 - 50:21]
exclusive and then you think, what does

[50:18 - 50:24]
exclusive mean? Exclusive means one or

[50:21 - 50:29]
the other. So like a a mutex

[50:24 - 50:30]
like a a lock is a mutual mutual

[50:29 - 50:33]
exclusion.

[50:30 - 50:37]
They cannot be both active at the same

[50:33 - 50:38]
time. It's mutually exclusive meaning

[50:37 - 50:44]
only one of these things can do this at

[50:38 - 50:48]
a time. Mutex. So exor is

[50:44 - 50:50]
the exclusive or and so they can't both

[50:48 - 50:51]
be at the same thing. So it's the same

[50:50 - 50:53]
principle.

[50:51 - 50:56]
Mutex

[50:53 - 50:59]
the exclusive part exclusive or

[50:56 - 51:04]
exclusive part same principle same

[50:59 - 51:07]
principle and now when you think about

[51:04 - 51:09]
exor it's probably like oh right yeah

[51:07 - 51:11]
it's just the exclusive part and then

[51:09 - 51:14]
when you think about normal or it's

[51:11 - 51:15]
there's the silent inclusive part and so

[51:14 - 51:18]
hopefully this has made your life a

[51:15 - 51:21]
little bit better is that now you can

[51:18 - 51:24]
look at code in maybe a slightly

[51:21 - 51:26]
more uh deeply nuanced way

[51:24 - 51:28]
and start to ask yourself really

[51:26 - 51:31]
interesting questions like what about

[51:28 - 51:31]
and

[51:33 - 51:38]
and I don't I I haven't thought about

[51:34 - 51:41]
and and it's exclusivity or inclusivity.

[51:38 - 51:45]
uh I think by default based on the

[51:41 - 51:47]
properties of and it has to be inclusive

[51:45 - 51:50]
in the sense that you kind of need both

[51:47 - 51:53]
things to be uh true simultaneously. So

[51:50 - 51:55]
I don't know that you can have an

[51:53 - 51:58]
exclusive and I mean we'd have to think

[51:55 - 52:00]
about this a little bit more.

[51:58 - 52:04]
This is just sort of my uh very quick

[52:00 - 52:06]
treatment of this, but

[52:04 - 52:08]
hopefully if if nothing else, we've

[52:06 - 52:13]
learned a little bit more about

[52:08 - 52:14]
programming and the silent inclusive

[52:13 - 52:17]
that's in front of all these ore

[52:14 - 52:19]
operators. So given all this our

[52:17 - 52:22]
discussion about what engineering is and

[52:19 - 52:24]
it's based on the principles of science

[52:22 - 52:28]
and math and now we go back to this

[52:24 - 52:31]
thing about uh prompt engineering.

[52:28 - 52:33]
It's my hope that you can see

[52:31 - 52:35]
that's not engineering. That's

[52:33 - 52:37]
something, but it's not engineering.

[52:35 - 52:40]
Now, you may be asking yourself this

[52:37 - 52:43]
question. Why do why do a lot of these

[52:40 - 52:45]
big vendors and trillion dollar

[52:43 - 52:46]
companies or whatever keep calling this

[52:45 - 52:49]
prompt engineering and they're

[52:46 - 52:51]
encouraging people to do this?

[52:49 - 52:55]
I have some thoughts on this

[52:51 - 52:57]
and I don't want to be I don't want to

[52:55 - 53:00]
promote this idea of thinking people are

[52:57 - 53:03]
bad actors and I want to continue to

[53:00 - 53:06]
push the narrative of I'm going to

[53:03 - 53:07]
foster goodwill. But I do want to share

[53:06 - 53:10]
that

[53:07 - 53:12]
there is this interesting

[53:10 - 53:15]
there is this interesting side effect

[53:12 - 53:19]
that could potentially be happening

[53:15 - 53:21]
which is if if enough people start doing

[53:19 - 53:25]
prompts that they're specifically

[53:21 - 53:27]
testing with pick your vendor favorite

[53:25 - 53:30]
uh

[53:27 - 53:35]
your your favorite vendor's uh company.

[53:30 - 53:38]
So, we'll say Google, Microsoft,

[53:35 - 53:42]
I'm not even going to list this one that

[53:38 - 53:44]
I'm not a super fan of. Anthropic,

[53:42 - 53:47]
Amazon,

[53:44 - 53:49]
Meta, so on so forth. If you notice that

[53:47 - 53:54]
there's one Oh,

[53:49 - 53:57]
sorry, IBM. My bad. Uh, there may be a

[53:54 - 54:01]
big one that I'm leaving off, and it's

[53:57 - 54:03]
not X, by the way. I'll even add X for

[54:01 - 54:06]
it's um

[54:03 - 54:08]
what is it? Grock or something.

[54:06 - 54:11]
There is one on here that I'm not

[54:08 - 54:14]
listing cuz I just I feel like they're

[54:11 - 54:15]
they're being such bad actors. I I'm not

[54:14 - 54:18]
even going to talk about them anymore.

[54:15 - 54:22]
That's that's my way of

[54:18 - 54:24]
not saying something nice. They won't

[54:22 - 54:26]
even get their name on this channel.

[54:24 - 54:28]
That's I'm I'm really not super

[54:26 - 54:32]
impressed by the stuff that they've been

[54:28 - 54:35]
doing, but and and they are definitely

[54:32 - 54:37]
leading the charge on promoting this in

[54:35 - 54:42]
a not super great way. But for these

[54:37 - 54:46]
other guys, you can imagine that if you

[54:42 - 54:48]
are creating prompts for their language

[54:46 - 54:51]
models, what are you really doing? What

[54:48 - 54:54]
you're doing is in some sense you're

[54:51 - 54:56]
writing code for them that is I mean

[54:54 - 54:58]
it's not even reliable code because it's

[54:56 - 55:01]
not deterministic but if it were it is

[54:58 - 55:06]
likely going to give you the answer that

[55:01 - 55:08]
you want specifically for their system.

[55:06 - 55:11]
You're essentially writing proprietary

[55:08 - 55:12]
code that's not code that is only sort

[55:11 - 55:15]
of guaranteed to work with Google or

[55:12 - 55:18]
Microsoft or IBM or Anthropic or Amazon

[55:15 - 55:21]
or Meta or X and so on and so forth. Why

[55:18 - 55:23]
would you do that? Why instead would you

[55:21 - 55:26]
not write something that is not

[55:23 - 55:29]
proprietary that will function across

[55:26 - 55:32]
all systems like actual code like C++

[55:29 - 55:34]
code or go code or JavaScript or

[55:32 - 55:39]
TypeScript

[55:34 - 55:41]
or something else that is not owned by a

[55:39 - 55:44]
specific company that is solving a

[55:41 - 55:47]
problem that is going to be specifically

[55:44 - 55:49]
reliant on their language model. Cuz if

[55:47 - 55:52]
even if your prompt does work really

[55:49 - 55:54]
well for Google, I can pretty much

[55:52 - 55:56]
guarantee you that prompt when you try

[55:54 - 55:59]
to take it and use it on something over

[55:56 - 56:02]
here or over here or over here, your

[55:59 - 56:06]
answers are going to be different. And

[56:02 - 56:06]
here's the real question.

[56:06 - 56:10]
Who does that benefit?

[56:11 - 56:17]
Because it doesn't benefit you.

[56:14 - 56:20]
It's more work for you, but does it

[56:17 - 56:23]
benefit them?

[56:20 - 56:25]
Like, think about this for a second. You

[56:23 - 56:28]
write this prompt and you have all this

[56:25 - 56:30]
stuff that works really well with just

[56:28 - 56:33]
Gemini

[56:30 - 56:33]
or just

[56:34 - 56:40]
co-pilot

[56:36 - 56:42]
or just Claude and just Llama, just

[56:40 - 56:43]
Grock and then you try to move to one of

[56:42 - 56:46]
these other ones and it doesn't work

[56:43 - 56:52]
anymore. What does that do to your life?

[56:46 - 56:54]
Well, I guess you are a single uh vendor

[56:52 - 56:56]
uh customer now. You can only work with

[56:54 - 56:59]
a single vendor. And guess what? They

[56:56 - 57:01]
now have cloud lock on you. You are

[56:59 - 57:03]
they've wanted to do it for a while.

[57:01 - 57:05]
Lock you into their cloud. Now they've

[57:03 - 57:07]
just done it. And they've done it in a

[57:05 - 57:09]
really clever way that's probably not

[57:07 - 57:12]
even something you're thinking about.

[57:09 - 57:15]
You just signed your life away to where

[57:12 - 57:17]
now they are your perpetual cloud

[57:15 - 57:18]
vendor.

[57:17 - 57:21]
I don't think that's good. I don't think

[57:18 - 57:24]
you want to do that. I'm fighting for

[57:21 - 57:27]
you here. I think you want to be able to

[57:24 - 57:31]
use any of these systems.

[57:27 - 57:33]
And you should build solutions that will

[57:31 - 57:37]
work with any of those systems. And it's

[57:33 - 57:39]
not by interfacing directly with them

[57:37 - 57:43]
using their prompt,

[57:39 - 57:45]
whatever. It's probably by building a

[57:43 - 57:48]
layer of abstraction between you and

[57:45 - 57:50]
them and then building different

[57:48 - 57:53]
interfaces that are aligned with working

[57:50 - 57:56]
with their stochastic systems and they

[57:53 - 57:58]
change uh each time but you function at

[57:56 - 57:59]
this level here that does not interface

[57:58 - 58:02]
with them directly. This is computer

[57:59 - 58:05]
science 101 stuff. You're adding a layer

[58:02 - 58:08]
of abstraction to uh abstract away the

[58:05 - 58:10]
specificity that's built into these. Now

[58:08 - 58:15]
the upside of prompt engineering for

[58:10 - 58:16]
them is don't use those abstractions.

[58:15 - 58:18]
Not because you wouldn't build more

[58:16 - 58:20]
resilient software. You would you would

[58:18 - 58:23]
build far more resilient software. If

[58:20 - 58:29]
you don't believe me just like Google

[58:23 - 58:29]
why is programming using abstractions

[58:29 - 58:32]
man

[58:31 - 58:35]
helpful?

[58:32 - 58:36]
Oh,

[58:35 - 58:38]
I was actually going to look at the

[58:36 - 58:40]
inverse. Abstraction of programming

[58:38 - 58:41]
simplifies complex system by focusing on

[58:40 - 58:43]
essential features and hiding

[58:41 - 58:46]
unnecessary details. Making code easier

[58:43 - 58:47]
to understand, manage, and res reuse.

[58:46 - 58:49]
This allows developers to work at a

[58:47 - 58:51]
higher level abstraction, focusing on

[58:49 - 58:54]
what rather than how. Yes, the what

[58:51 - 58:57]
rather than the how. The what is we're

[58:54 - 59:01]
going to try to build

[58:57 - 59:05]
build this thing.

[59:01 - 59:05]
This is the what,

[59:08 - 59:12]
this is the how. Let them figure that

[59:10 - 59:14]
out. You focus on this. And so you're

[59:12 - 59:15]
going to function at this level, not at

[59:14 - 59:18]
this level. And then that's going to

[59:15 - 59:20]
give you a solution that's potentially

[59:18 - 59:22]
portable. This is also why you generally

[59:20 - 59:24]
don't program directly at the operating

[59:22 - 59:27]
system level, because you don't want to

[59:24 - 59:30]
be bound specifically to any OS. You

[59:27 - 59:32]
want it to work on Windows, on Linux, on

[59:30 - 59:35]
Unix, on Mac OS, on all this stuff, on

[59:32 - 59:37]
Android, on iOS. So you program at a

[59:35 - 59:40]
higher order abstraction.

[59:37 - 59:40]
For example,

[59:41 - 59:47]
mentor runs in your browser.

[59:44 - 59:47]
Why?

[59:48 - 59:53]
It's portable.

[59:50 - 59:55]
It's portable. It can run on all these

[59:53 - 59:57]
different places. as long as you have a

[59:55 - 59:59]
browser, that's all that it really cares

[59:57 - 01:00:02]
about. This is sort of not an accident,

[59:59 - 01:00:05]
right? Uh so do the same thing. Don't

[01:00:02 - 01:00:08]
buy into the propaganda

[01:00:05 - 01:00:10]
which is programmed directly against our

[01:00:08 - 01:00:12]
model because the more I think about it,

[01:00:10 - 01:00:14]
the more like at first I just thought

[01:00:12 - 01:00:16]
people were confused. Again, I'm going

[01:00:14 - 01:00:17]
to still try to foster goodwill as much

[01:00:16 - 01:00:19]
as possible. For a while, I thought

[01:00:17 - 01:00:21]
people were just confused. I was like,

[01:00:19 - 01:00:22]
why do you guys keep calling this

[01:00:21 - 01:00:24]
engineering? It's not. It's prompt

[01:00:22 - 01:00:27]
something. And then they just kept doing

[01:00:24 - 01:00:30]
it. And now I'm starting to think that

[01:00:27 - 01:00:32]
I'm the one that's clueless.

[01:00:30 - 01:00:34]
That they knew this whole time that it's

[01:00:32 - 01:00:36]
not engineering. But by selling it like

[01:00:34 - 01:00:38]
engineering that somehow this is like

[01:00:36 - 01:00:40]
some magical thing like look, I'm a

[01:00:38 - 01:00:44]
prompt engineer. I do all this stuff.

[01:00:40 - 01:00:48]
Really, it's giving them a vehicle that

[01:00:44 - 01:00:49]
is allowing uh them to present this as

[01:00:48 - 01:00:51]
like fostering goodwill. Oh yeah, you

[01:00:49 - 01:00:53]
got to be a prompt engineer. and they

[01:00:51 - 01:00:56]
can all rally around it and and really

[01:00:53 - 01:00:58]
what it's doing is it's binding you to

[01:00:56 - 01:01:02]
their specific APIs, their specific

[01:00:58 - 01:01:03]
cloud, which is a big I mean,

[01:01:02 - 01:01:08]
do you notice something that's kind of

[01:01:03 - 01:01:08]
in common here with these guys?

[01:01:08 - 01:01:12]
Google makes a huge amount of money on

[01:01:10 - 01:01:14]
their cloud. So does Microsoft. I think

[01:01:12 - 01:01:17]
IBM not as much. Anthropic, that's

[01:01:14 - 01:01:20]
pretty pretty much their entire income.

[01:01:17 - 01:01:24]
uh AWS uh Meta has got some plans for

[01:01:20 - 01:01:26]
this. I I'm fairly certain. We don't

[01:01:24 - 01:01:29]
generally know I think the n-dimensional

[01:01:26 - 01:01:31]
chess that Zuck is playing at all times,

[01:01:29 - 01:01:33]
but he's playing it. I can pretty much

[01:01:31 - 01:01:39]
guarantee you. And then similar thing is

[01:01:33 - 01:01:42]
happening over in Elon Musk land. So

[01:01:39 - 01:01:44]
the takeaway here

[01:01:42 - 01:01:47]
uh not engineering,

[01:01:44 - 01:01:49]
it's prompt something. And when you do

[01:01:47 - 01:01:53]
this directly at the language model

[01:01:49 - 01:01:57]
level, likely the only one that benefits

[01:01:53 - 01:02:00]
are these guys, not you.

[01:01:57 - 01:02:02]
So maybe don't do that. Okay. I'm going

[01:02:00 - 01:02:04]
to try to do something that I've never

[01:02:02 - 01:02:07]
done before, which is I don't want to

[01:02:04 - 01:02:11]
stop the stream, but I really need a

[01:02:07 - 01:02:13]
short break. Uh bio break. And Uber Eats

[01:02:11 - 01:02:15]
dropped off the dog food that I need to

[01:02:13 - 01:02:17]
go grab.

[01:02:15 - 01:02:19]
and the dog's starving. So, minimally, I

[01:02:17 - 01:02:21]
need to go pick that stuff up. So, I'm

[01:02:19 - 01:02:23]
going to step off stream, but I need I'm

[01:02:21 - 01:02:26]
going to keep this going.

[01:02:23 - 01:02:31]
And I'll leave a comment here saying

[01:02:26 - 01:02:34]
um stepping away for just

[01:02:31 - 01:02:39]
two minutes.

[01:02:34 - 01:02:42]
I'll be right back.

[01:02:39 - 01:02:45]
Please stick around

[01:02:42 - 01:02:49]
and make comments.

[01:02:45 - 01:02:52]
in the chat. Smiley face. All right, I'm

[01:02:49 - 01:02:56]
gonna go do that. Now's be a good time

[01:02:52 - 01:02:59]
to get a drink break, popcorn break, but

[01:02:56 - 01:03:01]
I want to talk more

[01:02:59 - 01:03:03]
specifically about this one and then

[01:03:01 - 01:03:05]
maybe we'll talk about this. Uh I think

[01:03:03 - 01:03:07]
that we've we've beaten the prompt

[01:03:05 - 01:03:10]
engineering topic

[01:03:07 - 01:03:13]
uh sufficiently and I do hope that some

[01:03:10 - 01:03:16]
of my friends like keep in mind I know

[01:03:13 - 01:03:18]
people and I love many of the engineers

[01:03:16 - 01:03:21]
that work at these places and I know you

[01:03:18 - 01:03:25]
guys are super smart and I know that you

[01:03:21 - 01:03:28]
know just like I know that it's not

[01:03:25 - 01:03:30]
engineering. I know you know this. I

[01:03:28 - 01:03:33]
know this isn't news to you guys. you're

[01:03:30 - 01:03:36]
super smart and can you please just

[01:03:33 - 01:03:37]
stop? Like seriously guys, you're you're

[01:03:36 - 01:03:39]
moving us in the wrong direction. You

[01:03:37 - 01:03:42]
know that whole dark ages thing. You're

[01:03:39 - 01:03:44]
moving us to that and I know that you're

[01:03:42 - 01:03:47]
smart enough to know this is not super

[01:03:44 - 01:03:50]
great. So while these decisions may be

[01:03:47 - 01:03:53]
outside of your pay grade, I get that

[01:03:50 - 01:03:54]
you may not be CEO or whatever. you can

[01:03:53 - 01:03:58]
at least float this up to your

[01:03:54 - 01:03:59]
leadership and say, "Yeah, I think that

[01:03:58 - 01:04:01]
we should probably stop calling it

[01:03:59 - 01:04:04]
engineering because people are starting

[01:04:01 - 01:04:07]
to get on to us and it's it's not a good

[01:04:04 - 01:04:10]
look." Because ultimately, it it's going

[01:04:07 - 01:04:12]
to cause a huge amount of brand damage.

[01:04:10 - 01:04:14]
Once people wise up to the fact that

[01:04:12 - 01:04:16]
it's non-engineering and you've been

[01:04:14 - 01:04:17]
pushing all this engineering, they're

[01:04:16 - 01:04:20]
going to find you on all these tweets

[01:04:17 - 01:04:21]
and these ads and all of these things

[01:04:20 - 01:04:22]
and they're going to quote you and

[01:04:21 - 01:04:25]
they're going to re they're going to

[01:04:22 - 01:04:27]
create the reverse content about why

[01:04:25 - 01:04:30]
you're bad actors and you promoted this,

[01:04:27 - 01:04:32]
it's going to happen. So, just stop.

[01:04:30 - 01:04:35]
Right? It's very much like what's

[01:04:32 - 01:04:38]
happened with Deepseek is that Deepseek

[01:04:35 - 01:04:40]
it was clear to me based on things that

[01:04:38 - 01:04:42]
I know that they were doing some

[01:04:40 - 01:04:44]
malicious stuff and I tried to let

[01:04:42 - 01:04:46]
people know some people believed me

[01:04:44 - 01:04:48]
others didn't and now it's very clear

[01:04:46 - 01:04:51]
that they were feeding information

[01:04:48 - 01:04:53]
directly to the CCP. If you don't mind

[01:04:51 - 01:04:55]
your information going to the Chinese

[01:04:53 - 01:04:59]
government,

[01:04:55 - 01:05:02]
cool, have at it. But as a company, if

[01:04:59 - 01:05:05]
you're a company and you're allowing or

[01:05:02 - 01:05:07]
encouraging your employees to use Deep

[01:05:05 - 01:05:08]
Seek,

[01:05:07 - 01:05:11]
then what you're doing is you're

[01:05:08 - 01:05:15]
basically your company is supporting the

[01:05:11 - 01:05:17]
Chinese government. And that may not be

[01:05:15 - 01:05:20]
the best thing to do. Like maybe it is

[01:05:17 - 01:05:22]
the right thing to do, maybe it's not,

[01:05:20 - 01:05:25]
but minimally

[01:05:22 - 01:05:27]
being aware of that and then realizing

[01:05:25 - 01:05:30]
that that's like bad news. If you if

[01:05:27 - 01:05:31]
those people find out later that this is

[01:05:30 - 01:05:33]
what's happening, they're not going to

[01:05:31 - 01:05:36]
be happy. And a lot of people sadly are

[01:05:33 - 01:05:38]
figuring this out later, they're

[01:05:36 - 01:05:40]
figuring out later. And now they're in

[01:05:38 - 01:05:41]
an uproar because they're like not super

[01:05:40 - 01:05:44]
happy about what's going on with

[01:05:41 - 01:05:48]
Deepseek. I'm trying to share with this

[01:05:44 - 01:05:50]
time it's it's it's not the the

[01:05:48 - 01:05:52]
deepseek. I'm trying to share with the

[01:05:50 - 01:05:55]
companies that are promoting this

[01:05:52 - 01:05:57]
language with you to save you some brand

[01:05:55 - 01:06:00]
damage. Because if you keep using this

[01:05:57 - 01:06:03]
term, people are going to associate a

[01:06:00 - 01:06:04]
lot of this bad actor stuff with your

[01:06:03 - 01:06:06]
company when they figure out that this

[01:06:04 - 01:06:08]
is not actual engineering. And they're

[01:06:06 - 01:06:11]
going to be coming for your heads.

[01:06:08 - 01:06:12]
They're going to be coming for you. and

[01:06:11 - 01:06:13]
they're going to be well within their

[01:06:12 - 01:06:15]
right to come for you because you

[01:06:13 - 01:06:18]
promoted it and you knew that it wasn't

[01:06:15 - 01:06:21]
engineering. So the short answer is just

[01:06:18 - 01:06:24]
stop. Just be like we were wrong. Our

[01:06:21 - 01:06:26]
bad. It's not engineering. We thought

[01:06:24 - 01:06:28]
about it some more

[01:06:26 - 01:06:31]
and it's actually too speculative in

[01:06:28 - 01:06:34]
nature and so we are now calling it

[01:06:31 - 01:06:37]
something else. Just just do that. At

[01:06:34 - 01:06:40]
least it'll show some good faith

[01:06:37 - 01:06:43]
and we all make mistakes. So, let's

[01:06:40 - 01:06:45]
foster goodwill. Let's assume that it

[01:06:43 - 01:06:47]
was a mistake that it was an oversight

[01:06:45 - 01:06:49]
that you guys missed. And so, now you

[01:06:47 - 01:06:51]
have the opportunity to course correct.

[01:06:49 - 01:06:54]
But I would recommend you do it fast

[01:06:51 - 01:06:56]
because people are getting wise to this.

[01:06:54 - 01:06:58]
And

[01:06:56 - 01:06:59]
if you continue to let this float

[01:06:58 - 01:07:01]
around,

[01:06:59 - 01:07:03]
yeah, you're going to you're eventually

[01:07:01 - 01:07:06]
going to get to the point to where your

[01:07:03 - 01:07:09]
name I I'm not going to show your name

[01:07:06 - 01:07:12]
even on my channel. And it's not so much

[01:07:09 - 01:07:13]
about me. It's more about I'm not the

[01:07:12 - 01:07:15]
only one that's seeing this. There are

[01:07:13 - 01:07:17]
far smarter people out there that have

[01:07:15 - 01:07:20]
already made these observations and

[01:07:17 - 01:07:22]
they've cut a lot of these big companies

[01:07:20 - 01:07:25]
off their list. And frankly, as these

[01:07:22 - 01:07:27]
big companies, a big part of your job is

[01:07:25 - 01:07:29]
to build

[01:07:27 - 01:07:31]
uh goodwill

[01:07:29 - 01:07:33]
in the community. You want people to

[01:07:31 - 01:07:35]
love your brand, so they'll buy stuff

[01:07:33 - 01:07:39]
from you. But they're not going to buy

[01:07:35 - 01:07:42]
stuff from you if they don't trust you.

[01:07:39 - 01:07:45]
And the fastest way to lose trust is to

[01:07:42 - 01:07:47]
lie to people. Don't lie to them. Just

[01:07:45 - 01:07:50]
be like, "Hey, we kind of screwed this

[01:07:47 - 01:07:53]
up." And our bad. I'm sorry. We're going

[01:07:50 - 01:07:56]
to stop using this. So, just do that.

[01:07:53 - 01:07:59]
Okay. I'll be back. Hopefully, you guys

[01:07:56 - 01:08:03]
stick around and

[01:07:59 - 01:08:03]
we'll continue in a moment.

[01:08:07 - 01:08:10]
Jellybean

[01:08:12 - 01:08:19]
the

[01:08:15 - 01:08:23]
what is this? This is a Debian app code

[01:08:19 - 01:08:23]
on mentor staging.

[01:08:24 - 01:08:32]
Okay. I if that is the code.

[01:08:28 - 01:08:32]
Yeah, I agree. I

[01:08:32 - 01:08:41]
it it's kind of interesting in that.

[01:08:37 - 01:08:43]
This is a little backstory.

[01:08:41 - 01:08:47]
Hold on one second. Let me readjust my

[01:08:43 - 01:08:47]
my seat here.

[01:08:48 - 01:08:54]
Uh a little backstory is

[01:08:51 - 01:08:57]
one of the things that I teach at the

[01:08:54 - 01:09:00]
university is I have this sort of

[01:08:57 - 01:09:03]
experiment where

[01:09:00 - 01:09:08]
the students are asked to look at some

[01:09:03 - 01:09:12]
code and reason about intentionality

[01:09:08 - 01:09:15]
uh not about the so I think they're

[01:09:12 - 01:09:17]
supposed to reason about

[01:09:15 - 01:09:20]
the syntax

[01:09:17 - 01:09:22]
then to reason about the semantics and

[01:09:20 - 01:09:24]
then to reason about the intention. So

[01:09:22 - 01:09:26]
the syntax is basically all the stuff

[01:09:24 - 01:09:31]
that we see directly here and you can

[01:09:26 - 01:09:34]
say this is an std a stood set that

[01:09:31 - 01:09:37]
takes uh ins

[01:09:34 - 01:09:37]
and

[01:09:43 - 01:09:47]
normally I would say this should be

[01:09:45 - 01:09:51]
passed by reference but given that this

[01:09:47 - 01:09:54]
is an execute fork

[01:09:51 - 01:09:57]
um it could actually be that passing it

[01:09:54 - 01:10:00]
by copy is the right thing to do because

[01:09:57 - 01:10:04]
by reference would keep it shared

[01:10:00 - 01:10:06]
in the same shared memory space

[01:10:04 - 01:10:07]
as the original thread and it seems like

[01:10:06 - 01:10:10]
if they're doing a fork that's going to

[01:10:07 - 01:10:12]
create a new primary thread of execution

[01:10:10 - 01:10:15]
and so they'd need to make it a copy.

[01:10:12 - 01:10:18]
Okay. Anyway, the the syntax of this is

[01:10:15 - 01:10:20]
basically that's a stood set int and

[01:10:18 - 01:10:22]
that's basically

[01:10:20 - 01:10:27]
what it is.

[01:10:22 - 01:10:31]
The semantics of this are a stood set is

[01:10:27 - 01:10:35]
an ordered list that in this case is

[01:10:31 - 01:10:38]
going to store just integers and ordered

[01:10:35 - 01:10:41]
sets instead set

[01:10:38 - 01:10:43]
uh have

[01:10:41 - 01:10:49]
a support for only a single singular

[01:10:43 - 01:10:54]
cardality for each number in the set. If

[01:10:49 - 01:10:58]
you wanted more than one uh cardality,

[01:10:54 - 01:11:00]
a non singular cardality,

[01:10:58 - 01:11:01]
you'd make it a multi-set. I think

[01:11:00 - 01:11:04]
something like that. So this is a

[01:11:01 - 01:11:06]
semantics e explanation. So we talked

[01:11:04 - 01:11:09]
about the syntax first. Now we talk

[01:11:06 - 01:11:13]
about the semantics and the semantics

[01:11:09 - 01:11:16]
are explaining the uh what like what is

[01:11:13 - 01:11:18]
it actually doing kind of thing. We

[01:11:16 - 01:11:21]
could also explain the how, like how it

[01:11:18 - 01:11:23]
does this, but the next piece is about

[01:11:21 - 01:11:26]
the intention.

[01:11:23 - 01:11:28]
So, syntax, semantics, intention. And

[01:11:26 - 01:11:30]
the intention is usually the part that's

[01:11:28 - 01:11:31]
like

[01:11:30 - 01:11:33]
you got to read the tea leaves a little

[01:11:31 - 01:11:35]
bit depending on the environment you're

[01:11:33 - 01:11:37]
in.

[01:11:35 - 01:11:39]
And

[01:11:37 - 01:11:41]
uh

[01:11:39 - 01:11:45]
you usually need more dimensions than

[01:11:41 - 01:11:45]
just the

[01:11:45 - 01:11:49]
code that you're looking at. You you

[01:11:47 - 01:11:53]
often need to understand a little bit

[01:11:49 - 01:11:56]
about who created it as well as its

[01:11:53 - 01:11:58]
evolution. And so the students will

[01:11:56 - 01:12:01]
sometimes get stuck on trying to explain

[01:11:58 - 01:12:03]
what the intention is. And that's part

[01:12:01 - 01:12:06]
of the exercise is the point is that

[01:12:03 - 01:12:09]
intentionality can often

[01:12:06 - 01:12:15]
only be kind of like deeply observed if

[01:12:09 - 01:12:17]
you understand who originated the

[01:12:15 - 01:12:20]
code and what they were thinking. So

[01:12:17 - 01:12:21]
you're doing meta reasoning.

[01:12:20 - 01:12:24]
This is I'm I'm doing my typical

[01:12:21 - 01:12:26]
meandering. This wasn't on the list, but

[01:12:24 - 01:12:30]
what the heck? Let's just drop it in

[01:12:26 - 01:12:34]
here. So this is uh syntax

[01:12:30 - 01:12:37]
semantics and intention

[01:12:34 - 01:12:41]
code reasoning

[01:12:37 - 01:12:42]
CS 329M

[01:12:41 - 01:12:45]
uh and the point that I'm trying to make

[01:12:42 - 01:12:47]
in this is that most of the students at

[01:12:45 - 01:12:49]
this because this is grad level mostly

[01:12:47 - 01:12:51]
masters and doctorate so they'll

[01:12:49 - 01:12:52]
understand the syntax part they'll have

[01:12:51 - 01:12:54]
worked a little bit with semantics but

[01:12:52 - 01:12:56]
they likely haven't thought about

[01:12:54 - 01:12:58]
intentionality much at all And the point

[01:12:56 - 01:13:00]
is is to help them see that

[01:12:58 - 01:13:04]
intentionality usually requires two

[01:13:00 - 01:13:07]
things. Uh it requires meta reasoning

[01:13:04 - 01:13:10]
or it requires

[01:13:07 - 01:13:10]
um

[01:13:10 - 01:13:15]
temporal analysis

[01:13:13 - 01:13:17]
or

[01:13:15 - 01:13:19]
time dilation

[01:13:17 - 01:13:23]
uh one of those two things. And I don't

[01:13:19 - 01:13:27]
feel like having uh a 5h hour oration on

[01:13:23 - 01:13:31]
this but the highle landscape of this is

[01:13:27 - 01:13:35]
that when you look at things on code

[01:13:31 - 01:13:38]
related to time dilation. So time

[01:13:35 - 01:13:40]
essentially moving forward or backwards

[01:13:38 - 01:13:42]
much in in a much more controlled way

[01:13:40 - 01:13:44]
than just like it happening. That's

[01:13:42 - 01:13:46]
that's what time dilation essentially

[01:13:44 - 01:13:49]
means. You you can speed it up or slow

[01:13:46 - 01:13:52]
it down. And so time dilation is useful

[01:13:49 - 01:13:56]
for reasoning about code over time

[01:13:52 - 01:13:58]
because you don't want to wait

[01:13:56 - 01:14:00]
24 hours to see the code change. You

[01:13:58 - 01:14:01]
just want to be like move forward to the

[01:14:00 - 01:14:02]
next thing, right? That's why it's

[01:14:01 - 01:14:04]
called time dilation. You're just like

[01:14:02 - 01:14:06]
speeding it up, slowing. Just think

[01:14:04 - 01:14:09]
about like Netflix. You're speeding it

[01:14:06 - 01:14:10]
up and then you're pausing it and then

[01:14:09 - 01:14:13]
you're going like frame by frame. That's

[01:14:10 - 01:14:15]
all time dilation. It's just like a

[01:14:13 - 01:14:17]
fancy phrase to capture a really

[01:14:15 - 01:14:19]
important concept. That concept is

[01:14:17 - 01:14:21]
really important because what happens

[01:14:19 - 01:14:24]
when you look at things, especially code

[01:14:21 - 01:14:26]
over time dilation, they start to give

[01:14:24 - 01:14:29]
you hints about what the intentionality

[01:14:26 - 01:14:31]
is of the system because you can see you

[01:14:29 - 01:14:34]
can start to do this. You can start to

[01:14:31 - 01:14:36]
do meta reasoning on the the thing or

[01:14:34 - 01:14:38]
the person that's writing the code. So

[01:14:36 - 01:14:41]
if we were to look at this through a

[01:14:38 - 01:14:44]
time dilation lens, we might start to

[01:14:41 - 01:14:46]
see this code evolve and pieces get

[01:14:44 - 01:14:50]
added, pieces get removed, and that

[01:14:46 - 01:14:52]
would give us insight not into just the

[01:14:50 - 01:14:56]
semantics, not just what it's doing, but

[01:14:52 - 01:14:58]
what they meant for the code to do

[01:14:56 - 01:15:00]
that it's doing correctly or it's doing

[01:14:58 - 01:15:02]
incorrectly. And truthfully, I think

[01:15:00 - 01:15:03]
this is the future of software. This is

[01:15:02 - 01:15:06]
one of the things that I'm most

[01:15:03 - 01:15:08]
interested in is how to automatically

[01:15:06 - 01:15:12]
learn intentionality

[01:15:08 - 01:15:15]
in code. So then the AI systems or in my

[01:15:12 - 01:15:17]
case the machine programming systems

[01:15:15 - 01:15:19]
that then

[01:15:17 - 01:15:22]
can start to reason about this and

[01:15:19 - 01:15:25]
either automatically correct the code or

[01:15:22 - 01:15:27]
potentially automatically fill in the

[01:15:25 - 01:15:31]
gaps of like features or things that are

[01:15:27 - 01:15:33]
missing from the initial intentionality.

[01:15:31 - 01:15:36]
Yeah, if that's what the programmer

[01:15:33 - 01:15:39]
wants. So,

[01:15:36 - 01:15:39]
uh

[01:15:48 - 01:15:52]
yeah. So, what does this say? The field

[01:15:51 - 01:15:54]
of machine programming is concerned with

[01:15:52 - 01:15:56]
the automation of software development.

[01:15:54 - 01:15:57]
Given the recent advances in software

[01:15:56 - 01:15:59]
algorithms, hardware efficiency and

[01:15:57 - 01:16:01]
capacity and ever increasing

[01:15:59 - 01:16:03]
availability of code data, it is now

[01:16:01 - 01:16:06]
possible to train machines to help

[01:16:03 - 01:16:07]
software to help develop software. In

[01:16:06 - 01:16:09]
this course, we teach students how to

[01:16:07 - 01:16:10]
build real world MP systems to begin a

[01:16:09 - 01:16:12]
high level overview field including

[01:16:10 - 01:16:15]
abbreviated analysis, state-of-the-art

[01:16:12 - 01:16:17]
mele mentor.

[01:16:15 - 01:16:20]
Next, we discuss the foundations of MP

[01:16:17 - 01:16:21]
and key areas of innovation, some of

[01:16:20 - 01:16:25]
which are unique to MP. Part of that is

[01:16:21 - 01:16:29]
time dilation and the hard science

[01:16:25 - 01:16:34]
nature that is code which is different

[01:16:29 - 01:16:37]
than analysis for uh natural languages.

[01:16:34 - 01:16:37]
This is

[01:16:47 - 01:16:55]
this is something that you you'd learn

[01:16:50 - 01:16:57]
if you uh took my course. Uh okay.

[01:16:55 - 01:16:59]
Next we discuss the foundations of MP

[01:16:57 - 01:17:02]
key areas of innovation blah blah. We

[01:16:59 - 01:17:05]
close discussion of current limitations.

[01:17:02 - 01:17:09]
I thought, huh, I thought there was

[01:17:05 - 01:17:12]
something about the the time dilation

[01:17:09 - 01:17:12]
part

[01:17:13 - 01:17:18]
in here.

[01:17:16 - 01:17:20]
Well, I know I I hint at it or

[01:17:18 - 01:17:23]
something. I talk about it somewhere in

[01:17:20 - 01:17:26]
in here.

[01:17:23 - 01:17:28]
Oh. Oh. Oh,

[01:17:26 - 01:17:30]
there we go. There it is. Okay. Fourth

[01:17:28 - 01:17:33]
dimension temporal code and develop

[01:17:30 - 01:17:35]
developer reasoning. Yeah. Okay, this

[01:17:33 - 01:17:39]
this is where I talk about and then I

[01:17:35 - 01:17:39]
also talk

[01:17:39 - 01:17:44]
uh there's something about meta

[01:17:41 - 01:17:44]
reasoning but

[01:17:46 - 01:17:50]
yeah right so here's the the reasoning

[01:17:48 - 01:17:52]
part is we go into analysis

[01:17:50 - 01:17:54]
understanding intention this goes back

[01:17:52 - 01:17:59]
to this mapping

[01:17:54 - 01:18:01]
uh syntax semantics uh intention

[01:17:59 - 01:18:05]
is

[01:18:01 - 01:18:09]
the analysis. Typically code analysis

[01:18:05 - 01:18:12]
has referred to syntax. Understanding

[01:18:09 - 01:18:16]
refers tends to refer to semantics and

[01:18:12 - 01:18:20]
then intention is intention. Uh

[01:18:16 - 01:18:22]
sometimes called comprehension.

[01:18:20 - 01:18:24]
So let okay I'm going to try to be a

[01:18:22 - 01:18:27]
little bit more scientific about this.

[01:18:24 - 01:18:32]
code analysis

[01:18:27 - 01:18:32]
is like syntax

[01:18:32 - 01:18:39]
uh code understanding is sort of like

[01:18:37 - 01:18:42]
semantics. This is this is like high

[01:18:39 - 01:18:46]
level. This isn't meant to be super

[01:18:42 - 01:18:48]
formal precise. So if you're if you're

[01:18:46 - 01:18:50]
trying to create a formal definition in

[01:18:48 - 01:18:52]
your head based on this uh don't do

[01:18:50 - 01:18:56]
that.

[01:18:52 - 01:19:00]
And this is code comprehension

[01:18:56 - 01:19:01]
sometimes related to called intention.

[01:19:00 - 01:19:04]
This is the one that we're still

[01:19:01 - 01:19:07]
defining. So this one's this one's

[01:19:04 - 01:19:10]
pretty formally

[01:19:07 - 01:19:12]
and crisply defined. When we talk about

[01:19:10 - 01:19:15]
code analysis, we're almost always

[01:19:12 - 01:19:16]
talking about uh syntactic reasoning.

[01:19:15 - 01:19:17]
Then when we talk about code

[01:19:16 - 01:19:19]
understanding, we often are talking

[01:19:17 - 01:19:22]
about semantics. And this is the one

[01:19:19 - 01:19:27]
that's like emerging. So this is sort of

[01:19:22 - 01:19:28]
bleeding edge uh computer science and uh

[01:19:27 - 01:19:32]
artificial intelligence and machine

[01:19:28 - 01:19:35]
programming. This is the part where

[01:19:32 - 01:19:37]
we're not totally clear. Like I don't

[01:19:35 - 01:19:42]
know if these are exactly the right

[01:19:37 - 01:19:42]
words, but if you look at

[01:19:45 - 01:19:50]
this paper,

[01:19:47 - 01:19:53]
you'll

[01:19:50 - 01:19:55]
yeah, you'll see that we decided to call

[01:19:53 - 01:19:58]
this uh intention.

[01:19:55 - 01:20:01]
So that was kind of like the best that

[01:19:58 - 01:20:05]
we could come up with at the time.

[01:20:01 - 01:20:09]
I don't know. We'll we'll find out over

[01:20:05 - 01:20:12]
time, I think, if we got this right or

[01:20:09 - 01:20:15]
if we didn't. And the truth is, I care

[01:20:12 - 01:20:17]
less about that being right or wrong.

[01:20:15 - 01:20:19]
But I definitely want us to figure that

[01:20:17 - 01:20:21]
out sooner so then we can fix it if it

[01:20:19 - 01:20:23]
needs to be fixed. Or if we think it's

[01:20:21 - 01:20:25]
right, then we can just lean on that and

[01:20:23 - 01:20:27]
be like, "Yeah, this is right. Let's

[01:20:25 - 01:20:29]
build the next thing. Now, the the cuz

[01:20:27 - 01:20:31]
what I really care about with intention

[01:20:29 - 01:20:34]
is intentionality systems. Systems that

[01:20:31 - 01:20:36]
can reason about intentionality.

[01:20:34 - 01:20:40]
So then they can start to automate

[01:20:36 - 01:20:43]
aspects of systems that don't properly

[01:20:40 - 01:20:45]
capture the intention.

[01:20:43 - 01:20:47]
Going back to this like we look at this

[01:20:45 - 01:20:50]
function and if we deeply understood

[01:20:47 - 01:20:53]
intentionality versus semantics we would

[01:20:50 - 01:20:57]
be able to say okay the semantics are

[01:20:53 - 01:21:00]
this the intention is this other thing

[01:20:57 - 01:21:00]
and

[01:21:04 - 01:21:08]
no not that one.

[01:21:11 - 01:21:16]
Yeah, here like in in this diagram that

[01:21:14 - 01:21:19]
we were talking about last time. Notice

[01:21:16 - 01:21:22]
just as a refresher. So we have this

[01:21:19 - 01:21:24]
this is how we start when we're trying

[01:21:22 - 01:21:28]
to write trying to build a program. So

[01:21:24 - 01:21:29]
we have ideas in our head of this

[01:21:28 - 01:21:31]
program we're going to build and then we

[01:21:29 - 01:21:32]
build the program and then we get this.

[01:21:31 - 01:21:34]
We've got the algorithms, data

[01:21:32 - 01:21:36]
structures, the system implementation.

[01:21:34 - 01:21:38]
This is all code. But then notice

[01:21:36 - 01:21:40]
there's still this intention that's not

[01:21:38 - 01:21:42]
covered. That's the disconnect.

[01:21:40 - 01:21:45]
that I'm talking about. I'm talking

[01:21:42 - 01:21:49]
about the cases where we still have

[01:21:45 - 01:21:53]
these clouds don't cover what we want.

[01:21:49 - 01:21:57]
What we really want is we want clouds

[01:21:53 - 01:21:57]
that are going to do

[01:21:57 - 01:22:04]
this that perfectly

[01:22:01 - 01:22:08]
capture and cover our intention.

[01:22:04 - 01:22:09]
They don't uh

[01:22:08 - 01:22:12]
they don't miss it. They don't miss

[01:22:09 - 01:22:15]
pieces because any missed piece is

[01:22:12 - 01:22:17]
essentially parts where we tried to

[01:22:15 - 01:22:19]
build something and we did build

[01:22:17 - 01:22:21]
something and it did something but it's

[01:22:19 - 01:22:22]
not exactly what we were trying to

[01:22:21 - 01:22:25]
build.

[01:22:22 - 01:22:28]
Does that make sense?

[01:22:25 - 01:22:30]
Hopefully that makes sense.

[01:22:28 - 01:22:34]
Uh, at this point, this is usually where

[01:22:30 - 01:22:35]
I'll yeah, ask the the the students to

[01:22:34 - 01:22:37]
share their thoughts and then they'll

[01:22:35 - 01:22:39]
say, "No, it doesn't make any sense."

[01:22:37 - 01:22:40]
And then I'll I'll explain it some more.

[01:22:39 - 01:22:43]
Or they'll say, "Yeah, yeah, this makes

[01:22:40 - 01:22:46]
sense." And then anyone that's confused,

[01:22:43 - 01:22:48]
I'll have their them turn to their buddy

[01:22:46 - 01:22:52]
and be like, "Have your have your buddy

[01:22:48 - 01:22:54]
explain it." I don't know. Uh, okay.

[01:22:52 - 01:22:57]
So, we'll we can talk about all of this

[01:22:54 - 01:23:00]
stuff later in more detail. I want to go

[01:22:57 - 01:23:03]
back now to

[01:23:00 - 01:23:05]
this. Don't be the agent of your own

[01:23:03 - 01:23:09]
destruction.

[01:23:05 - 01:23:12]
And maybe this will be my marker of

[01:23:09 - 01:23:16]
about an hour and 30 minutes into our

[01:23:12 - 01:23:19]
stream. Maybe I'll create multiple VODs

[01:23:16 - 01:23:19]
or something.

[01:23:21 - 01:23:26]
So, I'm intentionally putting in a

[01:23:23 - 01:23:29]
silence gap so I have a good cut point.

[01:23:26 - 01:23:32]
Okay, I have it. Now, now I want to talk

[01:23:29 - 01:23:34]
a little bit about not being the agent

[01:23:32 - 01:23:37]
of your own destruction. So, if you

[01:23:34 - 01:23:39]
checked out mentally

[01:23:37 - 01:23:43]
because you're like, "All this coding

[01:23:39 - 01:23:45]
stuff I don't really care about. Uh, I

[01:23:43 - 01:23:46]
want to know about I want to hear life

[01:23:45 - 01:23:49]
advice."

[01:23:46 - 01:23:51]
uh you should tune back in because this

[01:23:49 - 01:23:53]
is going to be more about life advice.

[01:23:51 - 01:23:55]
And I wanted to share this with all of

[01:23:53 - 01:23:57]
you because a lot of the reason why I

[01:23:55 - 01:24:00]
talk about some of these things is that

[01:23:57 - 01:24:04]
I've made these mistakes myself, right?

[01:24:00 - 01:24:07]
Like we've we've talked about how

[01:24:04 - 01:24:11]
was it like this?

[01:24:07 - 01:24:11]
Is it this lecture?

[01:24:11 - 01:24:17]
I'm trying to find the one where I have

[01:24:13 - 01:24:20]
the general life advice.

[01:24:17 - 01:24:22]
I think it's this one.

[01:24:20 - 01:24:25]
I accidentally closed some. Okay. Right.

[01:24:22 - 01:24:28]
So, we've got some general life advice.

[01:24:25 - 01:24:31]
Almost all these things

[01:24:28 - 01:24:33]
are things that I've experienced in my

[01:24:31 - 01:24:36]
life and I've made mistakes and I need

[01:24:33 - 01:24:38]
to course correct. And so then I've I've

[01:24:36 - 01:24:40]
started to write these things down. And

[01:24:38 - 01:24:43]
then many of you know this kind of got

[01:24:40 - 01:24:44]
inspired by my research lab. And then I

[01:24:43 - 01:24:46]
started teaching at the university and

[01:24:44 - 01:24:48]
now I talk about it on GTT. I, by the

[01:24:46 - 01:24:51]
way, I'm going to revise these slides

[01:24:48 - 01:24:55]
and make new slides that are just this.

[01:24:51 - 01:24:56]
I just haven't had time to do that.

[01:24:55 - 01:24:58]
Okay, that's not true. I have had time

[01:24:56 - 01:25:00]
to do this. There's just been other

[01:24:58 - 01:25:04]
things that are higher priority. That's

[01:25:00 - 01:25:06]
why I haven't done it. Okay. Now,

[01:25:04 - 01:25:09]
interestingly enough, this agent of your

[01:25:06 - 01:25:11]
own destruction is not on this list, but

[01:25:09 - 01:25:13]
it's going to be on this list now

[01:25:11 - 01:25:19]
because I've seen this too many times

[01:25:13 - 01:25:22]
now. And it's interesting because

[01:25:19 - 01:25:24]
like I think that I

[01:25:22 - 01:25:27]
have been falling victim to this at

[01:25:24 - 01:25:30]
least a little bit in my life.

[01:25:27 - 01:25:34]
But it's it's something that at least

[01:25:30 - 01:25:37]
for me I haven't had to struggle with

[01:25:34 - 01:25:39]
too much cuz I don't think so. Which is

[01:25:37 - 01:25:41]
probably the reason why I didn't write

[01:25:39 - 01:25:45]
this down. Like fostering goodwill was

[01:25:41 - 01:25:48]
really hard for me. I really had to stop

[01:25:45 - 01:25:51]
being snarky and cynical and sarcastic

[01:25:48 - 01:25:53]
and it just I I really needed to it to

[01:25:51 - 01:25:55]
be written down and to think about this

[01:25:53 - 01:25:57]
quite a bit

[01:25:55 - 01:25:58]
which is why it's written down. But this

[01:25:57 - 01:26:00]
not being the agent of your own

[01:25:58 - 01:26:01]
destruction.

[01:26:00 - 01:26:03]
There's a couple times in my life where

[01:26:01 - 01:26:05]
I did this and then I learned very

[01:26:03 - 01:26:08]
quickly like don't do this and then I

[01:26:05 - 01:26:11]
just stopped doing it. But I wanted to

[01:26:08 - 01:26:14]
mention this because I keep seeing this

[01:26:11 - 01:26:17]
in real life.

[01:26:14 - 01:26:22]
And it's my hope that maybe someone on

[01:26:17 - 01:26:25]
GTT is experiencing this and maybe this

[01:26:22 - 01:26:28]
will reach even just one person. Maybe

[01:26:25 - 01:26:32]
you're that person. Maybe you know

[01:26:28 - 01:26:34]
someone that has this problem and you

[01:26:32 - 01:26:36]
can help them. So here's what I mean

[01:26:34 - 01:26:38]
about this. Don't be the agent of your

[01:26:36 - 01:26:41]
own destruction.

[01:26:38 - 01:26:47]
There are things in life that happen to

[01:26:41 - 01:26:49]
us that we kind of have gotten ourselves

[01:26:47 - 01:26:52]
into.

[01:26:49 - 01:26:58]
It's sort of our own fault.

[01:26:52 - 01:26:58]
And in many cases, we know,

[01:27:02 - 01:27:07]
sorry, I got uh distracted by rereading

[01:27:05 - 01:27:09]
Jellybean's comment. Um, in many cases

[01:27:07 - 01:27:11]
when we get into these hard situations,

[01:27:09 - 01:27:14]
we know

[01:27:11 - 01:27:18]
that we're doing the wrong thing, that

[01:27:14 - 01:27:18]
we've kind of messed up.

[01:27:19 - 01:27:23]
Now, those of us who are wise,

[01:27:22 - 01:27:25]
I think we acknowledge this and we say,

[01:27:23 - 01:27:27]
"Yeah, I messed up. I'm going to learn

[01:27:25 - 01:27:31]
my lesson and not do that again." Right?

[01:27:27 - 01:27:33]
So, I started rough housing with the dog

[01:27:31 - 01:27:36]
and I was doing that with the dog and it

[01:27:33 - 01:27:38]
was getting uh rough with me. And then I

[01:27:36 - 01:27:40]
put my face right next to the dog and

[01:27:38 - 01:27:44]
then the dog bit me on the face. Who's

[01:27:40 - 01:27:46]
to blame here? Is the dog to blame or

[01:27:44 - 01:27:49]
are you to blame? I think you're to

[01:27:46 - 01:27:53]
blame. You know, my my view is I've seen

[01:27:49 - 01:27:55]
people do this. I I I know real stories

[01:27:53 - 01:27:59]
of this happening in real life and then

[01:27:55 - 01:28:00]
people blaming the dog and it's like

[01:27:59 - 01:28:03]
what?

[01:28:00 - 01:28:06]
Like but

[01:28:03 - 01:28:08]
like you why was your face right next to

[01:28:06 - 01:28:12]
the dog's mouth and you're roughousing

[01:28:08 - 01:28:15]
it? Like you it feels like you were the

[01:28:12 - 01:28:19]
architect of your own demise. So, this

[01:28:15 - 01:28:19]
is what I'm talking about. Now,

[01:28:20 - 01:28:23]
it can be way worse than just getting

[01:28:21 - 01:28:28]
bitten by a dog. This can be like career

[01:28:23 - 01:28:30]
ending. This can this can destroy

[01:28:28 - 01:28:32]
everything

[01:28:30 - 01:28:35]
if you don't pay attention and you

[01:28:32 - 01:28:39]
you're not careful. So, you'll hear a

[01:28:35 - 01:28:41]
lot about these uh executives in big

[01:28:39 - 01:28:44]
tech that end up being on the wrong side

[01:28:41 - 01:28:46]
of like the MeToo movement that they've

[01:28:44 - 01:28:48]
done some really uh just not great

[01:28:46 - 01:28:51]
things. And I think that's another great

[01:28:48 - 01:28:53]
example is here you have people that

[01:28:51 - 01:28:56]
potentially have dedicated their lives

[01:28:53 - 01:28:58]
in probably a mostly upright fashion.

[01:28:56 - 01:29:00]
I'm speculating. Again, I'm fostering

[01:28:58 - 01:29:02]
goodwill, so I'm just going to pretend

[01:29:00 - 01:29:05]
that they're doing good things up till

[01:29:02 - 01:29:07]
that point. Maybe I'm wrong. That's, you

[01:29:05 - 01:29:09]
know, that that's entirely possible, but

[01:29:07 - 01:29:12]
I'm going to pretend like they're good

[01:29:09 - 01:29:14]
citizens, but then they end up doing one

[01:29:12 - 01:29:16]
thing

[01:29:14 - 01:29:20]
or a few things and it destroys

[01:29:16 - 01:29:23]
everything. So, they end up uh doing bad

[01:29:20 - 01:29:26]
things with the intern or they send

[01:29:23 - 01:29:27]
photos they shouldn't be sending or this

[01:29:26 - 01:29:29]
kind of thing. They're asking for stuff

[01:29:27 - 01:29:32]
they shouldn't be asking for and then

[01:29:29 - 01:29:34]
their whole career, their whole life is

[01:29:32 - 01:29:36]
cooked and they have really no one to

[01:29:34 - 01:29:38]
blame but themselves. Is that what

[01:29:36 - 01:29:40]
happens I think in many of these cases

[01:29:38 - 01:29:44]
is people they get this like

[01:29:40 - 01:29:48]
complacency. They get this they forget

[01:29:44 - 01:29:51]
that like I've built this much. I've

[01:29:48 - 01:29:55]
worked this hard to create this space

[01:29:51 - 01:29:57]
that I'm in and it's incredible.

[01:29:55 - 01:30:01]
and instead

[01:29:57 - 01:30:03]
they get pulled into this

[01:30:01 - 01:30:05]
imp impulsiveness.

[01:30:03 - 01:30:07]
They're overwhelmed by the moment and

[01:30:05 - 01:30:11]
then they make a really really bad

[01:30:07 - 01:30:14]
decision and all it takes is just one.

[01:30:11 - 01:30:15]
Now, some people

[01:30:14 - 01:30:18]
will be the agents of their own

[01:30:15 - 01:30:19]
destruction, but it'll be micro steps.

[01:30:18 - 01:30:21]
They won't do something super salacious

[01:30:19 - 01:30:23]
like the Me Too movement where it's

[01:30:21 - 01:30:26]
like, "Wow, you did this super bad

[01:30:23 - 01:30:28]
thing." It will be progressive. It will

[01:30:26 - 01:30:34]
be little things that they will chip

[01:30:28 - 01:30:37]
away at their own future simply by

[01:30:34 - 01:30:40]
like not caring enough,

[01:30:37 - 01:30:44]
not trying hard enough. Like here's one

[01:30:40 - 01:30:48]
I think very good example

[01:30:44 - 01:30:48]
uh in this list.

[01:30:48 - 01:30:52]
I think there's something about

[01:30:56 - 01:31:02]
being positive.

[01:30:59 - 01:31:02]
I thought there was

[01:31:10 - 01:31:13]
Okay, maybe not.

[01:31:15 - 01:31:22]
But there should be I'll I'll add that

[01:31:18 - 01:31:24]
to my list if it's not on here.

[01:31:22 - 01:31:25]
But what you wouldn't see on this list

[01:31:24 - 01:31:28]
is

[01:31:25 - 01:31:30]
be negative. Be negative about things.

[01:31:28 - 01:31:33]
And the reason why that won't show up on

[01:31:30 - 01:31:35]
this list is that

[01:31:33 - 01:31:37]
it's been my experience and there's

[01:31:35 - 01:31:40]
actually a lot of sort of uh

[01:31:37 - 01:31:42]
psychological studies that have shown

[01:31:40 - 01:31:44]
this that

[01:31:42 - 01:31:48]
negativity

[01:31:44 - 01:31:52]
can not only erode your own ability to

[01:31:48 - 01:31:54]
do things. So your own desire to get

[01:31:52 - 01:31:58]
something done like create the spot or

[01:31:54 - 01:32:01]
do your homework or go get groceries or

[01:31:58 - 01:32:05]
do the laundry, it can decipher that,

[01:32:01 - 01:32:09]
but it can act like a like a virus. It

[01:32:05 - 01:32:11]
can infect other people. It's

[01:32:09 - 01:32:14]
it's contagious

[01:32:11 - 01:32:18]
in some capacity. What I've seen some

[01:32:14 - 01:32:23]
people do is they allow negativity to

[01:32:18 - 01:32:25]
creep its way in to their lives and it

[01:32:23 - 01:32:27]
rather than becoming like what's

[01:32:25 - 01:32:30]
possible, it's simply uh that won't

[01:32:27 - 01:32:31]
work. That's not possible. We're not

[01:32:30 - 01:32:34]
going to do this. And that's sort of

[01:32:31 - 01:32:38]
like their get out of jail free card for

[01:32:34 - 01:32:41]
everything is they're just constantly

[01:32:38 - 01:32:44]
uh Debbie Downer or the person that's

[01:32:41 - 01:32:46]
super negative for any idea. Now, this

[01:32:44 - 01:32:48]
doesn't mean you should be like have

[01:32:46 - 01:32:50]
toxic positivity or whatever the heck

[01:32:48 - 01:32:51]
that means where you're just like yes to

[01:32:50 - 01:32:55]
everything. I think you can still have a

[01:32:51 - 01:32:58]
critical uh thought process, but you

[01:32:55 - 01:33:01]
don't need to immediately shoot things

[01:32:58 - 01:33:03]
down. Now, it doesn't have to be just

[01:33:01 - 01:33:05]
negativity. It could be other things.

[01:33:03 - 01:33:08]
And one of the other things that I want

[01:33:05 - 01:33:11]
to share that is a huge one that I see

[01:33:08 - 01:33:13]
people make this mistake on as far as

[01:33:11 - 01:33:16]
being agents of their own destruction

[01:33:13 - 01:33:18]
has to do with

[01:33:16 - 01:33:20]
this piece

[01:33:18 - 01:33:21]
is that they are not careful about their

[01:33:20 - 01:33:25]
inner circle. They choose their inner

[01:33:21 - 01:33:29]
circle very poorly. And this is one of

[01:33:25 - 01:33:33]
the fastest ways to self-sabotage

[01:33:29 - 01:33:35]
because all it takes is one person in

[01:33:33 - 01:33:39]
your inner circle that's not a good

[01:33:35 - 01:33:42]
actor to bring the whole thing down.

[01:33:39 - 01:33:44]
Like think about it like how hard it is

[01:33:42 - 01:33:47]
to build something. How hard it is to

[01:33:44 - 01:33:50]
build a house or to build a skyscraper

[01:33:47 - 01:33:53]
or whatever. You got this house. Super

[01:33:50 - 01:33:56]
hard to build. Takes an entire team. It

[01:33:53 - 01:33:58]
just takes one match and one person to

[01:33:56 - 01:34:02]
burn that whole thing to the ground.

[01:33:58 - 01:34:05]
It's very easy to destroy things. It's

[01:34:02 - 01:34:08]
very hard to build things. That's why in

[01:34:05 - 01:34:10]
some sense I don't have like an insane

[01:34:08 - 01:34:12]
amount of respect for the people that

[01:34:10 - 01:34:16]
just spend their time hacking.

[01:34:12 - 01:34:19]
And I am I know

[01:34:16 - 01:34:22]
people that like work in security and

[01:34:19 - 01:34:27]
stuff and just they're sort of paid good

[01:34:22 - 01:34:29]
guys to be on the red team. And I mean

[01:34:27 - 01:34:33]
like that's cool, but I have more

[01:34:29 - 01:34:36]
respect for the blue team uh or the the

[01:34:33 - 01:34:38]
guys that are essentially

[01:34:36 - 01:34:40]
building the stuff and trying to harden

[01:34:38 - 01:34:42]
it because I in my view is much harder.

[01:34:40 - 01:34:44]
It's much harder to build the house that

[01:34:42 - 01:34:47]
can't be burned down than to just walk

[01:34:44 - 01:34:50]
around and burn houses down.

[01:34:47 - 01:34:53]
So with this in mind though, you can be

[01:34:50 - 01:34:54]
the best person in the world. You can be

[01:34:53 - 01:34:58]
super amazing, but if you choose your

[01:34:54 - 01:35:00]
inner circle poorly,

[01:34:58 - 01:35:03]
it doesn't matter how awesome you are

[01:35:00 - 01:35:06]
cuz these people will come in and they

[01:35:03 - 01:35:10]
can send you in the wrong direction.

[01:35:06 - 01:35:12]
Now, I'm not talking about evicting

[01:35:10 - 01:35:14]
everyone from your inner circle and just

[01:35:12 - 01:35:16]
doing it casually and just you you stop

[01:35:14 - 01:35:20]
watching this VOD and you're like,

[01:35:16 - 01:35:22]
"Okay, Goju says it's time to evict

[01:35:20 - 01:35:24]
people from inner circles. I'm cleaning

[01:35:22 - 01:35:26]
mine out." Don't do that. Don't do that.

[01:35:24 - 01:35:28]
That That's not at all what I'm saying.

[01:35:26 - 01:35:30]
Please don't do that. What I am saying

[01:35:28 - 01:35:32]
though is that if you have someone in

[01:35:30 - 01:35:34]
your inner circle

[01:35:32 - 01:35:37]
that is acting in a way that doesn't

[01:35:34 - 01:35:40]
match the things that you're trying to

[01:35:37 - 01:35:42]
do in life, talk with this person. This

[01:35:40 - 01:35:44]
is my recommendation. I try to talk with

[01:35:42 - 01:35:47]
these these folks. Try to explain what

[01:35:44 - 01:35:50]
it is I'm doing and I try to explain

[01:35:47 - 01:35:54]
through concrete evidence being data

[01:35:50 - 01:35:56]
driven driven by data not by emotion. So

[01:35:54 - 01:35:58]
there is something in here. Yeah. here.

[01:35:56 - 01:36:00]
Don't let your emotions control you. So,

[01:35:58 - 01:36:04]
these conversations are never heated.

[01:36:00 - 01:36:07]
I'm never angry at the person or sad or

[01:36:04 - 01:36:08]
I feel victimized. It's more just this

[01:36:07 - 01:36:11]
is not going the way that I think it

[01:36:08 - 01:36:16]
should be. And it's very much

[01:36:11 - 01:36:18]
being it's it's being based off of data.

[01:36:16 - 01:36:20]
It's here's what happened. Here's what

[01:36:18 - 01:36:22]
you did. And this doesn't align with

[01:36:20 - 01:36:23]
kind of the way that I think about these

[01:36:22 - 01:36:26]
things, the person that I'm trying to

[01:36:23 - 01:36:28]
be. And then you try to figure out the

[01:36:26 - 01:36:31]
path forward. Maybe the path forward is

[01:36:28 - 01:36:34]
this person just needs to be a better

[01:36:31 - 01:36:36]
actor and they take ownership of their

[01:36:34 - 01:36:41]
problem. That's great. I think that's

[01:36:36 - 01:36:43]
awesome. But maybe it's that they have

[01:36:41 - 01:36:46]
no accountability, right? They're not

[01:36:43 - 01:36:48]
responsible and you're at fault. And you

[01:36:46 - 01:36:51]
know kind of that you're doing the right

[01:36:48 - 01:36:53]
thing. You know that you haven't made a

[01:36:51 - 01:36:54]
misstep. And again, this is going to

[01:36:53 - 01:36:56]
require you to be introspective and

[01:36:54 - 01:37:00]
honest because if you did make a

[01:36:56 - 01:37:03]
misstep, then you should own that. Don't

[01:37:00 - 01:37:06]
try to place it on someone else and just

[01:37:03 - 01:37:10]
make yourself better. Don't use this as

[01:37:06 - 01:37:11]
a opportunity to, I think, belittle and

[01:37:10 - 01:37:13]
pass the buck to someone else. But in

[01:37:11 - 01:37:15]
the case where you have people that are

[01:37:13 - 01:37:18]
like not doing what they're supposed to

[01:37:15 - 01:37:20]
be doing and you have evidence of this,

[01:37:18 - 01:37:22]
I think this could be very very useful

[01:37:20 - 01:37:24]
to talk to people about because I think

[01:37:22 - 01:37:28]
it can help them grow.

[01:37:24 - 01:37:29]
On top of that, if it turns out that

[01:37:28 - 01:37:33]
this is intentional or the person won't

[01:37:29 - 01:37:36]
own it and you see this enough times,

[01:37:33 - 01:37:38]
my view is that I don't know what enough

[01:37:36 - 01:37:42]
times is. It's going to be different for

[01:37:38 - 01:37:45]
everybody, I think, in in some capacity.

[01:37:42 - 01:37:45]
But I think that

[01:37:48 - 01:37:53]
I think that

[01:37:51 - 01:37:56]
once you reach your threshold, whatever

[01:37:53 - 01:37:56]
that is,

[01:37:56 - 01:38:01]
and you realize that this is enough,

[01:38:00 - 01:38:03]
then you should remove that person from

[01:38:01 - 01:38:05]
your inner circle.

[01:38:03 - 01:38:07]
And this is super important because if

[01:38:05 - 01:38:09]
you don't,

[01:38:07 - 01:38:11]
this is my strong conviction, you could

[01:38:09 - 01:38:13]
be sending yourself down this path of

[01:38:11 - 01:38:15]
becoming the agent of your own

[01:38:13 - 01:38:17]
destruction. And it's one of those

[01:38:15 - 01:38:20]
degrees of separation things that we

[01:38:17 - 01:38:22]
talked about, right? It's one of those

[01:38:20 - 01:38:24]
things where

[01:38:22 - 01:38:26]
you

[01:38:24 - 01:38:28]
are here

[01:38:26 - 01:38:30]
and you can see out here that this is

[01:38:28 - 01:38:33]
your inner circle and this person is

[01:38:30 - 01:38:36]
doing this and it's going to come back

[01:38:33 - 01:38:38]
and affect you. If you believe that this

[01:38:36 - 01:38:40]
person is so far out that you're not

[01:38:38 - 01:38:42]
going to be affected, I think that

[01:38:40 - 01:38:44]
there's a good chance you're probably

[01:38:42 - 01:38:48]
wrong if that person is your in your

[01:38:44 - 01:38:51]
inner circle because out here it's doing

[01:38:48 - 01:38:54]
all kinds of things. It's talking to

[01:38:51 - 01:38:56]
other people. It's planning. It's doing

[01:38:54 - 01:38:59]
stuff that's going to come and have this

[01:38:56 - 01:39:01]
rebound effect to hit you. So even if

[01:38:59 - 01:39:03]
you're doing all the right stuff, this

[01:39:01 - 01:39:06]
is an autonomous

[01:39:03 - 01:39:10]
agent that is acting on its own free

[01:39:06 - 01:39:12]
will or like what we believe. And if you

[01:39:10 - 01:39:14]
believe this person is acting in bad

[01:39:12 - 01:39:15]
faith

[01:39:14 - 01:39:18]
based on the things they're doing with

[01:39:15 - 01:39:18]
you,

[01:39:18 - 01:39:24]
how are they not acting in bad faith

[01:39:21 - 01:39:27]
with things that are not related to you?

[01:39:24 - 01:39:30]
like it's related to the people that you

[01:39:27 - 01:39:33]
know, your friends, maybe people that

[01:39:30 - 01:39:35]
are not your friends. Like there's and

[01:39:33 - 01:39:37]
again I'm I'm I'm not trying to

[01:39:35 - 01:39:40]
speculate and paint people as a bad

[01:39:37 - 01:39:42]
person. I'm simply following this logic

[01:39:40 - 01:39:44]
of when people show you who they are and

[01:39:42 - 01:39:48]
you have enough evidence. It's not for

[01:39:44 - 01:39:50]
me personally, it's usually there are at

[01:39:48 - 01:39:53]
least probably let's just say five

[01:39:50 - 01:39:57]
cases. So So let's do it like this.

[01:39:53 - 01:40:01]
Let's say I have uh a student

[01:39:57 - 01:40:01]
at the university that takes my course

[01:40:02 - 01:40:08]
and

[01:40:05 - 01:40:10]
they miss the deadline for one homework

[01:40:08 - 01:40:13]
assignment.

[01:40:10 - 01:40:15]
Not a big deal. I generally don't allow

[01:40:13 - 01:40:17]
late homework, but if it's a one-time

[01:40:15 - 01:40:18]
thing, I I'll probably forgive them and

[01:40:17 - 01:40:19]
be like, "Okay, I'll let it slide this

[01:40:18 - 01:40:23]
one time, but don't let it happen

[01:40:19 - 01:40:25]
again." Then it happens a second time.

[01:40:23 - 01:40:27]
And by all means, there's generally

[01:40:25 - 01:40:30]
going to be good excuses. So now we have

[01:40:27 - 01:40:33]
two. By the time it gets to five, if it

[01:40:30 - 01:40:35]
gets to five, I have enough data of the

[01:40:33 - 01:40:38]
same type of offense

[01:40:35 - 01:40:42]
that this is like this is not an

[01:40:38 - 01:40:45]
accident anymore. It's a habit.

[01:40:42 - 01:40:48]
Not an accident. It's a habit.

[01:40:45 - 01:40:50]
And people that have bad habits are not

[01:40:48 - 01:40:51]
people that I generally keep in my inner

[01:40:50 - 01:40:54]
circle. Again, I'm not trying to be

[01:40:51 - 01:40:57]
self-righteous here, so I apologize if

[01:40:54 - 01:40:59]
it comes off that way. What I found the

[01:40:57 - 01:41:01]
hard way is that when you have people

[01:40:59 - 01:41:03]
that have bad habits,

[01:41:01 - 01:41:05]
no matter how much we love those people,

[01:41:03 - 01:41:08]
if those people are deeply inside of our

[01:41:05 - 01:41:11]
inner circle, the problem is they rub

[01:41:08 - 01:41:14]
off on us.

[01:41:11 - 01:41:19]
They rub off on you. And now that bad

[01:41:14 - 01:41:23]
habit that was just their bad habit

[01:41:19 - 01:41:26]
is now your bad habit. It's yours. You

[01:41:23 - 01:41:28]
have it, too. I mean, one of the saddest

[01:41:26 - 01:41:32]
stories that I ever hear about is you'll

[01:41:28 - 01:41:34]
have some really amazing person that's

[01:41:32 - 01:41:36]
got an incredible future ahead of them

[01:41:34 - 01:41:39]
and then you hear that they have like

[01:41:36 - 01:41:40]
overdosed on drugs and you're like, I

[01:41:39 - 01:41:42]
don't understand. This person like

[01:41:40 - 01:41:44]
didn't even do drugs. And you were

[01:41:42 - 01:41:47]
right. They they don't do drugs. By the

[01:41:44 - 01:41:50]
way, I'm not saying don't do drugs. I'm

[01:41:47 - 01:41:52]
just let let me walk down this path with

[01:41:50 - 01:41:54]
me for a moment. Then what you find out

[01:41:52 - 01:41:57]
is that this person got a partner and

[01:41:54 - 01:41:59]
the partner that they connected with is

[01:41:57 - 01:42:01]
someone that is a heavy drug user and

[01:41:59 - 01:42:03]
they're using some really advanced

[01:42:01 - 01:42:06]
things like stuff that you're not

[01:42:03 - 01:42:08]
getting at uh the Circle K or the

[01:42:06 - 01:42:12]
7-Eleven, right? or off of Amazon or

[01:42:08 - 01:42:14]
even like the the gray web like this is

[01:42:12 - 01:42:17]
full dark web kind of stuff and they're

[01:42:14 - 01:42:19]
like leaning in on it and then they

[01:42:17 - 01:42:22]
bring their partner in just to dabble

[01:42:19 - 01:42:24]
but they have a built-in tolerance to

[01:42:22 - 01:42:26]
all these things but their partner

[01:42:24 - 01:42:29]
doesn't so they don't get the dosage

[01:42:26 - 01:42:32]
right and then your friend does one does

[01:42:29 - 01:42:34]
this thing one time and they're cooked

[01:42:32 - 01:42:36]
and it's game over and that's why you

[01:42:34 - 01:42:39]
didn't know about it because they don't

[01:42:36 - 01:42:41]
have an addiction into drugs. They just

[01:42:39 - 01:42:43]
were following this cuz that was their

[01:42:41 - 01:42:47]
partner and their partner convinced them

[01:42:43 - 01:42:50]
to do this and then they ended up not

[01:42:47 - 01:42:53]
surviving or or potentially worse.

[01:42:50 - 01:42:56]
They're now lifelong addicts. And this

[01:42:53 - 01:42:58]
may sound super derived. This may sound

[01:42:56 - 01:43:02]
super contrived that this doesn't happen

[01:42:58 - 01:43:06]
in real life. Goju, it does though. This

[01:43:02 - 01:43:09]
happens in real life all the time. I

[01:43:06 - 01:43:12]
have seen this in my own life. Not not

[01:43:09 - 01:43:15]
me personally. I have friends that have

[01:43:12 - 01:43:17]
ended up dating people that have done

[01:43:15 - 01:43:21]
this and they've gone in really bad

[01:43:17 - 01:43:21]
directions to where

[01:43:22 - 01:43:26]
we don't even need to go where where

[01:43:24 - 01:43:28]
they've gone. It's not a super great

[01:43:26 - 01:43:29]
place.

[01:43:28 - 01:43:31]
So

[01:43:29 - 01:43:33]
now you can take this concept of inner

[01:43:31 - 01:43:36]
circle.

[01:43:33 - 01:43:39]
You can take this concept of

[01:43:36 - 01:43:41]
doing really bad things and you can take

[01:43:39 - 01:43:43]
this in all these other directions. It

[01:43:41 - 01:43:46]
can be your own ego. It can be that you

[01:43:43 - 01:43:49]
hold grudges. Uh any of these things,

[01:43:46 - 01:43:52]
right? Uh it could be that you race to

[01:43:49 - 01:43:55]
the end. Most of these things here are

[01:43:52 - 01:43:57]
kind of meant to

[01:43:55 - 01:43:59]
at least from my perspective, the goal

[01:43:57 - 01:44:01]
that I have is these are meant to help

[01:43:59 - 01:44:05]
you become better.

[01:44:01 - 01:44:08]
This one is to stop you from being

[01:44:05 - 01:44:09]
worse,

[01:44:08 - 01:44:13]
which is part of the reason why it like

[01:44:09 - 01:44:14]
kind of isn't really on this.

[01:44:13 - 01:44:16]
But maybe it should be because don't

[01:44:14 - 01:44:19]
hold grudges.

[01:44:16 - 01:44:21]
Well, yeah. I I think that

[01:44:19 - 01:44:24]
Yeah, I'm not entirely sure my logic

[01:44:21 - 01:44:26]
holds up.

[01:44:24 - 01:44:28]
This should be on here. I think this is

[01:44:26 - 01:44:31]
more just oversight.

[01:44:28 - 01:44:36]
O oversight on my part. is that I I

[01:44:31 - 01:44:36]
missed it. I

[01:44:39 - 01:44:42]
I should have had it on the list and

[01:44:40 - 01:44:44]
it'll it'll go on the list. Well,

[01:44:42 - 01:44:46]
basically anything that you see here

[01:44:44 - 01:44:48]
could be an agent of your own

[01:44:46 - 01:44:51]
destruction. It could be that you hold

[01:44:48 - 01:44:53]
grudges and you know you shouldn't and

[01:44:51 - 01:44:54]
then you eventually hold grudges with

[01:44:53 - 01:44:56]
someone else that hold grudges and then

[01:44:54 - 01:44:58]
you guys having this super grudge match

[01:44:56 - 01:45:01]
and then it destroys both of you. Don't

[01:44:58 - 01:45:02]
be the agent of your own destruction. It

[01:45:01 - 01:45:04]
could be that you don't foster goodwill,

[01:45:02 - 01:45:06]
but you know you should. And instead,

[01:45:04 - 01:45:08]
you think really bad things about people

[01:45:06 - 01:45:10]
and then you go down this path and you

[01:45:08 - 01:45:11]
make assumptions and you try to like

[01:45:10 - 01:45:12]
burn somebody, but it turns out that

[01:45:11 - 01:45:14]
they were actually doing the right thing

[01:45:12 - 01:45:16]
and then you're the bad guy and now

[01:45:14 - 01:45:19]
you've destroyed yourself, right? Could

[01:45:16 - 01:45:21]
be the inner circle. Any one of these, I

[01:45:19 - 01:45:24]
think, could be the agent of your

[01:45:21 - 01:45:27]
destruction. And so my request to all of

[01:45:24 - 01:45:29]
you or my guidance to all of you is

[01:45:27 - 01:45:30]
generally speaking as I was mentioning

[01:45:29 - 01:45:33]
before

[01:45:30 - 01:45:36]
we often know

[01:45:33 - 01:45:40]
when we're doing the wrong thing.

[01:45:36 - 01:45:40]
You know that like well

[01:45:41 - 01:45:47]
I should have been here at 8 for work

[01:45:44 - 01:45:49]
and I got in at 9. Not a big deal. And

[01:45:47 - 01:45:51]
then somehow it becomes a habit and then

[01:45:49 - 01:45:54]
9ine becomes 10 and the next thing you

[01:45:51 - 01:45:56]
know you're rolling into work at 11:00

[01:45:54 - 01:45:57]
a.m. every day and you think

[01:45:56 - 01:45:59]
everything's fine and you're doing this

[01:45:57 - 01:46:01]
in perpetuity and then one day your

[01:45:59 - 01:46:02]
manager shows up and it's like you're

[01:46:01 - 01:46:05]
fired and you're like wait why? It's

[01:46:02 - 01:46:08]
like well Bob you've been showing up at

[01:46:05 - 01:46:09]
11 every day and

[01:46:08 - 01:46:10]
we've been tracking all this. We were

[01:46:09 - 01:46:12]
just hoping that you would get your act

[01:46:10 - 01:46:15]
together and you didn't and so now

[01:46:12 - 01:46:16]
you're gone. These types of things also

[01:46:15 - 01:46:18]
happen. Now, in an ideal world, your

[01:46:16 - 01:46:22]
manager would tell you, but look, you

[01:46:18 - 01:46:24]
knew you should have showed up at 8.

[01:46:22 - 01:46:26]
That was the whole beginning of the

[01:46:24 - 01:46:29]
story. But then you felt like you kind

[01:46:26 - 01:46:31]
of were getting away with it and then

[01:46:29 - 01:46:34]
you weren't.

[01:46:31 - 01:46:37]
Same thing applies that anytime that you

[01:46:34 - 01:46:40]
are feeling maybe like you got away with

[01:46:37 - 01:46:43]
something, more likely than not,

[01:46:40 - 01:46:46]
you didn't get away with it. somebody

[01:46:43 - 01:46:49]
was noticing or minimally, you know, as

[01:46:46 - 01:46:53]
uh my brother likes to say, even if no

[01:46:49 - 01:46:55]
one else knows, I know. I agree with

[01:46:53 - 01:46:57]
this.

[01:46:55 - 01:46:59]
And Paul, if you're watching, I still

[01:46:57 - 01:47:01]
don't think it's cheating if you turn

[01:46:59 - 01:47:04]
Madden off after you lose the Super Bowl

[01:47:01 - 01:47:06]
so you can try again. But I get that you

[01:47:04 - 01:47:08]
might think that it's cheating and so

[01:47:06 - 01:47:10]
that's fine. But I'm going to do that

[01:47:08 - 01:47:11]
till forever. And I do that with

[01:47:10 - 01:47:15]
Balders's Gate. I do that with

[01:47:11 - 01:47:19]
Cyberpunk. I do that with uh Elden Ring.

[01:47:15 - 01:47:22]
Yeah, I'm I'm doing it all the time. So,

[01:47:19 - 01:47:24]
and all I I I'm actually not even

[01:47:22 - 01:47:26]
embarrassed by it. I think the majority

[01:47:24 - 01:47:29]
of the world does this. You might be in

[01:47:26 - 01:47:31]
the small minority, but I love you for

[01:47:29 - 01:47:35]
it. I love the fact that you're so

[01:47:31 - 01:47:38]
ethical that like you won't even

[01:47:35 - 01:47:40]
you'll spend the whole

[01:47:38 - 01:47:43]
month getting to the Super Bowl or

[01:47:40 - 01:47:45]
whatever in Madden just so you can lose

[01:47:43 - 01:47:48]
on one try and then do it all over

[01:47:45 - 01:47:49]
again. I'm of the mind,

[01:47:48 - 01:47:52]
speaking of the agent of your own

[01:47:49 - 01:47:54]
destruction, I'm of the mind that I

[01:47:52 - 01:47:56]
would rather just have multiple attempts

[01:47:54 - 01:47:58]
at the Super Bowl. And if it takes me

[01:47:56 - 01:48:00]
seven attempts and I win, then the new

[01:47:58 - 01:48:04]
goal would be to see if I could beat it

[01:48:00 - 01:48:07]
in six attempts. So, I get more practice

[01:48:04 - 01:48:10]
at the Super Bowl level and I get used

[01:48:07 - 01:48:12]
to that feeling. and and so but that

[01:48:10 - 01:48:15]
being said, I love the fact that you're

[01:48:12 - 01:48:17]
so in tune with sort of what you think

[01:48:15 - 01:48:18]
is right and wrong and that you won't do

[01:48:17 - 01:48:21]
the wrong thing. This is what I'm

[01:48:18 - 01:48:23]
talking about is like in the dark

[01:48:21 - 01:48:25]
recesses of our mind often times I think

[01:48:23 - 01:48:27]
we know

[01:48:25 - 01:48:29]
when we're doing something that's not

[01:48:27 - 01:48:33]
super great.

[01:48:29 - 01:48:36]
We know like like for example

[01:48:33 - 01:48:41]
I didn't always pick up the

[01:48:36 - 01:48:43]
the uh dog poop when I'd walk the dog.

[01:48:41 - 01:48:45]
I didn't

[01:48:43 - 01:48:48]
I'm just being honest.

[01:48:45 - 01:48:50]
When I was like in my 20s, maybe even my

[01:48:48 - 01:48:52]
30s, there would be time, especially

[01:48:50 - 01:48:53]
like when I was in Colorado and it was

[01:48:52 - 01:48:55]
super cold and I was like, "Well, it's

[01:48:53 - 01:48:56]
just going to melt into here and you

[01:48:55 - 01:48:58]
know, then I'm gonna have to try to

[01:48:56 - 01:49:02]
scoop." like I was like, "No, I'm not

[01:48:58 - 01:49:03]
going to do it." Now, I try to do it

[01:49:02 - 01:49:05]
even if it's at 3:00 in the morning.

[01:49:03 - 01:49:07]
Now, again, I'm not going to, as I've

[01:49:05 - 01:49:09]
said before, I'm going to tell it to you

[01:49:07 - 01:49:12]
straight. I don't carry the bag with me

[01:49:09 - 01:49:14]
all the way home. I I maybe I need to

[01:49:12 - 01:49:16]
level up some more, but I do. I just did

[01:49:14 - 01:49:19]
it tonight when I was walking the dog

[01:49:16 - 01:49:20]
while I was talking to Taylor. He went

[01:49:19 - 01:49:22]
to the bathroom. I picked it up. I put

[01:49:20 - 01:49:26]
it in the bag.

[01:49:22 - 01:49:30]
I tied off the bag. I dropped it in the

[01:49:26 - 01:49:32]
yard. I mean, in in the the side part

[01:49:30 - 01:49:34]
where it's very obvious and it could be

[01:49:32 - 01:49:36]
dropped into a trash bin. Yes, I'm

[01:49:34 - 01:49:38]
potentially a bad person cuz I didn't

[01:49:36 - 01:49:39]
carry it all the way home. But I do

[01:49:38 - 01:49:41]
think that the person who lives there,

[01:49:39 - 01:49:43]
cuz it gets done at my house, too. This

[01:49:41 - 01:49:45]
is my justification is that when the

[01:49:43 - 01:49:47]
poop bags are out in front of my house,

[01:49:45 - 01:49:50]
I just throw them in the trash in my

[01:49:47 - 01:49:53]
trash, which is what people I suspect

[01:49:50 - 01:49:55]
can do as well. And the reason why I do

[01:49:53 - 01:49:57]
this is sometimes I go on these many

[01:49:55 - 01:49:59]
mile walks like I did tonight. It was

[01:49:57 - 01:50:01]
probably three or four miles and he goes

[01:49:59 - 01:50:04]
to the bathroom at half a mile and then

[01:50:01 - 01:50:06]
I if I remember on the way home to grab

[01:50:04 - 01:50:07]
it, I will grab it. But sometimes I

[01:50:06 - 01:50:11]
forget. I'm not perfect. Okay. I'm just

[01:50:07 - 01:50:13]
trying to be authentic and real. I do

[01:50:11 - 01:50:16]
put it in a bag, but I don't always

[01:50:13 - 01:50:18]
throw it away. There's more work to do.

[01:50:16 - 01:50:20]
But it's better than it used to be. It

[01:50:18 - 01:50:22]
used to be that I just wouldn't even uh

[01:50:20 - 01:50:25]
pick it up. So, I'm making forward

[01:50:22 - 01:50:30]
progress. Anyway, the point of this is I

[01:50:25 - 01:50:34]
know I know it's not good to not uh

[01:50:30 - 01:50:37]
scoop up the the dog stuff.

[01:50:34 - 01:50:40]
And I knew even when I was not doing it

[01:50:37 - 01:50:43]
that it was not great. And now the truth

[01:50:40 - 01:50:45]
of the matter is with the bag I actually

[01:50:43 - 01:50:48]
I'm a bit conflicted because I think

[01:50:45 - 01:50:50]
that we can all do our part. These are

[01:50:48 - 01:50:52]
other neighbors that I have and it's not

[01:50:50 - 01:50:55]
that hard to just drop it in that but

[01:50:52 - 01:50:59]
whatever we can we can debate those

[01:50:55 - 01:51:02]
finer details another time.

[01:50:59 - 01:51:05]
The higher order bit here

[01:51:02 - 01:51:08]
is that often times when we're doing

[01:51:05 - 01:51:12]
things that are not great

[01:51:08 - 01:51:15]
we kind of know we know that it's not

[01:51:12 - 01:51:17]
super great and we can do better. And so

[01:51:15 - 01:51:19]
with this one, all I'm saying is that if

[01:51:17 - 01:51:21]
you get into that space where you're

[01:51:19 - 01:51:23]
doing something and you kind of know

[01:51:21 - 01:51:26]
it's not super great, even if you can

[01:51:23 - 01:51:30]
get away with it, my recommendation is

[01:51:26 - 01:51:32]
don't do it. Just don't do it because

[01:51:30 - 01:51:35]
sooner or later it will catch up with

[01:51:32 - 01:51:37]
you. Karma,

[01:51:35 - 01:51:39]
like whether you believe in karma or

[01:51:37 - 01:51:43]
not, it's been my experience that karma

[01:51:39 - 01:51:45]
is is a real thing. And how it works, I

[01:51:43 - 01:51:48]
don't know. Yeah, maybe it's just after

[01:51:45 - 01:51:50]
you make the same mistake enough, the

[01:51:48 - 01:51:53]
likelihood that you're going to be seen

[01:51:50 - 01:51:57]
doing this increases and so then it just

[01:51:53 - 01:51:58]
comes out. I don't know. But

[01:51:57 - 01:52:01]
I'd rather

[01:51:58 - 01:52:03]
not have those things linger. Like if

[01:52:01 - 01:52:05]
you're going to fight on a hill, if

[01:52:03 - 01:52:08]
you're going to actually like lean in

[01:52:05 - 01:52:10]
and throw down, I would do that on

[01:52:08 - 01:52:12]
something that actually matters to you.

[01:52:10 - 01:52:14]
Like for example, these two things.

[01:52:12 - 01:52:18]
These are two things that matter to me.

[01:52:14 - 01:52:20]
I I really want customers to be able to

[01:52:18 - 01:52:22]
own the games that they buy. I think

[01:52:20 - 01:52:24]
that's pretty cool. We used to be able

[01:52:22 - 01:52:26]
to do that. We'd have actually the

[01:52:24 - 01:52:28]
physical cartridges. That's how much

[01:52:26 - 01:52:29]
we'd own them.

[01:52:28 - 01:52:31]
Uh and I'm not saying it works in all

[01:52:29 - 01:52:34]
cases, but that's something that I would

[01:52:31 - 01:52:36]
like to see. And then I also think we

[01:52:34 - 01:52:38]
need to stop stop this. So stop killing

[01:52:36 - 01:52:40]
games.

[01:52:38 - 01:52:46]
Stop

[01:52:40 - 01:52:46]
killing the word engineering with

[01:52:46 - 01:52:51]
this.

[01:52:48 - 01:52:54]
Yeah, there we go. Maybe this will get

[01:52:51 - 01:52:57]
the message across.

[01:52:54 - 01:53:00]
Uh,

[01:52:57 - 01:53:02]
so

[01:53:00 - 01:53:04]
I completely lost my train of thought. I

[01:53:02 - 01:53:06]
have no idea why I was explaining these

[01:53:04 - 01:53:08]
things, but I think we've come full

[01:53:06 - 01:53:11]
circle.

[01:53:08 - 01:53:14]
At least with this stream,

[01:53:11 - 01:53:17]
we're about two hours. I wanted to do a

[01:53:14 - 01:53:20]
late Friday night, at least my time, uh,

[01:53:17 - 01:53:22]
stream so we could all wind down a

[01:53:20 - 01:53:25]
little bit. I feel like I've winded

[01:53:22 - 01:53:27]
myself down. I'm going to find the dog

[01:53:25 - 01:53:31]
in a moment. Let's see if I can get the

[01:53:27 - 01:53:35]
dog to come in here. Merl. Merl.

[01:53:31 - 01:53:39]
See if we can get a a short cameo at the

[01:53:35 - 01:53:42]
end of the stream. But I hope all of you

[01:53:39 - 01:53:44]
have a fantastic weekend.

[01:53:42 - 01:53:46]
I want to do some more VODs and streams

[01:53:44 - 01:53:48]
this weekend to talk about some more

[01:53:46 - 01:53:51]
stuff. I feel like there's a couple

[01:53:48 - 01:53:54]
things with mentor that I want to talk

[01:53:51 - 01:53:57]
about specifically like new features

[01:53:54 - 01:53:58]
that the team has been building out.

[01:53:57 - 01:54:00]
Yeah, there's some pretty cool stuff

[01:53:58 - 01:54:03]
like let me see if this I'm really

[01:54:00 - 01:54:07]
picking on Debian. Yeah, there's this

[01:54:03 - 01:54:07]
new um

[01:54:07 - 01:54:12]
uh red dot and the red dot is basically

[01:54:10 - 01:54:15]
stuff that's these are new issues that

[01:54:12 - 01:54:17]
have been introduced into the source uh

[01:54:15 - 01:54:20]
since the last time inference were run.

[01:54:17 - 01:54:24]
So this I think is super cool to help

[01:54:20 - 01:54:25]
people prioritize the new issues and it

[01:54:24 - 01:54:29]
categorizes them all together. So you

[01:54:25 - 01:54:31]
can see you can view the the priority

[01:54:29 - 01:54:33]
like we could just look at critical and

[01:54:31 - 01:54:34]
high and you can see the red dots

[01:54:33 - 01:54:37]
showing up at different intervals based

[01:54:34 - 01:54:40]
on a number of different parameters.

[01:54:37 - 01:54:44]
This looks like it's um

[01:54:40 - 01:54:48]
uh what is the ordering here?

[01:54:44 - 01:54:48]
Ordering here.

[01:54:48 - 01:54:53]
Yeah, it's it here's my speculation of

[01:54:50 - 01:54:55]
the ordering here. The ordering here is

[01:54:53 - 01:54:58]
based off of

[01:54:55 - 01:54:59]
the score

[01:54:58 - 01:55:02]
that's happening in the background which

[01:54:59 - 01:55:05]
is mostly invisible to the user. So

[01:55:02 - 01:55:08]
these are categoriz categorized as high

[01:55:05 - 01:55:12]
and critical but yes right okay here we

[01:55:08 - 01:55:15]
go score 9994 my speculation is that red

[01:55:12 - 01:55:18]
dot one is going to have a score that's

[01:55:15 - 01:55:18]
less than 994

[01:55:19 - 01:55:27]
I'm wrong

[01:55:23 - 01:55:27]
okay I'm wrong what

[01:55:27 - 01:55:33]
okay yeah I I guess I don't know why

[01:55:30 - 01:55:34]
it's starting Okay, I'll have to ask the

[01:55:33 - 01:55:36]
team.

[01:55:34 - 01:55:39]
There's there's probably some magical

[01:55:36 - 01:55:44]
reason why these are sorted the way

[01:55:39 - 01:55:44]
they're sorted. Wait, this is 1,029,35.

[01:55:47 - 01:55:54]
Uh, yeah. I don't know.

[01:55:50 - 01:55:57]
Oh, right. Uh,

[01:55:54 - 01:55:59]
I know that you could sort them

[01:55:57 - 01:56:00]
alphabetically now. I think that's

[01:55:59 - 01:56:01]
something you can sort them

[01:56:00 - 01:56:03]
alphabetically. I think that's also

[01:56:01 - 01:56:06]
cool, but something else must be going

[01:56:03 - 01:56:08]
on here

[01:56:06 - 01:56:11]
because they're not even sorted by the

[01:56:08 - 01:56:12]
numbers, right? So, we have the priority

[01:56:11 - 01:56:16]
here. And so, I thought it was sorting

[01:56:12 - 01:56:18]
it by the score. This is,029, but you

[01:56:16 - 01:56:23]
can see here this one's,33. So, this

[01:56:18 - 01:56:24]
would be this should be more at the top.

[01:56:23 - 01:56:27]
Uh, and then I was thinking it was

[01:56:24 - 01:56:30]
sorted by the ID, but you can see the ID

[01:56:27 - 01:56:33]
is not monotonically increasing. It's

[01:56:30 - 01:56:37]
like going both up and down. So starts

[01:56:33 - 01:56:40]
at six, peaks at 158,

[01:56:37 - 01:56:42]
and then is descending.

[01:56:40 - 01:56:44]
So I'm not sure.

[01:56:42 - 01:56:47]
Yeah. And it's not the age either

[01:56:44 - 01:56:48]
because this is 20 years, this is 6

[01:56:47 - 01:56:51]
months, this goes to four months, and

[01:56:48 - 01:56:54]
then it goes back to 3 years, 9 years.

[01:56:51 - 01:56:55]
So yeah, there's some something magical

[01:56:54 - 01:56:58]
that's going on here that I don't

[01:56:55 - 01:57:00]
understand. But anyway, I I want to show

[01:56:58 - 01:57:03]
off some awesome new mentor stuff,

[01:57:00 - 01:57:06]
mostly just cuz I'm happy about it. But

[01:57:03 - 01:57:08]
we'll discuss that next time or in one

[01:57:06 - 01:57:11]
of the podcasts

[01:57:08 - 01:57:15]
or VODs this weekend. With that, I think

[01:57:11 - 01:57:17]
I'm going to call it a night on the

[01:57:15 - 01:57:20]
stream. I'll work on uploading the HD

[01:57:17 - 01:57:22]
version of this. And I hope you guys

[01:57:20 - 01:57:26]
have a great night. Thanks for signing

[01:57:22 - 01:57:28]
in. And please comment in the comments

[01:57:26 - 01:57:31]
if you have any additional topics that

[01:57:28 - 01:57:33]
you'd like us to discuss or you agree

[01:57:31 - 01:57:38]
disagree with anything I've ranted about

[01:57:33 - 01:57:38]
tonight. Okay. Thanks everybody.

## コメント

### 1. @DoleTheOnly (👍 14)
Thank you for the point about choosing your inner circle wisely, Goju! I agree that seemingly small habits or influences can compound into bigger outcomes over time (both good and bad). Isn’t it worth continuously evaluating who’s influencing our decisions and environment?

> **@gojutechtalk** (👍 1): Hey T! I love that take. I think that follows our "iterate and adapt" philosophy, too. No?

### 2. @gojutechtalk (👍 34)
hi GTTers - if you're new here or have just been shy about introducing yourself, we'd love to hear from you. feel free to introduce yourself and reply to this message directly :)

### 3. @paulaccuardi9071 (👍 2)
Why do you have a million subscribers, but not more than a dozen comments and a few hundred thousand views on your most popular video?

> **@gojutechtalk** (👍 4): it's actually worse than that, hah. i did a live stream tonight and had like 10 concurrent viewers. :| but, in all seriousness, i dont generally care so much about quanity, as much as quality. as long as we're reaching a small group of high quality people (hopefully like you) im super happy. 

whatever the case, appreciate you stopping by and your comment. hope you come again, my friend!

### 4. @RajeshPatel-nx6bc (👍 0)
😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😊😅😅😅😅😅😅😅😅

