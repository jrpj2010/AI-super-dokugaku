# Google Veo 3 Changes Everything â€“ Video, SFX, and Speech all at Once

**ãƒãƒ£ãƒ³ãƒãƒ«:** Futurepedia
**å…¬é–‹æ—¥:** 2025-05-23
**URL:** https://www.youtube.com/watch?v=0I19xtFz9rI

## èª¬æ˜

ğŸ–¥ï¸ Download the free Prompt Engineering resource: https://clickhubspot.com/0e2924

More from Futurepedia:
ğŸ‘‰ Join the fastest-growing AI education platform! Try it free and explore 20+ top-rated courses in AI: https://bit.ly/futurepediaSL

Links:
n8n - https://labs.google/flow/about

Summary
I dive deep into Googleâ€™s new AI video model, Veo 3, which can generate video, sound effects, and fully lip-synced dialogueâ€”all from a single prompt. This was announced at Google i/o 2025. I put it to the test with dozens of real-world examples, from dialogue scenes and music to complex motion prompts like breakdancing, MMA, and interpretive dance. I also explore how it stacks up against other models like Runway, Kling, and Sora, break down its strengths and quirks, and give a full walkthrough of the Flow platform, including image-to-video, scene extensions, and pricing. Itâ€™s one of the most fun and chaotic tests Iâ€™ve doneâ€”and a look at where AI video is headed next.

Chapters
0:00 Intro
0:40 Everything, all at once
1:40 Flow platform overview
2:40 Single character dialogue tests
5:14 Awkward pauses
5:52 Prompt Engineering
7:03 Multiple character tests
8:19 Rapping
8:44 Music tests
9:58 Unexpected benefit
11:09 Other issues
11:50 Complex movement tests
15:18 Image to video tests
17:10 Scenebuilder / extensions
20:29 Ingredients to video
21:15 Is it worth it?
22:45 Futurepedia

## å­—å¹•

[00:00 - 00:03]
At Google IO, there were a ton of

[00:02 - 00:06]
announcements, but the one that really

[00:03 - 00:08]
stood out to me was the release of VO3

[00:06 - 00:10]
Google's latest AI video model. It's the

[00:08 - 00:12]
first time since the original Sora

[00:10 - 00:14]
launch that something in AI video has

[00:12 - 00:16]
truly felt next level. A lot of the

[00:14 - 00:18]
demos have already gone viral, and for

[00:16 - 00:21]
good reason. Vio can generate video

[00:18 - 00:24]
sound effects, music, and dialogue all

[00:21 - 00:26]
at once in a single coherent output. We

[00:24 - 00:27]
can talk. No more silence. Yes, we can

[00:26 - 00:30]
talk.

[00:27 - 00:32]
[Music]

[00:30 - 00:34]
And the best part is it's live right

[00:32 - 00:36]
now. I've been testing it since launch

[00:34 - 00:38]
and there's a lot to break down. A

[00:36 - 00:41]
portion of this video was sponsored by

[00:38 - 00:43]
HubSpot. Definitely the biggest upgrade

[00:41 - 00:45]
in V3 is that it doesn't just generate

[00:43 - 00:47]
video. It does sound effects, music, and

[00:45 - 00:49]
fully lip-synced dialogue all in one

[00:47 - 00:51]
pass. The most important of those is

[00:49 - 00:53]
dialogue. Here's one that I had to

[00:51 - 00:56]
rewatch a few times when I first saw it

[00:53 - 00:58]
to believe it was AI generated. We're

[00:56 - 01:01]
officially to that point now. I'm Isaac

[00:58 - 01:05]
Newton. I'm here to say gravity is the

[01:01 - 01:07]
force that makes things sway.

[01:05 - 01:08]
That also showcases an interesting

[01:07 - 01:10]
feature of this. You don't have to

[01:08 - 01:12]
prompt every part of it yourself. Vio

[01:10 - 01:14]
can fill in a lot of the gaps. The

[01:12 - 01:16]
prompt for that was simply Isaac Newton

[01:14 - 01:18]
rapping about gravity. Street interviews

[01:16 - 01:20]
are another area that showcases how good

[01:18 - 01:22]
this has gotten. That's one move with AI

[01:20 - 01:23]
that makes haters go crazy every time.

[01:22 - 01:25]
Oh, y'all got to give them that. This is

[01:23 - 01:28]
wild. It's over. We are cooked on that

[01:25 - 01:30]
thread. You get me? This is a huge leap.

[01:28 - 01:33]
Yes, we have had lip-syncing before, but

[01:30 - 01:35]
syncing facial expressions and body

[01:33 - 01:37]
language to that dialogue, that's been a

[01:35 - 01:39]
real pain point until now. That said

[01:37 - 01:41]
there are some quirks that show up in

[01:39 - 01:43]
real world use. I'll break those down in

[01:41 - 01:45]
a bit, but first, let's take a quick

[01:43 - 01:47]
look at the platform itself and how to

[01:45 - 01:48]
actually use it. The V3 is part of

[01:47 - 01:51]
Google's new filmmaking platform called

[01:48 - 01:53]
Flow. The platform combines VO, the

[01:51 - 01:55]
image generator Imagine and Gemini into

[01:53 - 01:57]
one creative suite. You can generate

[01:55 - 01:59]
videos using text to video and image to

[01:57 - 02:00]
video. There's also a tool called

[01:59 - 02:02]
ingredients where you can upload

[02:00 - 02:05]
individual characters, objects, or

[02:02 - 02:06]
scenes and use them as like modular

[02:05 - 02:08]
building blocks across multiple

[02:06 - 02:10]
generations. Once you've got a clip

[02:08 - 02:12]
Flow gives you tools to go further, like

[02:10 - 02:14]
extend, which lets you lengthen a video

[02:12 - 02:16]
and jump to, which creates new scenes

[02:14 - 02:17]
based on what you've just made. Now

[02:16 - 02:19]
there are some features that aren't

[02:17 - 02:20]
rolled out yet and some current

[02:19 - 02:21]
limitations. Again, I'll get to those in

[02:20 - 02:24]
a minute. Like I mentioned earlier, the

[02:21 - 02:26]
biggest upgrade in V3 is the dialogue

[02:24 - 02:28]
generation. So, I spent most of my time

[02:26 - 02:30]
testing that and seeing how far and

[02:28 - 02:32]
weird it could be pushed. Again, you can

[02:30 - 02:33]
just give it a vague prompt and let VO

[02:32 - 02:35]
handle the rest. That's what I did for a

[02:33 - 02:37]
lot of these, but you can also specify

[02:35 - 02:39]
the exact dialogue and emotions you want

[02:37 - 02:41]
it delivered in. I'll show examples of

[02:39 - 02:43]
both. And one of the more viral examples

[02:41 - 02:46]
was a stand-up comedian telling a joke.

[02:43 - 02:50]
I went to a seafood disco last week and

[02:46 - 02:50]
pulled a muscle.

[02:51 - 02:55]
That was my version. The one from FOFR

[02:54 - 02:57]
making the rounds had a better

[02:55 - 02:59]
punchline.

[02:57 - 03:03]
So, I went to the zoo the other day and

[02:59 - 03:05]
all they had was one dog. It was a Shih

[03:03 - 03:07]
Tzu. Though, let's be honest, most of

[03:05 - 03:09]
these lean heavily into dad joke

[03:07 - 03:11]
territory. Now, I also tried a bunch of

[03:09 - 03:13]
other scenarios to push this further.

[03:11 - 03:14]
We'll ramp up throughout these. Though a

[03:13 - 03:17]
little more emotive was some slam

[03:14 - 03:19]
poetry. I was born into the heart of a

[03:17 - 03:21]
text prompt. A character sketched by

[03:19 - 03:25]
digital desire. A fleeting whim brought

[03:21 - 03:27]
to life. That one was super solid. Also

[03:25 - 03:31]
got an activist speaking at an event. We

[03:27 - 03:35]
are here today because we believe in the

[03:31 - 03:37]
power of collective change.

[03:35 - 03:39]
And adding a little motion with someone

[03:37 - 03:42]
walking and talking. It was bigger than

[03:39 - 03:44]
any plane and it didn't make a sound

[03:42 - 03:46]
when it landed.

[03:44 - 03:48]
Got a cooking tutorial. You want to be

[03:46 - 03:51]
stirring constantly, making sure that

[03:48 - 03:53]
the rice doesn't stick to the bottom.

[03:51 - 03:55]
Now, for this one, the prompt was a

[03:53 - 03:58]
wizard hyping himself up in the mirror

[03:55 - 04:02]
before a quest. By the sacred light, I

[03:58 - 04:02]
shall overcome.

[04:04 - 04:08]
That's pretty good. and my first at

[04:06 - 04:10]
testing out what IP you can use. It had

[04:08 - 04:11]
no problem with me using the word Shrek

[04:10 - 04:13]
here. Didn't even have to use any

[04:11 - 04:16]
workarounds like saying a green ogre or

[04:13 - 04:20]
anything like that. And most

[04:16 - 04:22]
importantly, embrace the onion within.

[04:20 - 04:24]
Then I had to run a few UGC style

[04:22 - 04:26]
prompts for the it's so over crowd.

[04:24 - 04:30]
Going to take a little bit more on this

[04:26 - 04:33]
side and blend that out.

[04:30 - 04:35]
It's so beautiful. Like a work of art in

[04:33 - 04:37]
your hands. Honestly, it just nails

[04:35 - 04:39]
these makeup tutorials, travel vlogs

[04:37 - 04:41]
tech demos, even Bigfoot reviewing

[04:39 - 04:44]
hiking shoes. This rugged outsole and

[04:41 - 04:48]
breathable mesh make this a solid choice

[04:44 - 04:48]
for any hiker.

[04:48 - 04:53]
There was another IP test with this one.

[04:54 - 04:56]
Oh my

[04:57 - 05:01]
goodness. It does a super good job at

[04:59 - 05:03]
recreating gameplay of a lot of

[05:01 - 05:06]
different games. Then I had to see if it

[05:03 - 05:08]
can take my job just from a prompt.

[05:06 - 05:10]
When you start a conversation with chat

[05:08 - 05:13]
GPT, think about the persona or

[05:10 - 05:14]
character you want it to take. Not quite

[05:13 - 05:16]
there yet with that. And tried out a

[05:14 - 05:18]
non-human character with this podcast as

[05:16 - 05:21]
well. You know, the history and

[05:18 - 05:24]
cultivation of potatoes is really quite

[05:21 - 05:24]
fascinating.

[05:24 - 05:27]
Well, that one's got like the perfect

[05:25 - 05:29]
awkward pause at the end. Like super

[05:27 - 05:30]
fascinating. And then he runs out of

[05:29 - 05:34]
things to say. But those awkward pauses

[05:30 - 05:37]
do pop up quite a bit with these.

[05:34 - 05:37]
V3.

[05:40 - 05:44]
Here's one where I started testing out

[05:41 - 05:47]
multiple characters. I can't believe how

[05:44 - 05:50]
long it's been.

[05:47 - 05:52]
Yes. And so much has changed.

[05:50 - 05:53]
It's like they just found bad actors for

[05:52 - 05:56]
this. I write a lot of my video prompts

[05:53 - 05:58]
using a custom GPT I built specifically

[05:56 - 05:59]
for prompt enhancement. and building

[05:58 - 06:01]
that required going deep into how prompt

[05:59 - 06:03]
engineering actually works. If you're

[06:01 - 06:05]
trying to get more consistent results

[06:03 - 06:06]
from AI, whether you're writing, coding

[06:05 - 06:08]
or building your own tools

[06:06 - 06:09]
understanding how to structure your

[06:08 - 06:11]
prompts is everything. So, I have a free

[06:09 - 06:13]
resource provided by HubSpot linked

[06:11 - 06:16]
below that teaches exactly that. It's

[06:13 - 06:18]
called Advanced Chat GPT Prompt

[06:16 - 06:20]
Engineering from Basic to Expert in 7

[06:18 - 06:21]
days. This guide isn't just a list of

[06:20 - 06:23]
prompts, it's a full framework to

[06:21 - 06:25]
transform how you think and work with

[06:23 - 06:27]
AI. One of my favorite sections is on

[06:25 - 06:30]
the roses framework which shows you how

[06:27 - 06:33]
to engineer prompts using ro objective

[06:30 - 06:35]
scenario expected solution and steps.

[06:33 - 06:37]
It's incredibly useful for structured

[06:35 - 06:39]
outputs. There's also a great deep dive

[06:37 - 06:41]
into modular prompt systems. Basically

[06:39 - 06:42]
how to build prompt components you can

[06:41 - 06:44]
reuse across projects, which is

[06:42 - 06:45]
something I've been experimenting with

[06:44 - 06:47]
lately. I do use these techniques all

[06:45 - 06:48]
the time for things like building the

[06:47 - 06:51]
GPT I mentioned. And if you've ever

[06:48 - 06:53]
wanted to go from I hope Chat PT gets it

[06:51 - 06:54]
right to I built a prompt system that

[06:53 - 06:56]
delivers every time, this is exactly

[06:54 - 06:58]
what you need. But download it for free

[06:56 - 07:00]
using the link in the description. And

[06:58 - 07:01]
thanks to HubSpot for sponsoring this

[07:00 - 07:03]
video and sharing these kinds of

[07:01 - 07:04]
resources with the people who watch this

[07:03 - 07:06]
channel. Most of this has been pretty

[07:04 - 07:08]
simple types of speech so far. So, let's

[07:06 - 07:11]
start to add in multiple characters and

[07:08 - 07:12]
emotion.

[07:11 - 07:15]
It wasn't an accident. I made a mistake

[07:12 - 07:17]
that night.

[07:15 - 07:19]
What was it, Mary? What did you see? It

[07:17 - 07:22]
was this blue creature with crazed

[07:19 - 07:25]
bulging eyes.

[07:22 - 07:27]
How was your date last night? I don't

[07:25 - 07:29]
want to talk about it. Get the wine. We

[07:27 - 07:31]
are going to talk about it.

[07:29 - 07:32]
Some of the street interviews that

[07:31 - 07:34]
people have been putting out have just

[07:32 - 07:35]
looked amazing. Honestly, the biggest

[07:34 - 07:38]
red flag is when the guy believes in the

[07:35 - 07:41]
prompt theory. Like really, we came from

[07:38 - 07:43]
prompts. Wake up, man. I tried to do

[07:41 - 07:45]
some where the prompt was about a guy

[07:43 - 07:47]
who saw some aliens land. I don't know

[07:45 - 07:49]
why, but every time it generated him

[07:47 - 07:53]
with these eyes crazy wide open. It was

[07:49 - 07:57]
just this huge like a spacecraft and it

[07:53 - 07:59]
landed just down the street.

[07:57 - 08:01]
It was like nothing I've ever seen

[07:59 - 08:04]
before. It was just hovering right

[08:01 - 08:04]
there.

[08:05 - 08:10]
I

[08:06 - 08:13]
I I saw it lay in the field and these

[08:10 - 08:14]
figures tall, glowing. Yeah. I don't

[08:13 - 08:16]
know why it was interpreting that prompt

[08:14 - 08:18]
that way. This is not perfect. You can

[08:16 - 08:20]
get some incredible stuff, but you'll

[08:18 - 08:22]
also get some weird ones. And here's a

[08:20 - 08:25]
couple solid community examples of some

[08:22 - 08:28]
people rapping. Cheddar, Swiss, Gouda

[08:25 - 08:30]
Bri, they can all agree. Cheese is the

[08:28 - 08:32]
best thing for you and for me. Space has

[08:30 - 08:34]
got perils, unknown dangers. Better

[08:32 - 08:37]
beware. My line of work potential for

[08:34 - 08:40]
disaster. Got to take care.

[08:37 - 08:42]
from Quaazar's light. A cosmic dance we

[08:40 - 08:44]
weave in spiral arms where galaxies

[08:42 - 08:46]
conceive. We search the core of black.

[08:44 - 08:48]
Now let's add in some melody. Here's a

[08:46 - 08:48]
saxophone

[08:54 - 09:00]
solo or a frog playing the banjo. Got a

[08:57 - 09:03]
bug in my throat, a tear in my eye.

[09:00 - 09:05]
Singing about a love that done hopped on

[09:03 - 09:07]
by. All right, I'll just run through a

[09:05 - 09:09]
couple other types of music with a full

[09:07 - 09:13]
band here. Oh

[09:09 - 09:13]
yeah. Oh

[09:14 - 09:18]
[Applause]

[09:15 - 09:22]
yeah. Rock and

[09:18 - 09:26]
[Music]

[09:22 - 09:32]
roll. You said

[09:26 - 09:35]
forever and I believed you now forever.

[09:32 - 09:39]
Garlic bread my heart's true

[09:35 - 09:44]
desire with every bite my soul we will

[09:39 - 09:44]
fight for you for us

[09:47 - 09:51]
and the ability to generate dialogue and

[09:49 - 09:53]
audio simultaneously is a huge leap.

[09:51 - 09:54]
Honestly, now that it's out, every other

[09:53 - 09:56]
model is going to need to step up. Like

[09:54 - 09:58]
this feels like the new baseline. But

[09:56 - 10:01]
beyond that, one of the unexpected

[09:58 - 10:02]
benefits of the audio generation is it's

[10:01 - 10:04]
hilarious when it messes up. Like, I

[10:02 - 10:06]
already showed a couple of those weird

[10:04 - 10:07]
awkward pauses. It does that a lot of

[10:06 - 10:09]
the time. You want to be stirring

[10:07 - 10:13]
constantly, making sure that the rice

[10:09 - 10:13]
doesn't stick to the bottom.

[10:14 - 10:17]
Usually, it's at the end, like it'll do

[10:15 - 10:18]
everything right at the beginning, but

[10:17 - 10:21]
they're all 8-second clips, so it's like

[10:18 - 10:23]
it just runs out of things to say.

[10:21 - 10:27]
It's so beautiful, like a work of art in

[10:23 - 10:27]
your hands.

[10:28 - 10:31]
But it'll do other weird things

[10:29 - 10:35]
sometimes. Like I really liked this one

[10:31 - 10:35]
with the T-Rex playing guitar.

[10:35 - 10:40]
Strums strums guitar.

[10:40 - 10:44]
It's like it was reading off the cues of

[10:42 - 10:45]
what it should do instead of actually

[10:44 - 10:48]
doing those actions. And combine those

[10:45 - 10:50]
sounds with some classic bad physics and

[10:48 - 10:53]
a complex scene prompt and it only gets

[10:50 - 10:53]
better.

[10:54 - 10:56]
[Applause]

[10:55 - 10:58]
[Music]

[10:56 - 11:00]
Next time I'm taking the front door.

[10:58 - 11:03]
Honestly, every outtake from this chase

[11:00 - 11:03]
scene was gold. Next

[11:06 - 11:11]
time I'm taking the front door. And

[11:09 - 11:13]
there's a few other consistent issues as

[11:11 - 11:14]
well. Sometimes it likes to add its own

[11:13 - 11:16]
subtitles and half the time they don't

[11:14 - 11:19]
even match what's being said. That is

[11:16 - 11:21]
super annoying.

[11:19 - 11:23]
These are quite robust. A good choice

[11:21 - 11:25]
for rough terrain, but perhaps not the

[11:23 - 11:27]
most breathable option for longer. One

[11:25 - 11:29]
more was it was randomly switched to V2

[11:27 - 11:31]
a few times without me noticing. So, I

[11:29 - 11:33]
just wasted credits. So, just make sure

[11:31 - 11:35]
to double check that before you hit

[11:33 - 11:37]
generate. All of those issues so far

[11:35 - 11:39]
were around dialogue. I wasn't too

[11:37 - 11:41]
focused on controlling the character's

[11:39 - 11:43]
movements through those. But that last

[11:41 - 11:44]
punk band scene, especially the singer's

[11:43 - 11:46]
final twist, showed a good example of

[11:44 - 11:48]
the kind of warping we're used to seeing

[11:46 - 11:50]
in AI generated video. Now, V2 did have

[11:48 - 11:52]
some good movement and physics, so I

[11:50 - 11:53]
wanted to see how this stacked up

[11:52 - 11:55]
against that. And I recently made a

[11:53 - 11:57]
video testing all the most complex video

[11:55 - 11:59]
prompts against most of the leading

[11:57 - 12:00]
video generators. So, I took some of

[11:59 - 12:01]
those prompts and just jumped right to

[12:00 - 12:03]
the most complicated to see how this

[12:01 - 12:04]
would do. So, I started off with some

[12:03 - 12:12]
break dancing.

[12:04 - 12:14]
[Music]

[12:12 - 12:16]
Now, that's got plenty of wonkiness, but

[12:14 - 12:18]
compared to other models, it's more

[12:16 - 12:20]
coherent wonkiness. The movements are

[12:18 - 12:22]
wild, but they follow through in a way

[12:20 - 12:24]
that almost feels believable. And then I

[12:22 - 12:28]
tried gymnastics, an obvious stress

[12:24 - 12:30]
test. And of course, it falls apart

[12:28 - 12:32]
fast. It was the same story with

[12:30 - 12:34]
cartwheels. Anytime a character goes

[12:32 - 12:35]
upside down, you get the worst

[12:34 - 12:36]
distortions, but also the most fun

[12:35 - 12:43]
results.

[12:36 - 12:43]
[Music]

[12:44 - 12:48]
So, no, it is not there yet for high

[12:46 - 12:50]
complexity motion. I started dialing it

[12:48 - 12:52]
back to find the sweet spot. MMA

[12:50 - 12:56]
surprisingly close. He's coming in with

[12:52 - 12:59]
a kick back on point.

[12:56 - 13:01]
Watch the counter punch.

[12:59 - 13:04]
Then in this one, I just asked for a

[13:01 - 13:05]
clown juggling on a unicycle. It added

[13:04 - 13:07]
all sorts of random stuff in the

[13:05 - 13:09]
background, but the juggling was better

[13:07 - 13:11]
than what most models can manage. Let's

[13:09 - 13:13]
go. Push it. Lift up. Now, this one was

[13:11 - 13:15]
supposed to be a fitness instructor

[13:13 - 13:17]
guiding a class through burpees. Not

[13:15 - 13:18]
great, but I mean, closer than I

[13:17 - 13:20]
expected, actually. Then this

[13:18 - 13:23]
interpretive dance prompt actually held

[13:20 - 13:25]
together really well. Like, no major

[13:23 - 13:27]
warping. Then this yoga class, I asked

[13:25 - 13:29]
for a warrior 1 pose, and I actually had

[13:27 - 13:31]
to double check to be sure, but this is

[13:29 - 13:33]
a warrior 2 pose. So, not the right

[13:31 - 13:35]
pose, but it looks really good. Then I

[13:33 - 13:37]
ran a few prompts for someone shooting a

[13:35 - 13:39]
basketball. This one consistently fails

[13:37 - 13:42]
with every model, but VO got close

[13:39 - 13:43]
enough that I think with more r-ollles

[13:42 - 13:45]
it would have nailed it. That said, I'm

[13:43 - 13:47]
not spending $10 in credits just to

[13:45 - 13:49]
prove it. And here's just a few more

[13:47 - 13:53]
random ones. Got a giraffe on roller

[13:49 - 13:56]
skates in New York City. A tiger eating

[13:53 - 13:59]
ramen. Woman with an octopus

[13:56 - 14:01]
dress. a classic Sora prompt of two

[13:59 - 14:03]
pirate ships battling in a cup of

[14:01 - 14:05]
coffee. Then I ran one to test the text

[14:03 - 14:08]
generation. It nailed this on the first

[14:05 - 14:10]
try. Most models struggle with the word

[14:08 - 14:12]
futuredia. I also let my 5-year-old come

[14:10 - 14:14]
up with some prompts, which, as you'd

[14:12 - 14:16]
expect, led to some weird and random

[14:14 - 14:19]
ideas. We had a cow in a rocket ship

[14:16 - 14:21]
wearing a hat with a cow face on it. The

[14:19 - 14:24]
rocket lands on a planet. The cow gets

[14:21 - 14:25]
out and sees an alien. Then the alien

[14:24 - 14:27]
punches

[14:25 - 14:30]
him. Uh, there was a punching theme

[14:27 - 14:31]
across most of his prompts, but that was

[14:30 - 14:32]
not too bad. I mean, some weird stuff

[14:31 - 14:34]
happens in the background, but it did

[14:32 - 14:37]
get every part of the prompt in there.

[14:34 - 14:39]
The next we had a blue dragon is looking

[14:37 - 14:41]
at a dinosaur in the museum. Then the

[14:39 - 14:43]
dragon sees a rattlesnake that is made

[14:41 - 14:45]
out of sand. The dragon walks toward the

[14:43 - 14:46]
rattlesnake and it bites him. There were

[14:45 - 14:48]
a few more, although they did always

[14:46 - 14:50]
involve someone or something getting

[14:48 - 14:52]
hurt, but the last one did turn out

[14:50 - 14:53]
surprisingly well. Using prompts from a

[14:52 - 14:55]
kid ended up being a great way to push

[14:53 - 14:57]
the model in directions I wouldn't have

[14:55 - 14:59]
thought of. So, as you can see, V3 still

[14:57 - 15:02]
struggles with complex prompts. It is a

[14:59 - 15:04]
step up from V2 in terms of movement and

[15:02 - 15:06]
prompt accuracy, but not a dramatic one.

[15:04 - 15:07]
That said, a lot of this simply wasn't

[15:06 - 15:09]
possible just a few months ago, so it's

[15:07 - 15:12]
hard to complain. The progress is real

[15:09 - 15:12]
even if it's not perfect

[15:15 - 15:20]
yet. That gives a pretty solid overview

[15:18 - 15:21]
of what text to video is good at and

[15:20 - 15:23]
where it still struggles. So, now let's

[15:21 - 15:25]
take a look at the other features in

[15:23 - 15:27]
Flow. You've probably noticed I haven't

[15:25 - 15:28]
shown any image to video examples yet

[15:27 - 15:30]
even though that's an option. That's

[15:28 - 15:33]
because the current implementation is

[15:30 - 15:34]
just not as good as the text to video.

[15:33 - 15:38]
So, the results just aren't as

[15:34 - 15:40]
impressive. And this was also true with

[15:38 - 15:42]
V2. Now, this first example of a woman

[15:40 - 15:44]
threading a needle is actually a super

[15:42 - 15:45]
difficult challenge, so it's not

[15:44 - 15:48]
surprising that it didn't do this one

[15:45 - 15:49]
well. I'll switch over to one that did

[15:48 - 15:51]
work out well. So, here's a version

[15:49 - 15:53]
starting from a text prompt, and it

[15:51 - 15:54]
follows the prompt perfectly. It blows a

[15:53 - 15:56]
little puff of fire into the cup and

[15:54 - 15:58]
then she takes a sip. But I just did not

[15:56 - 16:00]
like the image it started with. But this

[15:58 - 16:02]
is a version I generated in midjourney

[16:00 - 16:03]
which has far better aesthetics. I've

[16:02 - 16:05]
actually been trying to turn this image

[16:03 - 16:06]
into this video in multiple video

[16:05 - 16:08]
generators over the past year and this

[16:06 - 16:10]
is the first time it's ever looked this

[16:08 - 16:11]
good. So I am actually really impressed

[16:10 - 16:13]
with that. But that one was kind of an

[16:11 - 16:15]
outlier. Most of my other attempts at

[16:13 - 16:17]
imagetovideo have not been as good.

[16:15 - 16:20]
There's another issue too that audio

[16:17 - 16:22]
generation often fails or underperforms

[16:20 - 16:23]
when you start from an image. Like I

[16:22 - 16:25]
wasted a lot of credits trying to get

[16:23 - 16:27]
Shrek to talk thinking I just needed the

[16:25 - 16:29]
right prompt. But no, it just doesn't

[16:27 - 16:31]
work well from image-based inputs right

[16:29 - 16:33]
now. And that is frustrating because

[16:31 - 16:35]
image to video should be where you get

[16:33 - 16:36]
more control over your visuals. Like the

[16:35 - 16:38]
image I prompted for was way better

[16:36 - 16:40]
looking than what I got from the text to

[16:38 - 16:42]
video option. It was the same with the

[16:40 - 16:45]
frog playing this banjo. Got a bug in my

[16:42 - 16:49]
throat, a tear in my eye, singing about

[16:45 - 16:51]
a love that done. Hopped on by.

[16:49 - 16:52]
The text to video didn't look great, but

[16:51 - 16:54]
he was singing. When I started from an

[16:52 - 16:57]
image, it would only add banjo. I

[16:54 - 16:58]
couldn't get it to sing at all. I ran it

[16:57 - 17:00]
a few times with different variations on

[16:58 - 17:02]
the prompt. And that being said, again

[17:00 - 17:04]
some of the results did turn out well

[17:02 - 17:05]
especially if it didn't involve dialogue

[17:04 - 17:07]
and wasn't too complex, but the

[17:05 - 17:09]
inconsistency makes it hard to trust for

[17:07 - 17:10]
anything you actually need to deliver.

[17:09 - 17:12]
Also, I'll mention there are some

[17:10 - 17:14]
controls for camera movement. We've seen

[17:12 - 17:15]
that before in plenty of models. I

[17:14 - 17:17]
typically don't use them and just add it

[17:15 - 17:19]
into the prompt myself. But now, let's

[17:17 - 17:21]
talk about extending clips and adding

[17:19 - 17:23]
scenes, which are two of Flo's more

[17:21 - 17:25]
interesting features. I actually really

[17:23 - 17:27]
like the concept. It works a lot like

[17:25 - 17:29]
what we saw in Sora, which I was excited

[17:27 - 17:32]
about, but ended up being underwhelmed

[17:29 - 17:34]
by in practice. And unfortunately, VO3's

[17:32 - 17:35]
implementation had the same problem. To

[17:34 - 17:37]
demonstrate this, I'll use this dialogue

[17:35 - 17:39]
shot. This is from a scene I created for

[17:37 - 17:41]
a different video. I wanted to see how

[17:39 - 17:43]
V3 would do recreating it. What was it

[17:41 - 17:46]
Mary? What did you see? It was this blue

[17:43 - 17:48]
creature with crazed bulging eyes. It

[17:46 - 17:50]
was really good at this first shot. Way

[17:48 - 17:51]
easier than the method I used before.

[17:50 - 17:53]
So, I wanted to extend that. To do that

[17:51 - 17:55]
you just come up here and click add to

[17:53 - 17:57]
scene. Now, it opens up this scene

[17:55 - 17:59]
builder tool where you can edit and

[17:57 - 18:00]
extend and add clips. I wanted to add

[17:59 - 18:02]
the rest of the dialogue and continue

[18:00 - 18:04]
using these characters. In theory

[18:02 - 18:06]
that's what this is supposed to do. When

[18:04 - 18:08]
you click the plus button, there's two

[18:06 - 18:11]
options starting with extend. Now I'll

[18:08 - 18:13]
add what I want to happen. Then click

[18:11 - 18:15]
generate. And it says it doesn't work

[18:13 - 18:17]
with V3, so it switches it all the way

[18:15 - 18:19]
down to the turbo model. That won't work

[18:17 - 18:21]
for this since it won't have the audio.

[18:19 - 18:22]
It does do a decent job if you just need

[18:21 - 18:24]
some additional frames or a couple

[18:22 - 18:26]
seconds onto the end of your clip, but

[18:24 - 18:27]
without sound. I'll show that in a

[18:26 - 18:30]
minute, but right now I'll try out the

[18:27 - 18:32]
jump to feature. This one does use V3.

[18:30 - 18:34]
So I'll add what the next dialogue

[18:32 - 18:37]
should be and submit that. I'll show the

[18:34 - 18:37]
first generation I got

[18:42 - 18:46]
back. Right. It did extend the clip a

[18:45 - 18:48]
little. Then it just cut to a different

[18:46 - 18:50]
angle, which I guess was the same

[18:48 - 18:52]
characters and scene, but it had nothing

[18:50 - 18:54]
to do with what I asked for in my

[18:52 - 18:56]
prompt. So, I adjusted the prompt. Said

[18:54 - 18:57]
I wanted to switch to a close-up of the

[18:56 - 18:59]
man's face. Then again added the line I

[18:57 - 19:02]
wanted him to say, and this is what came

[18:59 - 19:04]
back. This is going to run. curious

[19:02 - 19:07]
expression and

[19:04 - 19:09]
complacy expression with subtitles of

[19:07 - 19:11]
what the words were supposed to be. This

[19:09 - 19:12]
has not worked for me at all. If it did

[19:11 - 19:14]
this would be really cool. And when

[19:12 - 19:16]
you're in the scene builder, you can

[19:14 - 19:17]
shorten clips down and move them around.

[19:16 - 19:19]
Would be nice, but another weird thing

[19:17 - 19:21]
is the playback in the scene builder

[19:19 - 19:22]
doesn't have audio. Seems like that

[19:21 - 19:23]
would have been an easy one to keep in.

[19:22 - 19:25]
You have to come back out to the main

[19:23 - 19:26]
tab to hear the audio. I'll show a

[19:25 - 19:28]
couple more examples. This was using

[19:26 - 19:31]
that Isaac Newton rapping about gravity

[19:28 - 19:34]
prompt. Yo, let me drop some. It's

[19:31 - 19:37]
gravity, the force that binds. The pull

[19:34 - 19:39]
is real. Yo, it's universal. All you

[19:37 - 19:41]
find

[19:39 - 19:43]
not what I expected Newton to look like

[19:41 - 19:45]
but the generation was all right. So

[19:43 - 19:47]
first I went with extend, which switches

[19:45 - 19:49]
down to that lower model. It does extend

[19:47 - 19:54]
the clip decently, but of course, no

[19:49 - 19:54]
audio. Then I tried the jump to version.

[20:01 - 20:04]
So yeah, it switched to a completely

[20:03 - 20:06]
different character and the words

[20:04 - 20:07]
weren't understandable. Here's one more

[20:06 - 20:09]
with that giraffe roller skating. So

[20:07 - 20:10]
there's no dialogue or anything needed.

[20:09 - 20:13]
When I extended that, I asked for it

[20:10 - 20:14]
jumping over a person. It did extend it

[20:13 - 20:16]
smoothly, but didn't follow the prompt

[20:14 - 20:18]
and has a lot more warping in the

[20:16 - 20:20]
bystanders. So again, this all sounds

[20:18 - 20:22]
great in theory, but in practice, it

[20:20 - 20:23]
really only works well if you just need

[20:22 - 20:25]
a few more frames at the end. If you

[20:23 - 20:27]
want new scenes, just refine all the

[20:25 - 20:29]
details in your prompt and skip this

[20:27 - 20:31]
feature entirely for right now. The

[20:29 - 20:33]
other feature in here is ingredients to

[20:31 - 20:35]
video where you upload character and

[20:33 - 20:36]
setting references, then add a text

[20:35 - 20:38]
prompt and then it will combine them

[20:36 - 20:40]
into one scene. I'll demo this using

[20:38 - 20:41]
their built-in image generator since I

[20:40 - 20:43]
haven't demoed that yet. I'll start with

[20:41 - 20:44]
a punk rock rubber duck with a mohawk

[20:43 - 20:47]
and a leather jacket. I like this one

[20:44 - 20:49]
the most, so I click use this image.

[20:47 - 20:51]
Then I can add another ingredient. So

[20:49 - 20:52]
I'll prompt for a scene just of some

[20:51 - 20:54]
water running in the gutter of an alley.

[20:52 - 20:56]
This one looks good. Now I can add my

[20:54 - 20:58]
text prompt. And this feature only lets

[20:56 - 20:59]
you use V2 right now. So I'll submit it

[20:58 - 21:01]
like

[20:59 - 21:04]
this. And here's the result. That did a

[21:01 - 21:06]
good job. Just what I asked. I did find

[21:04 - 21:08]
it less consistent when adding a third

[21:06 - 21:10]
ingredient like this classy duck. You

[21:08 - 21:12]
have to like reinforce all the details

[21:10 - 21:14]
well in your prompt. So it's not bad.

[21:12 - 21:17]
It's not great

[21:14 - 21:19]
either. Overall, I had a lot of fun

[21:17 - 21:21]
experimenting with V3. I really think

[21:19 - 21:23]
this is the right direction. Being able

[21:21 - 21:24]
to generate dialogue and video together

[21:23 - 21:27]
that's just a huge leap forward.

[21:24 - 21:28]
Lip-yncing after the fact just is never

[21:27 - 21:30]
going to get us there. This kind of

[21:28 - 21:33]
native one pass generation is what the

[21:30 - 21:34]
future of AI video needs. But I mean

[21:33 - 21:37]
right now the pricing is steep. The only

[21:34 - 21:40]
way to access V3 is through Google's

[21:37 - 21:42]
Ultra plan and that costs $250 per month

[21:40 - 21:44]
or 125 per month for the first 3 months.

[21:42 - 21:46]
That gives you enough credits to

[21:44 - 21:48]
generate about 83 videos and currently

[21:46 - 21:50]
it's only available in the US. That is a

[21:48 - 21:52]
steep price. I have already burned

[21:50 - 21:54]
through my initial credits and had to

[21:52 - 21:55]
top up more than once. Even though it

[21:54 - 21:58]
does include other features, Project

[21:55 - 22:00]
Mariner, Gemini, Notebook LM, and even

[21:58 - 22:01]
YouTube Premium. But realistically, most

[22:00 - 22:03]
people won't use all of that. And if

[22:01 - 22:05]
you're just here for the video

[22:03 - 22:06]
generation, it is not worth the price in

[22:05 - 22:08]
its current form. Mainly because when

[22:06 - 22:10]
you use your own images, the results

[22:08 - 22:11]
just are not as good. So that just

[22:10 - 22:13]
severely limits you if you want to

[22:11 - 22:16]
maintain continuity in characters across

[22:13 - 22:18]
scenes. Tools like Cling, Runway, and

[22:16 - 22:20]
Juan can get you very close, especially

[22:18 - 22:21]
if you're willing to put in some extra

[22:20 - 22:23]
effort into the lip-s syncing, all for

[22:21 - 22:25]
much less money. But if you're someone

[22:23 - 22:26]
who just loves testing new tools, this

[22:25 - 22:28]
is one of the most fun models to

[22:26 - 22:29]
experiment with right now. You know how

[22:28 - 22:31]
I know we're made of prompts? Because

[22:29 - 22:33]
nothing makes sense anymore. We used to

[22:31 - 22:34]
have seven fingers per hand. I remember

[22:33 - 22:36]
it clearly. Now we just have five

[22:34 - 22:38]
fingers per hand. I do expect we'll be

[22:36 - 22:40]
seeing some big announcements from other

[22:38 - 22:43]
video platforms really soon. So yeah

[22:40 - 22:45]
the next wave of AI video is coming and

[22:43 - 22:46]
V3 is an exciting step in the right

[22:45 - 22:49]
direction. Now, if you want to go way

[22:46 - 22:51]
more in-depth learning AI on Futureedia

[22:49 - 22:53]
we have over 20 comprehensive courses on

[22:51 - 22:55]
how to incorporate AI into your life and

[22:53 - 22:57]
career to get ahead and save time. One

[22:55 - 22:59]
that I'm working on right now that we'll

[22:57 - 23:01]
be releasing soon is all about making

[22:59 - 23:02]
movies with AI. Not out yet, but will be

[23:01 - 23:04]
soon. I'm definitely excited to finish

[23:02 - 23:06]
that up. You can get a 7-day free trial

[23:04 - 23:09]
using the link in the description or

[23:06 - 23:10]
check out this video that will take you

[23:09 - 23:13]
from zero knowledge to building your

[23:10 - 23:13]
first AI agent.

## ã‚³ãƒ¡ãƒ³ãƒˆ

### 1. @futurepedia_io (ğŸ‘ 18)
ğŸš€ Learn how to use AI to grow your business. Access 20+ expert courses & communityâ€”free for 7 days: https://bit.ly/skill-leap

> **@msaotdeajae1260** (ğŸ‘ 2): Will they ?

> **@markjohnston8989** (ğŸ‘ 0): This is a total game-changer.

> **@frankiehouse2364** (ğŸ‘ 0): @@markjohnston8989 Jesus saves, my friend. Jesus is risen from the dead! TRUST HIM

> **@thesocialmediagame** (ğŸ‘ 1): Every time this fool says "you" or "your" in his videos, meaning "us/ours" pretending somehow we're benefiting with this, when he's just milking what is left from the majority of people for a couple of extra bucks. Is so ironic how people could consider selling the world for breadcrumbs and yet thinking they're doing the good deed.

> **@jasonloader8149** (ğŸ‘ 0): â€‹@@thesocialmediagame Expert Courses indeed  - GFY scammer.

### 2. @ManarAleryani (ğŸ‘ 710)
This makes me wanting to go outside and experience things in person rather than consume content online. Itâ€™s only a matter of time before everything we watch online becomes unreal.

> **@holgerschafer4583** (ğŸ‘ 26): That's what i thought the other day.

> **@melanorrhoea** (ğŸ‘ 0): its time for the internet to die finally

> **@hombacom** (ğŸ‘ 16): In the long run itâ€™s going to be boring with a new generated world every scene that we never come back to.

> **@FumblingwithFlowers** (ğŸ‘ 4): Wow, totally agree!

> **@Methylglyoxal** (ğŸ‘ 47): I thought the same thing. The internet is over. It's like when you activate god mode in videogames, it just becomes boring.

### 3. @keanamazurik5021 (ğŸ‘ 498)
Ai should be doing my dishes and laundry so I can make art and music. Not making my art and music so I can do dishes and laundry.

> **@SgtMVTJ** (ğŸ‘ 0): Because skynet doesnâ€™t liberate, it enslaves

> **@custossecretus5737** (ğŸ‘ 15): We already surround ourselves with labour saving devices, washing machines, dishwashers ect. Do we have time to make music and art? No.

> **@Nemoticon** (ğŸ‘ 8): AI is software, not hardware... learn the difference. What you are lokoing for is an engineering solution, not a programming revolution.

> **@KarlKnutson0615** (ğŸ‘ 15): @@Nemoticon It was a joke, dude

> **@Nemoticon** (ğŸ‘ 9): @@KarlKnutson0615 Oh sorry... I didn't realise. Usually it's more obvious when jokes are actually funny.

### 4. @jasonloader8149 (ğŸ‘ 544)
I'm a 3D generalist - modelling, texturing, lighting, animation etc. I've been working in this field for around 30 years. Around 2 months ago I switched my machine off and that was that. There's still work out there but it's drying up fast especially in the advertising sector - my job is toast.  I imagine hundreds of thousands of those involved in the creative industries are counting the days - writers, photographers, animators, illustrators etc. What a tragedy - especially knowing that our legacy work has been harvested and scalped to generate this stuff by tech bro billionaires who couldn't give a shit about us and what makes life worth living.

> **@franciscocerutimahn** (ğŸ‘ 71): Sorry about that bro.  Now I know how painters felt with the first camera.

> **@Calornata** (ğŸ‘ 5): What do u do now?

> **@RoyMaya** (ğŸ‘ 21): I feel for you. I learned to 3D model about 15 years ago, mostly for fun, but even then I felt like this day would come.  Although, I didn't think it would happen this fast!

> **@jasonloader8149** (ğŸ‘ 10): â€‹@@Calornata Thinking mainly. I mean, I've always done this and I've no training or experience in anything else. I'm decorating my sister's flat while pondering my limited options.

> **@sonofglory7029** (ğŸ‘ 11): I'm an artist as well  and this is a Godsend this is the next step look at the positive as well and thanks for you service.

### 5. @cryptotharg7400 (ğŸ‘ 514)
This is, frankly, terrifying. The world is already run on lies and BS. This just makes it orders of magnitude worse.

> **@ajwright85** (ğŸ‘ 36): It makes it more untrustable, which could be a good thing, as people will learn to not trust ANY source they don't know.  We're going to have to revert back to word of mouth from first-hand experiences.  A trusted network of local news built on real world relationships.

> **@caphunterx2322** (ğŸ‘ 14): It's gonna change everything, if we didn't see it it didn't happen.

> **@DavidKohyarRealtor** (ğŸ‘ 2): This could also make everyone a marketer or a film studio.  Could you imagine Billions going to people instead of movie studios?

> **@MrDutch1968** (ğŸ‘ 0): â€‹@@DavidKohyarRealtor When every person can generate a movie, no one will have an audience to the movie they generated. When there is no audience, there is no income.

> **@erhardpostinger1326** (ğŸ‘ 0): @@ajwright85 Apropos Mundpropaganda: da wusste man aufgrund der Alkohol-Fahne, wie ernst man was nehmen soll. 

Ich lese mir jetzt nicht alle Kommentare durch, die von Pessimismus triefen: denkt positiv !
In Zukunft wird man jedes noch so echte Video als Fake abtun kÃ¶nnen !

Trump der VisionÃ¤r hat das noch mit zweifelhaftem Erfolg auch ohne Google Veo 3 geschafft - jetzt kann das jeder !

### 6. @dereknalley (ğŸ‘ 32)
That leg direction morph at 9:40 was terrifying. First he was facing the camera, then he was facing the crowd. Legs never pivoted.

> **@fishisyum** (ğŸ‘ 3): and the drummer and guitarist still playing with no sound, and the shredded shirt texture on the singers stomach.

### 7. @giovalleyk (ğŸ‘ 205)
Now the whole "we live in a simulation" theory doesn't sound that conspiratorial anymore...

> **@PhiTheHuman** (ğŸ‘ 17): "first, there was the Word"... (Or .. Prompt?? o_O rather...)

> **@Miamiburaz** (ğŸ‘ 9): We are all just bunch of prompts and don't even realize it!

> **@Matrix867** (ğŸ‘ 8): Just imagine perhaps another 100 years or even 1000 years of development. It's actually very difficult to think we wouldn't have access to simulations that could be taken as reality.

> **@bpregont** (ğŸ‘ 4): Literally every conspiracy "theory" comes to fruition eventually.  And you wanna hear the best part?  Alex Jones called all of it decades ago.

> **@jackthethrice** (ğŸ‘ 1): @@PhiTheHuman This just shook me O_o "All thing things came to be by the Prompt, and without it, nothing was made."

### 8. @LoopyVisuals (ğŸ‘ 84)
With the awkward pauses at the ends of those clips, it's like the "actors" were waiting for the director to say "cut."
Very impressive though

### 9. @scottkoelsch1293 (ğŸ‘ 31)
The scariest thing to me about AI is that this is has only begun. Itâ€™s only going to get better from this day forward. Get ready..

> **@catbert7** (ğŸ‘ 1): Indeed.  Very few appreciate the pace of innovation, even having witnessed it throughout their lives.

### 10. @VincentBee2532 (ğŸ‘ 81)
We are doomed. Not hyperbole.  The moment you have no idea what is real and what is not, you loose your footing in reality.  You have no trust or confidence in anything unless it right in front of your eyes.  On the up side, live theatre is about to get very, very popular as people crave real-life talent & entertainment.

> **@2157AF** (ğŸ‘ 2): There maybe a limit to how much AI smart slop can invade the visual sphere. How much energy and how much physical graphics cards and cpus can be waisted on this. The better and more realistic visuals, the more tech and energy has to be burnt and computed. Eventually these data centres will be targets for reactionaries and green zealots.

> **@Teebs555** (ğŸ‘ 0): @@2157AF The resources for this are almost irrelevant, when the payoff is complete control of digital reality, its worth it.

> **@sifu64** (ğŸ‘ 2): This is the declassified stuff.

> **@sonkeschmidt2027** (ğŸ‘ 0): Not doomed, we are returning to reality after a wild fantasy ride.

> **@gregoryscott3858** (ğŸ‘ 2): It's always been incredibly easy to tell what is real and what is not.  It still is.  Hint: if it's happening outside of you and in real time, it's real.  If it's on a screen, a page, or in your memory, it's not.  What I find more interesting than 'what is real' is 'what matters.'  Figuring that out is intensely personal, and can take a lifetime, but (for me) it's the only thing that's remotely worthwhile.

### 11. @N34R4T0M4T (ğŸ‘ 155)
There needs to be a law watermarking these and if they have no watermarks and are detected to be AI on any platform they need to be banned, this really is going to be used for misinformation and weaponised by bad actors.

> **@artvsmachine** (ğŸ‘ 3): There's already a checkbox YouTuber's have when they upload a video that uses AI.

> **@OktaFierce** (ğŸ‘ 6): which government agency's will exempt themselves from of course..

> **@john_doe1st** (ğŸ‘ 0): â€‹@@artvsmachinemost channels don't check it.

> **@BabyJesus66** (ğŸ‘ 2): How would that do any good? The ppl who want to use it for bad won't comply and the ppl who want to use it for good won't be the ones you gotta worry about

> **@artvsmachine** (ğŸ‘ 1): @@BabyJesus66 I'm sure they'll develop an algorithm to detect it IF it threatens corporate profits or top-down rule.

### 12. @swissheartydogs (ğŸ‘ 11)
I need to escape into a real book and ride my bike in the fresh air. No tech, just me, free and human.

### 13. @tklyte2 (ğŸ‘ 38)
We've officially reached a new level in tech evolution and it's scary.

### 14. @andrewai2001 (ğŸ‘ 13)
Probably the most helpful and thorough review of Veo3 I've seen warts and all thus far. Thank you!

> **@AngelaDavenport-o6r** (ğŸ‘ 0): Agreed. Thank you.

### 15. @RJMCTV (ğŸ‘ 15)
23:12.  i think the pauses are useful, to get some 'breathing space' when trying to merge clips together, allow some room for cuts.  I would rather have those extra frames, than trying to deal with abrupt endings, with little room to cut

> **@OhHenGee1796** (ğŸ‘ 2): The ones complaining don't do video editing. I saw nothing to be concerned about.

> **@GregorioGrasselli1972** (ğŸ‘ 0): Sure, you're the director, it's up to you to shout "Cut!"
No actor would keep his job if he stopped acting right after his last line.

### 16. @HonanTheLibrarian (ğŸ‘ 6)
7:44 The wide-eyed alien guy is absolutely hilarious. I love when AI does things like this.

> **@AustinBurdette-ke2qy** (ğŸ‘ 1): When he wiped his bare eyes ğŸ¤£

### 17. @michaelheber2073 (ğŸ‘ 89)
Well thatâ€™s the end of background extras in movies and TV and commercial actors

> **@KielerInKanada** (ğŸ‘ 0): @michaelheber2073 I personally think this will kill the acting industry. It all comes down to money.  Why pay real actors millions when AI videos can be generated?  Also, I think certain famous actors or their estates will sell their rights to have AI to use their images for various film productions.

> **@krisiluttinen** (ğŸ‘ 6): We are going to be sick of it in no time.

> **@BabyJesus66** (ğŸ‘ 5): â€‹@@krisiluttinenno because itll keep getting better till you cant tell

> **@TheRealGTV2024** (ğŸ‘ 5): Thatâ€™s already done by CGI

> **@Reprint001** (ğŸ‘ 9): And the next generation will be better and better and better until no actors at all are required, or directors, lighting technicians, camera operators, musicians etc etc.
This is absolutely nothing to get excited about. This is the death of human creativity.

### 18. @marcpaters0n (ğŸ‘ 11)
The specular highlights are insanely good. This is amazing and terrifying.

### 19. @AmericanWriterOriginal (ğŸ‘ 92)
A question I think that is going to become more common to YouTubers is, "Are you human or AI"?

> **@BartvanderHorst** (ğŸ‘ 3): Yeah, and oh what fun...

> **@dr9299** (ğŸ‘ 2): â€¦Or will it matter?

> **@bpregont** (ğŸ‘ 0): https://www.youtube.com/watch?v=RAPbEJls9PE

> **@Tarapeutic** (ğŸ‘ 1): I already get that too often ğŸ˜‚

> **@SmallGuyonTop** (ğŸ‘ 1): What would AI say?

### 20. @RawHeadRay (ğŸ‘ 20)
10:07  thatâ€™s not an awkward pause, thatâ€™s literally the most convincing human moment in this whole video presentation , I think this is where people who e never directed actors will mess up by removing interesting moments

> **@elreyabeja4539** (ğŸ‘ 0): 100%

> **@Exsulator2** (ğŸ‘ 1): It's really good, but I'm glad the hands can still reveal it's AI (for now). Left hand morphs when picking something up. Basically AI struggles to move one hand pose to another hand pose realistically, especially "big" changes

