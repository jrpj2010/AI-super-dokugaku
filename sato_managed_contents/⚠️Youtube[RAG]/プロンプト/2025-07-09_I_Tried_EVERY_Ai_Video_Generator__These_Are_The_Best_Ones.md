# I Tried EVERY Ai Video Generator. These Are The Best Ones

**ãƒãƒ£ãƒ³ãƒãƒ«:** Tao Prompts
**å…¬é–‹æ—¥:** 2025-07-09
**URL:** https://www.youtube.com/watch?v=V-eez-Irfbc

## èª¬æ˜

âœ¨ğŸ‘‡Create Your Own Ai Videos Here (Relevant Links)
Veo 3 (text-to-video & dialogue): https://labs.google/fx/tools/flow 
Hailuo (image-to-video): https://hailuoai.video?invite-code=50199612
Kling (image-to-video): https://klingaiaffiliate.pxf.io/xLmXbd
OpenArt (Create with Multiple AI Video Generators): https://openart.ai/home/?ref=tao
Midjourney: https://www.midjourney.com/
Hedra (Ai Avatar/Lip Sync): https://www.hedra.com/app/home
Runway (not recommended for beginners): https://runwayml.com/

1-on-1 Consultation with me:
https://calendly.com/taoprompts/consultation
PDF Prompt Guides, Tutorials, etc:
https://taoprompts.gumroad.com/
My Instagram:
https://www.instagram.com/taoprompts/

Chapters:
00:00 What are the Best Ai Video Tools?
00:15 Google VEO3
03:45 Hailuo 02
07:34 Kling 2.1
09:46 Lip Sync in Kling 2.1
14:33 OpenArt + Seedance 1.0
17:55 Midjourney Ai Video
22:24 Animate Your Own Pictures in Midjourney
24:48 Hedra Ai Avatar 
26:30 Runway ML
27:55 Act One in Runway
29:20 Character References in Runway

## å­—å¹•

[00:00 - 00:04]
Over the last three years, I've spent

[00:02 - 00:06]
thousands of dollars creating AI videos

[00:04 - 00:07]
in all the different AI video platforms.

[00:06 - 00:09]
But I'm going to save you the time and

[00:07 - 00:11]
money of having to go and test all of

[00:09 - 00:13]
these tools out for yourself, and share

[00:11 - 00:15]
with you what I think are the best AI

[00:13 - 00:17]
video generators. The first AI video

[00:15 - 00:19]
generator worth talking about. It's

[00:17 - 00:20]
Google Veo3. What's special is its

[00:19 - 00:23]
ability to generate AI sounds,

[00:20 - 00:25]
specifically character dialogue.

[00:23 - 00:26]
These times are hard, my love, with

[00:25 - 00:29]
little to eat and the sickness

[00:26 - 00:30]
spreading. But we have each other and

[00:29 - 00:33]
our strength will see us through.

[00:30 - 00:35]
And what Veil 3 specializes in is text

[00:33 - 00:37]
to video where all we have to do is

[00:35 - 00:40]
enter a text prompt and the AI creates a

[00:37 - 00:42]
video for us. There's two super popular

[00:40 - 00:45]
video types in Google V3. The first one

[00:42 - 00:46]
is the man on the street interview where

[00:45 - 00:48]
you have someone holding up a microphone

[00:46 - 00:50]
and interviewing another character, for

[00:48 - 00:52]
example. And we can do this in all sorts

[00:50 - 00:54]
of different settings. For example, in

[00:52 - 00:56]
the text prompt, I'm going to say, "A

[00:54 - 00:58]
man on the street interview inside a

[00:56 - 01:00]
medieval castle. A peasant reporter

[00:58 - 01:02]
interviews a knight wearing dirty,

[01:00 - 01:04]
damaged armor about what life is like

[01:02 - 01:06]
living under siege fighting against the

[01:04 - 01:08]
French." Then we can roughly describe

[01:06 - 01:10]
the environment that they're in. There's

[01:08 - 01:11]
a battle going on in the background. By

[01:10 - 01:13]
the way, if you're going to use Google

[01:11 - 01:16]
V3, make sure you actually select the

[01:13 - 01:18]
newest quality model. Otherwise, it'll

[01:16 - 01:20]
default to using one of the older V2

[01:18 - 01:23]
models. Now, if we go ahead and run

[01:20 - 01:24]
this, it'll generate a complete

[01:23 - 01:26]
interview video for us.

[01:24 - 01:28]
So, Knight, what's it like living under

[01:26 - 01:30]
siege, fighting the French?

[01:28 - 01:32]
It's relentless. We hold the line, but

[01:30 - 01:34]
every day is a struggle for survival.

[01:32 - 01:39]
The other super popular format in Google

[01:34 - 01:41]
V3 is the AI vlogs where it recreates an

[01:39 - 01:43]
influencer vlogging about their life.

[01:41 - 01:44]
And what's fun about this is people have

[01:43 - 01:46]
been putting them in all sorts of

[01:44 - 01:48]
different situations and with all sorts

[01:46 - 01:50]
of different characters. So, the first

[01:48 - 01:52]
thing we'll need to do is describe the

[01:50 - 01:54]
character. A young man with red hair,

[01:52 - 01:55]
brown eyes. He shows himself in a

[01:54 - 01:58]
medieval training ground with other

[01:55 - 02:00]
knights. Make sure you add in this part

[01:58 - 02:01]
that says self aie camera angle shot

[02:00 - 02:03]
from an extended arm. That's what's

[02:01 - 02:05]
going to create the vlogging effect.

[02:03 - 02:07]
We are preparing for battle. The

[02:05 - 02:10]
training is intense, but we will be

[02:07 - 02:11]
ready for king and country.

[02:10 - 02:14]
When you put a bunch of them together,

[02:11 - 02:16]
you can create an entire story blog. You

[02:14 - 02:19]
can also create talking AI characters in

[02:16 - 02:22]
Google Veil 3 using reference images of

[02:19 - 02:24]
your own characters. Now, so inside Veil

[02:22 - 02:27]
3, we'll go to frames to video to upload

[02:24 - 02:29]
a reference image of our own characters.

[02:27 - 02:31]
I've already got a couple uploaded here.

[02:29 - 02:33]
So, I'll choose this one of a Yeti

[02:31 - 02:35]
wearing a yellow t-shirt. And then

[02:33 - 02:38]
inside the prompt, I'll directly tell

[02:35 - 02:44]
the AI to make a talking video of him.

[02:38 - 02:50]
So, let's go with a vlog of a Yeti and

[02:44 - 02:52]
he talks about life as a Yeti. If we run

[02:50 - 02:54]
this prompt, it should create a talking

[02:52 - 02:57]
dialogue scene with the character we

[02:54 - 03:00]
uploaded. Hey everyone, it's your boy

[02:57 - 03:03]
Yeti back with another vlog from my

[03:00 - 03:05]
snowy forest home. So, if you have

[03:03 - 03:07]
multiple reference images of the same

[03:05 - 03:09]
consistent character, you can generate a

[03:07 - 03:12]
complete vlog of the same exact

[03:09 - 03:14]
character using the frames to video

[03:12 - 03:16]
feature. What's up, my dudes? It's an

[03:14 - 03:19]
awesome day for snowboarding. The powder

[03:16 - 03:23]
is fresh and the sun is shining. Let's

[03:19 - 03:23]
shred some Yar.

[03:27 - 03:31]
Another thing to note is that Veo3 is

[03:29 - 03:33]
fairly expensive, at least relatively

[03:31 - 03:36]
compared to the other platforms. It's a

[03:33 - 03:38]
dollar for an 8-second video. So, Veil 3

[03:36 - 03:40]
is great at generating videos from

[03:38 - 03:42]
texts. But what if you want to use a

[03:40 - 03:44]
reference image? There are some really

[03:42 - 03:47]
great options out there. One of them is

[03:44 - 03:50]
Hya AI, which I think is the best

[03:47 - 03:52]
imagetovideo AI out there. Specifically,

[03:50 - 03:55]
it's really, really good at following

[03:52 - 03:57]
your instructions in the prompts. It

[03:55 - 03:59]
also has a huge variety of camera

[03:57 - 04:01]
movements we can add into the videos.

[03:59 - 04:03]
So, I'll put in this photo of a knight

[04:01 - 04:06]
riding through the street in a parade.

[04:03 - 04:07]
Then below, let's adding a prompt. A

[04:06 - 04:09]
parade riding through the street. The

[04:07 - 04:11]
knight raises his palm wearing a glove

[04:09 - 04:13]
and suddenly stops moving. He takes his

[04:11 - 04:15]
helmet off to reveal a scarred face

[04:13 - 04:17]
wearing an eye patch. Now, when we go

[04:15 - 04:19]
and generate the video, you'll want to

[04:17 - 04:21]
make sure you have this newest Hylo 2

[04:19 - 04:23]
model selected, which is the newest and

[04:21 - 04:25]
the best. There's a few different

[04:23 - 04:28]
settings that we can choose from. A

[04:25 - 04:30]
lower resolution at 768p.

[04:28 - 04:31]
And if you use that lower resolution, we

[04:30 - 04:34]
can choose between a 6second video or a

[04:31 - 04:36]
10-second video. Now, there's also a

[04:34 - 04:38]
full HD 1080p resolution, but if we use

[04:36 - 04:40]
the higher resolution setting, we can

[04:38 - 04:42]
only generate a 6-second video. So, for

[04:40 - 04:44]
me, I like to generate the longer

[04:42 - 04:46]
10-second videos. It gives me a little

[04:44 - 04:48]
more room to work with. Then, let's go

[04:46 - 04:49]
ahead and create this video down here.

[04:48 - 04:52]
The main thing that stands out

[04:49 - 04:54]
specifically about Hyua is how well it

[04:52 - 04:57]
follows your prompts. Yeah, that's

[04:54 - 04:59]
exactly what I expected to happen. The

[04:57 - 05:01]
guy looks pretty tough. Um, but if you

[04:59 - 05:02]
look at the details on the face, he has

[05:01 - 05:05]
the eye patch, he has the scars on his

[05:02 - 05:07]
face

[05:05 - 05:09]
where I told the ogre to pick up the

[05:07 - 05:11]
woman and put her so that she's sitting

[05:09 - 05:12]
on his shoulder. Out of all the

[05:11 - 05:14]
different platforms I tried, I think Hyo

[05:12 - 05:17]
created the best animation for this

[05:14 - 05:19]
specifically. I mean, it's not perfect.

[05:17 - 05:21]
If you look super closely at the woman's

[05:19 - 05:24]
face, it does smooth out the details a

[05:21 - 05:26]
little bit, but the animation quality is

[05:24 - 05:28]
really, really, really good. We can also

[05:26 - 05:31]
introduce tons of camera movements. In

[05:28 - 05:33]
this case, I told the camera to fly to a

[05:31 - 05:35]
bird's eye view to see him riding into

[05:33 - 05:37]
the forest. One thing that's unique

[05:35 - 05:40]
about Hyua is that it has this director

[05:37 - 05:43]
mode down here in the camera icon, which

[05:40 - 05:46]
allows us to adding a bunch of pre-made

[05:43 - 05:48]
camera movements into the videos that we

[05:46 - 05:51]
generate. So, if I have this image of a

[05:48 - 05:52]
queen from behind, uh, I can give it a

[05:51 - 05:54]
prompt like the knights kneel on the

[05:52 - 05:56]
ground in front of the queen. She looks

[05:54 - 05:59]
around with a defined expression and

[05:56 - 06:00]
takes of her tiara rock. But in the

[05:59 - 06:02]
middle of my prompt, I can actually

[06:00 - 06:05]
adding a camera movement for it to

[06:02 - 06:07]
follow. So, if we go ahead and select

[06:05 - 06:08]
this director's mode, there's a bunch of

[06:07 - 06:11]
different cool options for us right

[06:08 - 06:13]
here. Um, think I'm going to go with

[06:11 - 06:16]
this right circling movement. I want the

[06:13 - 06:18]
camera to rotate around the character.

[06:16 - 06:20]
Let's add that in there. And you'll see

[06:18 - 06:22]
this new keywords called truck right,

[06:20 - 06:23]
pan left, tracking shot that just

[06:22 - 06:25]
describes a camera movement for it to

[06:23 - 06:28]
follow. Because of all the different

[06:25 - 06:30]
cool camera movements you have and also

[06:28 - 06:33]
how closely it follows the prompts, I

[06:30 - 06:36]
think Halo 2 gives you the most control

[06:33 - 06:38]
over the actual AI animation in the

[06:36 - 06:40]
video itself. That's not to say it's

[06:38 - 06:42]
perfect. Of course, if you try to

[06:40 - 06:44]
generate a ton of movement, there are

[06:42 - 06:47]
going to be some deformities. If you

[06:44 - 06:48]
look at the hands between the servants

[06:47 - 06:50]
uh and the princess, you'll see that

[06:48 - 06:52]
there's a little bit of morphing, they

[06:50 - 06:54]
kind of blend together. So, there are

[06:52 - 06:57]
still some pain points when it comes to

[06:54 - 07:00]
AI video.

[06:57 - 07:02]
Hyua is kind of a barebones AI video

[07:00 - 07:04]
generator. So, there's not a ton of

[07:02 - 07:06]
other features in it besides the actual

[07:04 - 07:08]
AI animation itself. Now, it is

[07:06 - 07:10]
relatively cheaper compared to the other

[07:08 - 07:11]
platforms we'll talk about. For the

[07:10 - 07:15]
highest quality plan, it's going to cost

[07:11 - 07:17]
about 83 cents for a 6second video at

[07:15 - 07:21]
full HD. Or if you use a lower

[07:17 - 07:24]
resolution at 768p, it'll be around 52

[07:21 - 07:26]
cents for a 10-second video. So, Hyo is

[07:24 - 07:28]
great at creating videos from images,

[07:26 - 07:30]
but it doesn't have much else going on.

[07:28 - 07:32]
Is there another platform that is also

[07:30 - 07:34]
great at doing that, but has more

[07:32 - 07:36]
additional features we can use? If you

[07:34 - 07:38]
want more variety in the features that

[07:36 - 07:40]
are available, for example, adding lip

[07:38 - 07:42]
sync and dialogue to the characters,

[07:40 - 07:45]
Cling AI is a pretty good option out

[07:42 - 07:48]
there. What really stands out about

[07:45 - 07:50]
Cling is the quality of the details in

[07:48 - 07:53]
the videos you get. The character

[07:50 - 07:55]
animations are super lifelike. The

[07:53 - 07:58]
detail preservation and consistency is

[07:55 - 08:00]
also really strong, especially when it

[07:58 - 08:02]
comes to image to video.

[08:00 - 08:04]
Just like in Hyoa, we'll upload a

[08:02 - 08:06]
reference image of the character we want

[08:04 - 08:08]
to generate. And below that, we'll add

[08:06 - 08:11]
in the prompt that describes what should

[08:08 - 08:13]
happen inside the video. Now, Cling AI

[08:11 - 08:15]
also does have sound generation, which

[08:13 - 08:19]
means if we create a video, it'll also

[08:15 - 08:21]
add sounds to it. Now, the sounds aren't

[08:19 - 08:24]
going to be as good as in Google Vo 3. I

[08:21 - 08:24]
found

[08:28 - 08:33]
for a lot of these, it just sounds like

[08:30 - 08:35]
some kind of weird static background

[08:33 - 08:37]
noise or some wind blowing in the

[08:35 - 08:39]
background. So, I wouldn't count on

[08:37 - 08:43]
using the AI sound generation too much

[08:39 - 08:43]
with these videos.

[08:47 - 08:50]
As far as how good it is at following

[08:48 - 08:53]
the prompt, it's not quite as good as

[08:50 - 08:55]
Haya. Um, if you try to create a lot of

[08:53 - 08:57]
movement, like I tried to add into this

[08:55 - 09:00]
video, you see both of them deform quite

[08:57 - 09:03]
a lot. And it also changes the woman to

[09:00 - 09:05]
like a 3D game engine style versus if we

[09:03 - 09:09]
compare it to the video created in Hyoa.

[09:05 - 09:11]
The Hyoa video has higher quality. The

[09:09 - 09:12]
animation is more consistent. The

[09:11 - 09:14]
characters also don't deform and

[09:12 - 09:16]
disfigure like we see in the cling

[09:14 - 09:19]
video. But to be fair, this is a super

[09:16 - 09:20]
super complicated movement for most

[09:19 - 09:23]
things you'll want to animate. Cling

[09:20 - 09:25]
works super well. Unlike Google Video 3,

[09:23 - 09:28]
Cling is not able to generate character

[09:25 - 09:30]
dialogue. Here's a video where I tried

[09:28 - 09:34]
to create two characters talking to each

[09:30 - 09:34]
other. And listen to what happens.

[09:37 - 09:43]
As far as I know, that's not uh actual

[09:40 - 09:45]
human language. So, uh, it'll just

[09:43 - 09:46]
create characters talking gibberish if

[09:45 - 09:49]
you try to make them talk to each other.

[09:46 - 09:51]
The good news is that there is a lip

[09:49 - 09:54]
sync feature that lets you adding your

[09:51 - 09:56]
own dialogue to the shots. So, if you

[09:54 - 09:58]
have a video of a character, in this

[09:56 - 10:01]
case, I have one of the princess just

[09:58 - 10:03]
relaxing on an afternoon. Underneath,

[10:01 - 10:04]
there's this lip syncing which will let

[10:03 - 10:06]
us adding our own audio make the

[10:04 - 10:08]
character speak. It brings up this

[10:06 - 10:10]
option for us to add in the character

[10:08 - 10:12]
dialogue. Now you can enter text to

[10:10 - 10:16]
speech where Cling will generate the

[10:12 - 10:21]
audio for you. So let's try this. Hello,

[10:16 - 10:23]
how are you? Let's try a female voice.

[10:21 - 10:25]
Hello, how are you?

[10:23 - 10:27]
Okay, that sounds kind of like a child

[10:25 - 10:29]
for some reason. So, I'm actually going

[10:27 - 10:33]
to upload my own local audio which I've

[10:29 - 10:33]
created already.

[10:35 - 10:39]
Let's get that in there. Now let's go

[10:38 - 10:42]
ahead and generate this.

[10:39 - 10:46]
If only time could pause in moments like

[10:42 - 10:49]
these. No quarters, no council, just the

[10:46 - 10:52]
breeze, the scent of lavender.

[10:49 - 10:54]
If you look carefully at the lips, it's

[10:52 - 10:57]
not super realistic. A lot of the times

[10:54 - 10:58]
the lips only move a little bit, but it

[10:57 - 11:02]
is a cool tool to have inside the video

[10:58 - 11:04]
generator. Cling also does give you the

[11:02 - 11:07]
ability to add lip sync with videos that

[11:04 - 11:09]
have multiple characters inside. In this

[11:07 - 11:11]
video, I have two characters, the queen

[11:09 - 11:14]
and the king. Um, and I have them

[11:11 - 11:17]
looking towards each other. So, if we go

[11:14 - 11:19]
to the lip sync button inside, it

[11:17 - 11:21]
actually has detected both characters.

[11:19 - 11:23]
We have one for the queen and also one

[11:21 - 11:24]
for the king. And we can actually add

[11:23 - 11:27]
lip sync for each one of them

[11:24 - 11:30]
separately. So, specifically, if I want

[11:27 - 11:31]
to add lip sync for the queen herself, I

[11:30 - 11:33]
need to select this character. You'll

[11:31 - 11:35]
see her selected down here. Now, if I

[11:33 - 11:37]
selected the king character, which is

[11:35 - 11:39]
character two, you'll see it's a

[11:37 - 11:41]
character number two now in the

[11:39 - 11:44]
timeline. But let's go back to the

[11:41 - 11:47]
queen. From here, we can upload a local

[11:44 - 11:50]
audio file. Let's add that to the

[11:47 - 11:53]
timeline. Now, as far as I can tell, you

[11:50 - 11:56]
can't actually create a video that has

[11:53 - 11:58]
both characters speaking inside. You

[11:56 - 12:00]
need to create the lip sync for each

[11:58 - 12:02]
character separately and then combine

[12:00 - 12:05]
them together in a video editor.

[12:02 - 12:07]
So, let's first generate the uh lip sync

[12:05 - 12:09]
for the queen and then we'll do the king

[12:07 - 12:11]
after that. Here's what it sounds like

[12:09 - 12:15]
when I created it just for the queen.

[12:11 - 12:18]
Do you think they'll remember us kindly

[12:15 - 12:20]
or merely as names carved into stone?

[12:18 - 12:23]
Using that same exact method, I also

[12:20 - 12:25]
generated a lip-sync video for just the

[12:23 - 12:28]
king by himself.

[12:25 - 12:29]
That depends on what we leave behind.

[12:28 - 12:31]
Now, what we'll have to do is combine

[12:29 - 12:33]
both of these together in a video

[12:31 - 12:36]
editor. So, this is pretty simple. All

[12:33 - 12:38]
we have to do is stack the two videos on

[12:36 - 12:40]
top of each other. So, I have this

[12:38 - 12:42]
video, which is a lip-s sync with just

[12:40 - 12:44]
the king. This video beneath, which is a

[12:42 - 12:48]
lip sync of the queen. And what we need

[12:44 - 12:52]
to do is actually crop the top video so

[12:48 - 12:55]
that it just shows the king's dialogue.

[12:52 - 12:57]
So, we'll need to use the video masking,

[12:55 - 13:02]
but it's adding a mask.

[12:57 - 13:03]
and then drag this mask over just the

[13:02 - 13:07]
portion of the video that has the king

[13:03 - 13:09]
inside. So that way we'll be able to see

[13:07 - 13:11]
both the queen and the king talking to

[13:09 - 13:12]
each other. Now let's go ahead and take

[13:11 - 13:16]
a look.

[13:12 - 13:20]
Do you think they'll remember us kindly

[13:16 - 13:23]
or merely as names carved into stone?

[13:20 - 13:24]
That depends on what we leave behind.

[13:23 - 13:26]
Okay, it looks like she whispers

[13:24 - 13:29]
something to him at the end there. Um,

[13:26 - 13:32]
again, the quality of the limb sync not

[13:29 - 13:33]
amazing. Uh, she just moves her lips a

[13:32 - 13:35]
little bit there. It kind of looks

[13:33 - 13:37]
robotic. But anyways, you wanted to

[13:35 - 13:38]
create multiple characters talking to

[13:37 - 13:41]
each other. This is how you could do it

[13:38 - 13:44]
inside Cling. Cling is one of the more

[13:41 - 13:46]
expensive AI video generators.

[13:44 - 13:49]
Especially if we use the newest Cling

[13:46 - 13:51]
2.1 master model, which is going to give

[13:49 - 13:55]
the highest quality. It's going to cost

[13:51 - 13:58]
about a dollar for a 5-second full HD

[13:55 - 14:00]
resolution video or $2 for a 10-second

[13:58 - 14:03]
video every time you generate it. So,

[14:00 - 14:05]
it's up there in terms of the cost. I

[14:03 - 14:07]
think the quality is exceptional. It has

[14:05 - 14:09]
some pretty cool features in it. And if

[14:07 - 14:11]
you're serious about AI film making or

[14:09 - 14:13]
making really high quality videos, Cling

[14:11 - 14:15]
is a good option for you to check out.

[14:13 - 14:17]
So far, we've seen how the different AI

[14:15 - 14:20]
video platforms are good at different

[14:17 - 14:21]
things. Google BO is great at creating

[14:20 - 14:23]
videos from text prompts while a

[14:21 - 14:25]
platform like Cling is much better

[14:23 - 14:27]
suited for generating videos from

[14:25 - 14:29]
reference images, but it is pretty

[14:27 - 14:31]
inconvenient when you have to keep

[14:29 - 14:33]
switching between the different AI video

[14:31 - 14:35]
generators depending on what you need.

[14:33 - 14:37]
Is it possible to get access to the best

[14:35 - 14:39]
features of all the platforms at the

[14:37 - 14:41]
same time? There's aggregator platforms

[14:39 - 14:44]
which give you access to all the

[14:41 - 14:46]
different popular AI video generators

[14:44 - 14:47]
all inside one tool. I'm showing one

[14:46 - 14:50]
called open art and we have access to

[14:47 - 14:53]
the newest cling models, hixverse which

[14:50 - 14:55]
is another AI video generator, Hyua, the

[14:53 - 14:57]
Google veil models. So depending on

[14:55 - 14:58]
exactly what type of video you need to

[14:57 - 14:59]
generate, you can pick and choose

[14:58 - 15:01]
between the different AI videos

[14:59 - 15:04]
available here. I want to try out this

[15:01 - 15:06]
new C dance model that was just recently

[15:04 - 15:08]
released and has been showing some

[15:06 - 15:10]
really really good potential with its

[15:08 - 15:12]
high quality AI videos. Um, I'm going to

[15:10 - 15:14]
be using image to video. Let's let's

[15:12 - 15:17]
make sure I have seed down selected.

[15:14 - 15:19]
Then I'll upload the image I want to

[15:17 - 15:21]
animate and put in the prompt again

[15:19 - 15:24]
describing the princess holding the

[15:21 - 15:27]
flowers. So there is an option to

[15:24 - 15:30]
automatically add an AI sounds. Um let's

[15:27 - 15:33]
go with the highest quality models just

[15:30 - 15:35]
to see what we get. And we'll go ahead

[15:33 - 15:39]
and create this

[15:35 - 15:39]
[Music]

[15:44 - 15:49]
So, I think the AI sound generated is

[15:46 - 15:50]
basically background music. So, you

[15:49 - 15:52]
won't really hear sounds from her moving

[15:50 - 15:55]
around or anything like that. Cance is

[15:52 - 15:57]
pretty good. It gives you the super high

[15:55 - 16:00]
quality of detail that you get using the

[15:57 - 16:01]
newest cling model. And at the same

[16:00 - 16:04]
time, it's also decent at following the

[16:01 - 16:06]
prompts. Overall, it's pretty solid. But

[16:04 - 16:08]
if you look super closely right here

[16:06 - 16:10]
where the castle is collapsing, the

[16:08 - 16:12]
tower kind of just vanishes into the

[16:10 - 16:14]
ground. While you would probably

[16:12 - 16:16]
actually expect this to fall apart into

[16:14 - 16:19]
a bunch of little rocks and bricks and

[16:16 - 16:21]
rubble, it kind of just sinks into the

[16:19 - 16:24]
earth. So, when it comes to like the

[16:21 - 16:26]
physics and stuff, it's not as accurate.

[16:24 - 16:28]
Some of the videos it generates also

[16:26 - 16:31]
have a little bit of weird quirkiness to

[16:28 - 16:33]
the emotions. uh here the way that she's

[16:31 - 16:35]
pumping her hands up and down. We

[16:33 - 16:37]
wouldn't actually expect anyone to move

[16:35 - 16:39]
like that in a real movie or in real

[16:37 - 16:42]
life. So, I also generate that same

[16:39 - 16:44]
video using the newest clean 2.1 model.

[16:42 - 16:46]
For one, it follows our prompt more

[16:44 - 16:48]
closely. You see her taking her fist and

[16:46 - 16:50]
slamming it down. And the shouting

[16:48 - 16:52]
motion and the anger that she's showing

[16:50 - 16:54]
is much more realistic compared to the

[16:52 - 16:57]
weird fist pumping motion that was

[16:54 - 16:58]
inside C dance. It's also not quite as

[16:57 - 17:01]
good as following the instructions. you

[16:58 - 17:03]
give it in the prompts. So, for this

[17:01 - 17:06]
video, I asked for the camera to fly to

[17:03 - 17:07]
a bird's eye view above the rider to

[17:06 - 17:09]
follow him riding through the forest.

[17:07 - 17:11]
And you can see that the angle of the

[17:09 - 17:12]
camera doesn't change. It's below him

[17:11 - 17:15]
and slightly tilts up for the entire

[17:12 - 17:18]
video. If I run the same exact video and

[17:15 - 17:20]
prompt inside Hyoa, you'll see that it

[17:18 - 17:22]
follows my instructions way more

[17:20 - 17:25]
closely. The camera flies above and

[17:22 - 17:28]
tilts down to watch the rider as he

[17:25 - 17:30]
rides into the forest.

[17:28 - 17:32]
So, I wouldn't really recommend this sea

[17:30 - 17:34]
dance model even though it seems to be

[17:32 - 17:36]
kind of popular right now. From what

[17:34 - 17:39]
I've seen in the majority of character

[17:36 - 17:42]
animations, it's not as good as cling or

[17:39 - 17:44]
hyoa. Even though it costs about the

[17:42 - 17:46]
same. While seed dance can create some

[17:44 - 17:49]
really high quality and detailed videos,

[17:46 - 17:51]
it doesn't necessarily excel at any

[17:49 - 17:55]
particular thing compared to the other

[17:51 - 17:55]
best AI video models out there.

[17:55 - 17:59]
Next up is Midjourney's AI video

[17:57 - 18:01]
generator. Midjourney has been one of

[17:59 - 18:03]
the most powerful AI image generators.

[18:01 - 18:05]
Finally, they've also released their own

[18:03 - 18:07]
AI video model. I'm just on their

[18:05 - 18:10]
homepage. And you can see the huge

[18:07 - 18:12]
variety of different visual styles,

[18:10 - 18:15]
whether it's a more retro old school

[18:12 - 18:17]
style of video. Here's another example

[18:15 - 18:19]
of what looks like to be a animated oil

[18:17 - 18:21]
painting with these bright bold colors.

[18:19 - 18:23]
I'm not sure exactly what this is, but

[18:21 - 18:26]
it's kind of a cute little style.

[18:23 - 18:29]
Midjourney's video generator is a little

[18:26 - 18:31]
unique in that it only has imagetovideo,

[18:29 - 18:32]
which means it can't take only text

[18:31 - 18:35]
prompts. You have to create a reference

[18:32 - 18:37]
image first. Luckily for us, MidJourney

[18:35 - 18:39]
is probably the best image generator out

[18:37 - 18:41]
there. So, it's very, very easy to get

[18:39 - 18:44]
highquality reference images. To create

[18:41 - 18:46]
a reference image, I'll first use this

[18:44 - 18:48]
prompt barb to describe the photo I'm

[18:46 - 18:50]
looking for. In this case, I'm going

[18:48 - 18:52]
with a close-up photo of a king sitting

[18:50 - 18:54]
on a throne and a cinematic medieval

[18:52 - 18:57]
film shot on single film with muted

[18:54 - 18:59]
colors. Midi really excels at

[18:57 - 19:01]
highquality photorealistic images. Now,

[18:59 - 19:04]
if we hover over the images, there's

[19:01 - 19:06]
this animate button we can hit, which

[19:04 - 19:09]
will immediately start turning our

[19:06 - 19:11]
images into an AI video. It's one of the

[19:09 - 19:14]
fastest compared to the other video

[19:11 - 19:16]
platforms and it'll generate four

[19:14 - 19:19]
different variations of that video at

[19:16 - 19:21]
the same time. So these are five second

[19:19 - 19:23]
videos. The details look really, really

[19:21 - 19:25]
sharp. The animation quality is good.

[19:23 - 19:28]
But if we go and hover over one of them,

[19:25 - 19:30]
we can actually extend the length by 4

[19:28 - 19:32]
seconds. So let's go ahead and try it

[19:30 - 19:35]
with this one that's zooming in on his

[19:32 - 19:37]
face a little bit. And I'm curious to

[19:35 - 19:39]
see where it extends it in the next 4

[19:37 - 19:41]
seconds. What's really cool about this

[19:39 - 19:45]
AI video is that you can extend it all

[19:41 - 19:47]
the way up to a 21 second video, which

[19:45 - 19:50]
is super long compared to the other

[19:47 - 19:51]
platforms that's out there. Here's the

[19:50 - 19:54]
video I generated of this character that

[19:51 - 19:56]
extends all the way up to 17 seconds.

[19:54 - 19:59]
Now, you'll notice that some of the

[19:56 - 20:00]
movements are a little sudden. Uh, he

[19:59 - 20:03]
looks like he's shifting around a little

[20:00 - 20:05]
uncomfortably. That is a characteristic

[20:03 - 20:07]
of the MidJourney AI video. Some of the

[20:05 - 20:09]
movements are a little sudden. It feels

[20:07 - 20:11]
like they move around a little bit too

[20:09 - 20:14]
much sometimes. It does have to squeeze

[20:11 - 20:17]
your entire prompt into a five-second

[20:14 - 20:19]
video and so it does tend to animate a

[20:17 - 20:21]
ton of motion very very quickly

[20:19 - 20:23]
sometimes.

[20:21 - 20:26]
Now you do have a little bit of control

[20:23 - 20:30]
over how much animation is added in. If

[20:26 - 20:32]
we click on one of these images below in

[20:30 - 20:36]
the animate image section there's a low

[20:32 - 20:39]
motion and also a higher motion option.

[20:36 - 20:40]
So, let's take a look at both of these.

[20:39 - 20:42]
Uh, I'll select both of them to create

[20:40 - 20:44]
the videos with low and high motion, and

[20:42 - 20:46]
we'll do a comparison between them. If

[20:44 - 20:48]
we take a look at the lower motion

[20:46 - 20:51]
videos, it looks like he picks up his

[20:48 - 20:53]
sword. Uh, for some reason, they seem to

[20:51 - 20:55]
transform into different looking swords

[20:53 - 20:58]
and all of them. This one on the left

[20:55 - 21:00]
looks probably the most normal, whereas

[20:58 - 21:02]
the one on the right looks like he's

[21:00 - 21:03]
picking up a microphone or something.

[21:02 - 21:05]
So, I'm not sure what that is. Now,

[21:03 - 21:07]
let's take a look at the higher motion

[21:05 - 21:09]
videos.

[21:07 - 21:12]
Yeah, definitely a bit more jittery. His

[21:09 - 21:13]
head is looking around some more. So,

[21:12 - 21:15]
there's a bit of a difference between

[21:13 - 21:17]
the low and high motion settings.

[21:15 - 21:20]
Although, I found that even when you use

[21:17 - 21:23]
low motion animation, it still creates

[21:20 - 21:26]
quite a bit of movement. Now, you can

[21:23 - 21:28]
also manually enter your own prompts.

[21:26 - 21:30]
So, once again, let's go ahead and

[21:28 - 21:32]
select an image. And underneath here in

[21:30 - 21:34]
the animate image section, there's this

[21:32 - 21:37]
manual

[21:34 - 21:39]
uh feature which lets us use a prompt to

[21:37 - 21:42]
add instructions to tell the AI video

[21:39 - 21:44]
what character motions to animate. So

[21:42 - 21:47]
let's go with a

[21:44 - 21:50]
low motion animation. And then up here,

[21:47 - 21:55]
uh we'll be able to enter a prompt. So,

[21:50 - 22:00]
let's go with the ogre takes up the

[21:55 - 22:02]
woman so that she is sitting on his

[22:00 - 22:06]
shoulder.

[22:02 - 22:08]
Let's try that. So, not bad at all. Um,

[22:06 - 22:09]
it looks like for most of these he

[22:08 - 22:11]
doesn't actually have her sitting on his

[22:09 - 22:14]
shoulder, but like I said, even if you

[22:11 - 22:16]
use a lower motion setting, it can still

[22:14 - 22:18]
create some really, really dynamic

[22:16 - 22:20]
animations. And these don't look bad at

[22:18 - 22:23]
all. especially

[22:20 - 22:25]
this uh one in the top left right here.

[22:23 - 22:28]
By the way, you are actually able to

[22:25 - 22:30]
animate pictures of yourself as well.

[22:28 - 22:32]
This was myself on vacation a while ago.

[22:30 - 22:35]
There isn't actually a feature to do

[22:32 - 22:37]
this. We'll have to use a workaround.

[22:35 - 22:40]
So, what we need to do is go to this

[22:37 - 22:42]
edit image button. Let's hit that. And

[22:40 - 22:44]
then from here, I'll actually upload a

[22:42 - 22:46]
photo of myself. There's me on vacation

[22:44 - 22:49]
again. What I'll actually need to do is

[22:46 - 22:52]
using this eraser brush right here,

[22:49 - 22:54]
actually select a tiny portion of the

[22:52 - 22:57]
image that I'm going to erase and

[22:54 - 23:00]
impaint. Then what we'll do is use

[22:57 - 23:02]
midjourney to actually ink paint in this

[23:00 - 23:04]
part of the image. Now I know this seems

[23:02 - 23:06]
very complicated, but basically what we

[23:04 - 23:08]
have to do is use midjourney to generate

[23:06 - 23:11]
a tiny variation of our image and then

[23:08 - 23:14]
we'll be able to animate that variation

[23:11 - 23:16]
of the image. So, I've erased a small

[23:14 - 23:18]
portion uh in the top corner of this

[23:16 - 23:20]
photo right here. And inside the prompt,

[23:18 - 23:23]
I tell it to generate a tiny variation

[23:20 - 23:25]
for the part of the image that I erased.

[23:23 - 23:29]
So, let's just use a super super simple

[23:25 - 23:32]
prompt actually. Photo on vacation.

[23:29 - 23:35]
Then, we can submit this edit button.

[23:32 - 23:37]
So, if we go through these uh we can see

[23:35 - 23:40]
that the top corner area is going to be

[23:37 - 23:43]
slightly different in all of them. um

[23:40 - 23:45]
although barely noticeable. And then

[23:43 - 23:48]
we'll need to hit this upscale to

[23:45 - 23:51]
gallery which is actually going to put

[23:48 - 23:54]
this image into the midjourney database.

[23:51 - 23:57]
Let's use that. And then if we go back

[23:54 - 24:01]
to this create tab, we can see our new

[23:57 - 24:03]
photo being upscale here. And once

[24:01 - 24:05]
that's finished, we can simply animate

[24:03 - 24:08]
it just like any other photo generated

[24:05 - 24:10]
inside midjourney. And by doing that, I

[24:08 - 24:12]
was able to animate photos of myself.

[24:10 - 24:14]
You can obviously do this with photos of

[24:12 - 24:17]
yourself or any other type of picture

[24:14 - 24:20]
you have. Here's a same image animated

[24:17 - 24:23]
using higher motion settings. And I do a

[24:20 - 24:26]
360 spin. Not sure I'd actually do that

[24:23 - 24:28]
in real life, but I really do like Mid

[24:26 - 24:30]
Journey's video generator. The quality

[24:28 - 24:33]
is good. I love the fact that you can

[24:30 - 24:35]
extend these videos all the way up to 21

[24:33 - 24:37]
seconds. and it does have an unlimited

[24:35 - 24:39]
plan where you can generate as many

[24:37 - 24:41]
images and videos as you want without

[24:39 - 24:42]
running out of credits. The next AI

[24:41 - 24:44]
video platform is a little different.

[24:42 - 24:47]
It's going to be specifically targeted

[24:44 - 24:49]
towards AI avatars and creating lip sync

[24:47 - 24:51]
and that's Hedra AI which I think is one

[24:49 - 24:53]
of the best at adding expressive

[24:51 - 24:55]
dialogue to your characters.

[24:53 - 24:58]
If only time could pause in moments like

[24:55 - 25:01]
these. No courtiers, no council, just

[24:58 - 25:05]
the breeze, the scent of lavender. So

[25:01 - 25:08]
inside Hedraw, if we go to the videos

[25:05 - 25:10]
option, we can add in a image of a

[25:08 - 25:12]
character to animate. So let's go with

[25:10 - 25:15]
that photo of a princess from before.

[25:12 - 25:18]
And then we can add in an audio script

[25:15 - 25:20]
with dialogue for the character to say.

[25:18 - 25:23]
So if we hit this, let's actually

[25:20 - 25:24]
generate the speech. There's a bunch of

[25:23 - 25:27]
different voices up here that we can

[25:24 - 25:28]
choose from. I found this one called

[25:27 - 25:31]
Lily.

[25:28 - 25:31]
I have never been hurt by anything. I

[25:31 - 25:33]
didn't say

[25:31 - 25:34]
that should work well for our princess.

[25:33 - 25:36]
And then down here, we can put in a

[25:34 - 25:38]
dialogue for them to say. I'll add in a

[25:36 - 25:40]
couple lines that I want the AI

[25:38 - 25:43]
character to say. And then if we

[25:40 - 25:46]
generate this character, it'll actually

[25:43 - 25:49]
lip-s sync the dialogue we added in to

[25:46 - 25:52]
the headshot of the character to create

[25:49 - 25:55]
a talking expressive character.

[25:52 - 25:58]
At last, a moment untouched by duty. The

[25:55 - 26:01]
crown may weigh heavy, but here with the

[25:58 - 26:02]
sun on my skin and silence in the air, I

[26:01 - 26:04]
feel almost human again.

[26:02 - 26:06]
The videos that you generate are a

[26:04 - 26:08]
little bit wobbly. If you look around

[26:06 - 26:11]
the head, it bobbles around a little

[26:08 - 26:14]
bit. That is one issue you might run

[26:11 - 26:16]
into with Hedra. But in terms of the

[26:14 - 26:18]
expressiveness of the lip sync and all

[26:16 - 26:20]
the different hand gestures and body

[26:18 - 26:23]
motions, I do think it's one of the most

[26:20 - 26:26]
impressive AI lips sync video generators

[26:23 - 26:28]
out there. One of the most well-known AI

[26:26 - 26:31]
video generators that we haven't talked

[26:28 - 26:33]
about yet is Runway ML. Before diving

[26:31 - 26:36]
into it, I do think Runway has

[26:33 - 26:38]
absolutely the best marketing team,

[26:36 - 26:41]
which is why they're so popular. Do the

[26:38 - 26:43]
AI video generation capabilities match

[26:41 - 26:46]
up with how wellknown they are? To be

[26:43 - 26:49]
honest, not really. When it comes to the

[26:46 - 26:51]
actual AI animations, I'd say Runway is

[26:49 - 26:53]
one of the weaker ones compared to all

[26:51 - 26:55]
the other platforms we talked about.

[26:53 - 26:58]
Whether that's Google Veil, Clean Hya,

[26:55 - 27:01]
Mid Journey, Sea Dance, the animations

[26:58 - 27:03]
just aren't that good. If we take a look

[27:01 - 27:05]
at this one of the ogre picking up the

[27:03 - 27:08]
woman, it looks like another character

[27:05 - 27:09]
suddenly flies out of nowhere. I'm not

[27:08 - 27:12]
sure what's going on. He's just holding

[27:09 - 27:14]
on to something in the air.

[27:12 - 27:16]
Again, the another character just

[27:14 - 27:18]
suddenly elevates and floats into the

[27:16 - 27:22]
air. For this video of the grieving

[27:18 - 27:24]
female knight, the movements look okay,

[27:22 - 27:26]
but I mean, if you compare it to the

[27:24 - 27:28]
same exact video generated inside

[27:26 - 27:31]
Clling, there's a huge difference in

[27:28 - 27:32]
terms of the animation quality, the

[27:31 - 27:35]
sharpness of the details that are

[27:32 - 27:35]
preserved.

[27:38 - 27:42]
Okay, so why did I include runway in

[27:40 - 27:44]
this list then? Well, for one, it's

[27:42 - 27:45]
super super popular. Everybody talks

[27:44 - 27:47]
about it, even though I don't think the

[27:45 - 27:50]
animation qualities are that high, but

[27:47 - 27:51]
there are a few features inside runway

[27:50 - 27:54]
that I think are really promising and

[27:51 - 27:56]
really cool. One of them is the act one

[27:54 - 27:59]
feature where you can use a performance

[27:56 - 28:01]
from an actor. In this case, I used an

[27:59 - 28:04]
AI animated headshot of a man and drive

[28:01 - 28:06]
the performance of another character

[28:04 - 28:09]
inside a video. In this case, the king

[28:06 - 28:11]
sitting on the throne. And what Runway

[28:09 - 28:13]
does is detect the facial movements, the

[28:11 - 28:16]
dialogue of the driving performance from

[28:13 - 28:18]
the character and map it into our AI

[28:16 - 28:20]
video, which I think is a super super

[28:18 - 28:24]
useful tool. Using this feature to make

[28:20 - 28:26]
an AI actor is pretty easy. All we need

[28:24 - 28:29]
to do is go to the runway dashboard and

[28:26 - 28:31]
find this act one feature down at the

[28:29 - 28:33]
bottom. Then inside we can add in a

[28:31 - 28:36]
driving performance. This is going to be

[28:33 - 28:38]
the video of our actor. And so in this

[28:36 - 28:41]
case, I actually used Hedra AI to

[28:38 - 28:44]
generate a video of a voice actor. What

[28:41 - 28:47]
I'll do is actually upload this AI video

[28:44 - 28:50]
of a king that I generated inside Mid

[28:47 - 28:54]
Journey from earlier. And what Ren will

[28:50 - 28:57]
do is actually map the AI actor's facial

[28:54 - 28:59]
movements and dialogue onto this video

[28:57 - 29:01]
of a king. And let's go ahead and

[28:59 - 29:02]
generate it so you can see what it looks

[29:01 - 29:04]
like.

[29:02 - 29:07]
When I was young, I believed the crown

[29:04 - 29:09]
would make me powerful, that I would

[29:07 - 29:12]
bend the world to my will with mere

[29:09 - 29:15]
words and decrees. But power is not

[29:12 - 29:17]
control. It is responsibility.

[29:15 - 29:20]
This is a really, really cool way where

[29:17 - 29:23]
you could actually use a human actor and

[29:20 - 29:25]
then turn them into an AI character.

[29:23 - 29:28]
Another powerful feature inside runway

[29:25 - 29:30]
is their consistent character references

[29:28 - 29:32]
feature which lets you take reference

[29:30 - 29:35]
photos of your character and put them

[29:32 - 29:37]
inside scene. So in this case I have

[29:35 - 29:40]
this photo of an orc and also this

[29:37 - 29:43]
hooded character and I put both of them

[29:40 - 29:45]
together inside the same landscape. And

[29:43 - 29:46]
this is a really powerful way to build

[29:45 - 29:50]
an entire world from just a few

[29:46 - 29:51]
reference images. So, inside runway, uh,

[29:50 - 29:53]
we've been generating videos, but I'm

[29:51 - 29:55]
actually going to go to this image

[29:53 - 29:57]
creation tab, and underneath you'll see

[29:55 - 29:59]
the references,

[29:57 - 30:01]
which can be the individual photos of

[29:59 - 30:04]
our characters. I'm going to go and

[30:01 - 30:07]
adding a couple new characters. So, I'll

[30:04 - 30:10]
put in this picture of a queen, which I

[30:07 - 30:13]
used earlier, and also this character

[30:10 - 30:15]
image of a king.

[30:13 - 30:17]
And in addition, I'll even adding a

[30:15 - 30:19]
landscape photo of the environment I

[30:17 - 30:21]
want to put them inside. So, let's go

[30:19 - 30:24]
ahead and drag that in there, too. All

[30:21 - 30:27]
right. So, you can't for some reason see

[30:24 - 30:29]
the entire body of the character, but

[30:27 - 30:32]
I'll give them a name. So, I'll call the

[30:29 - 30:35]
female character queen.

[30:32 - 30:38]
I'll call the male character king.

[30:35 - 30:40]
And I'll call this landscape

[30:38 - 30:41]
castle.

[30:40 - 30:43]
And then to generate the image I can

[30:41 - 30:51]
simply refer to the character

[30:43 - 30:53]
references. So uh at queen and at king

[30:51 - 30:57]
are walking

[30:53 - 30:58]
through the at castle. And what this

[30:57 - 31:01]
should do is generate images of the

[30:58 - 31:04]
queen and king characters walking inside

[31:01 - 31:06]
the castle landscape. Okay. So, it

[31:04 - 31:10]
actually doesn't look like it copy over

[31:06 - 31:13]
our characters into the image correctly.

[31:10 - 31:15]
If you look at the king uh right here,

[31:13 - 31:18]
it looks like he's a white guy actually,

[31:15 - 31:21]
but in the original character image I

[31:18 - 31:24]
had, it was supposed to be a black

[31:21 - 31:27]
character. So, I have a theory for why

[31:24 - 31:29]
this didn't work. It might be because if

[31:27 - 31:31]
you look at these character reference

[31:29 - 31:33]
images, it actually cuts off the top

[31:31 - 31:35]
head of the character and also the

[31:33 - 31:37]
bottom of his character. So maybe that's

[31:35 - 31:40]
why it wasn't able to copy over the

[31:37 - 31:42]
exact likeness. So I actually tested

[31:40 - 31:45]
this theory out. This time I'm going

[31:42 - 31:48]
with a wider image of my king character

[31:45 - 31:49]
and also a wider image of my queen

[31:48 - 31:52]
character so that it shows the entire

[31:49 - 31:54]
body. So before I was using a narrow

[31:52 - 31:56]
vertical image of my character. This

[31:54 - 31:59]
time I actually expand the image a bit

[31:56 - 32:02]
so it's using a wider square photo of

[31:59 - 32:04]
the character. So that way inside the

[32:02 - 32:06]
runway references it looks like it can

[32:04 - 32:10]
actually fit the entire character into

[32:06 - 32:12]
the references box. Now when I generated

[32:10 - 32:15]
those images, it gives me a much more

[32:12 - 32:16]
accurate depiction of what these two

[32:15 - 32:19]
characters would look like if they were

[32:16 - 32:21]
walking through this environment. But

[32:19 - 32:25]
once we've created these photos of our

[32:21 - 32:27]
characters, we can use a video generator

[32:25 - 32:30]
uh just like before to turn them into an

[32:27 - 32:33]
AI video. Not bad. Not too much motion

[32:30 - 32:35]
in there, but like I said, runway isn't

[32:33 - 32:37]
really great when it comes to the actual

[32:35 - 32:39]
AI animation part. Now, it does have a

[32:37 - 32:42]
lot of cool features. For example, you

[32:39 - 32:44]
can upscale the videos to 4K inside

[32:42 - 32:46]
runway, but overall, I probably wouldn't

[32:44 - 32:48]
recommend this as the one you start out

[32:46 - 32:50]
using. If you want to see all these

[32:48 - 32:53]
different AI video generators put to the

[32:50 - 32:55]
test when creating the same exact AI

[32:53 - 32:56]
animation, go take a look at this video

[32:55 - 32:59]
right here where I created the same

[32:56 - 33:02]
animation in all the different AI video

[32:59 - 33:15]
generation platforms.

[33:02 - 33:15]
[Music]

## ã‚³ãƒ¡ãƒ³ãƒˆ

### 1. @DJ-New-Moon (ğŸ‘ 11)
almost 100k lets go my friend ! 
thanks for your great work ! 
always here to like and watch the entire video !ğŸŒš

> **@taoprompts** (ğŸ‘ 1): Almost there! It's a huge milestone. Thank you so much for supporting and watching these guides ğŸ”¥

### 2. @mattc3510 (ğŸ‘ 6)
Congrats on 100k .  Awesome videos

### 3. @HeHasAPlanForYou (ğŸ‘ 6)
Congrats to getting 100k! To all the small creators out here: keep going. Every video is a step forward. Someone out there needs your voiceâ€”donâ€™t quit now.

> **@taoprompts** (ğŸ‘ 0): Thank you so much!

### 4. @soer4964 (ğŸ‘ 16)
Suggestion: could you please make a price/ feature comparison at the end of your videos like a conclusion? 

I really appreciate that you did it at the end of each section, but a graphic with all providers side by side at the end would help a lot to get a fast and clear overview over prices and features. 

Thanks for the very helpful contents you create.

> **@RandallPAulActorDirectorWriter** (ğŸ‘ 0): You can get that in chatgpt

### 5. @MilkyAIWorld (ğŸ‘ 1)
"Finally, a no-hype comparison! ğŸ™Œ Thanks for testing all the tools â€” this saves me hours of trial and error. Subscribed for more honest reviews!"

### 6. @FilmSpook (ğŸ‘ 7)
ğŸ˜„ğŸ‘ğŸ¾ğŸ‘ğŸ¾ I also often add "muted colors" in my prompts in both my image and video generations. Great guide! Thanks again. Looking forward to your 100K subscriber milestone. You'll probably hit 200K or more by the end of the year.  Peace

> **@taoprompts** (ğŸ‘ 0): Thanks man! 100k is a big milestone for the channel ğŸ¾

### 7. @StorySnapWorld (ğŸ‘ 7)
man do you know how blessed you are... just to give you some context, that midjourney video that played instantly, where i live that video doesnt even buffer .. i need to click on one of them and wait 3 min until the 5 sec video buffers.. and i pay for this connection 30 $ per month the speed am getting is less than 400kb/second .. when you live in 3rd world country even mundane things like high speed internet connection becomes like a big hurdle that you cant bypass even if you pay 100 $ per month .. 

people who live in developed countries just dont know how lucky they are ..

> **@AuriSpark** (ğŸ‘ 0): really good point!

### 8. @planetofshireen (ğŸ‘ 7)
What I have seen after experimentation with KLING- it changes the facial features of the reference image. It's not consistent.

> **@Lisawolf007** (ğŸ‘ 1): True

> **@taoprompts** (ğŸ‘ 0): the newer 2.1 models add in a ton of motion, which causes more deformations unfortunately

### 9. @TSLRLab (ğŸ‘ 0)
Congrats on 100k! You are the GOAT of AI tutorials ğŸ™ŒğŸ™Œ

### 10. @JayShiven (ğŸ‘ 0)
Through your guidance sir, released the first episode of the show "The Clash of Powers". Thank you so much for always teaching us. I pray your channel grows multifold this year.

### 11. @NewMarshallPlan (ğŸ‘ 0)
Awesome video again!

### 12. @sonythabeast (ğŸ‘ 3)
You forgot to mention the character reference in Kling ai just like runway does but Kling quality is super great try it

> **@taoprompts** (ğŸ‘ 1): The multi-elements feature in Kling is pretty cool ğŸ‘

### 13. @greggold2 (ğŸ‘ 0)
Congrats on the 100k milestone homie! Youâ€™ve definitely earned it!

> **@taoprompts** (ğŸ‘ 0): Thanks man! I really appreciate that

### 14. @musikausdemosten (ğŸ‘ 0)
19:30 you can extend videos how long if you want when you make a picture of the last frame and put in again

### 15. @JayShiven (ğŸ‘ 0)
Thank you sir for the overview. Congratulations for 100k!

> **@taoprompts** (ğŸ‘ 0): Thanks, I really appreciate that!

### 16. @DJ-New-Moon (ğŸ‘ 2)
moonvalley is back with non copyright video generator and they look very good and ALOT of option for camera movement with time frame ! like a video editor :) and you can change style of a video :) , its not free and it look a little blurry when you use the camera movement fast  but if they go in this way and update new model, moonvalley will be a great competitor

> **@iveywebb44** (ğŸ‘ 0): moonvalley is useless, half the generations fail and they dont refund you.

> **@taoprompts** (ğŸ‘ 0): I haven't had a chance to try the Moonvalley model yet, but from what I saw it looks very promising

### 17. @Skyhall-004 (ğŸ‘ 4)
Congrats on 100K. You missed Neocu. prob my fav one for game based videos. can do up to 4 minutes.

> **@taoprompts** (ğŸ‘ 1): Thank you! I haven't heard of Neocu yet, 4 minutes is a lot though

### 18. @sevenseven31 (ğŸ‘ 5)
Making the sound more realistic is the first thing I faced a while ago since the beginning of making videos with artificial intelligence, and that was before Google and other popular video generation versions, and it was solved with older versions than that, and since it worked for me, I started using it, and now they are proposing solutions that cost hundreds of dollars and they do not even work to make the scene more realistic.ğŸ¤£

### 19. @cingoyenk (ğŸ‘ 4)
The most important aspect that AI video developers need to prioritize is visual consistency between frames. Smooth transitions from one image to the next are key to creating a compelling viewing experience.

For audio, simple narration is quite effectiveâ€”as evidenced by the National Geographic documentary "Wild," which remains captivating despite relying solely on consistent, high-quality visuals and informative narration.

Of the hundreds of AI videos I've observed across various social media platforms, not a single one has maintained its quality beyond the first five minutes. Most exhibit distracting visual inconsistencies, drawing criticism from adult viewers. The question is: is this technology still limited to children's content, or can it evolve into a compelling storytelling medium for all audiences?

### 20. @davehedgehog9795 (ğŸ‘ 4)
Midjourney and Veo3 are the best ones thus far. Midj being my fav

> **@taoprompts** (ğŸ‘ 0): Midjourney's is really good value since it's attached to a really powerful image generator ğŸ‘

