# Discover Prompt Engineering | Google AI Essentials

**チャンネル:** Google Career Certificates
**公開日:** 2024-05-13
**URL:** https://www.youtube.com/watch?v=jNNatjruXx8

## 説明

This video is a preview of Module 3 in Google AI Essentials, available on Coursera. In this module, you'll write effective prompts to get the output you want. You'll learn how to incorporate prompting techniques, such as few-shot prompting, into your work, and you'll understand how LLMs produce output and the importance of evaluating output before using it. By the end of this module, you will be able to write clear and specific prompts and produce outputs that help accomplish workplace tasks. You'll learn from experts at Google and get essential AI skills to boost your productivity with Google AI Essentials, zero experience required. 

In this self-paced course, you’ll gain hands-on experience using generative AI tools to help develop ideas and content, make more informed decisions, and speed up daily work tasks. After you complete the course through Coursera, you’ll earn a certificate from Google to share with your network and potential employers.

Access all videos, readings, and activities: https://goo.gle/49yq5h5
Take the course HERE: https://career.skills.google/paths/2336/?utm_source=youtube&utm_medium=social&utm_campaign=youtube-description__geo--Global__con--AIE__ter--jNNatjruXx8

00:00 Module 3 introduction: Discover the art of prompt engineering
3:05 Understand large language models
8:13 Write clear and specific prompts
12:28 Leverage an LLM's capabilities at work
18:10 Improve AI output through iteration
24:03 Discover few-shot prompting
28:26 Wrap-up


Subscribe HERE: https://bit.ly/SubscribeGCC

#GoogleCareerCertificates #AI #GenerativeAI

About the Google Career Certificates program: 

Google Career Essentials help you build in-demand skills to grow your career. Whether you’re looking to add a skill to your resume, apply a new skill to uplevel your work, or simply learn something new, the course will help you quickly gain valuable competencies. At the end, you’ll earn a certificate to showcase to employers.

Google Career Certificates prepare you for entry-level jobs and take about three to six months to complete. At the end, you’ll unlock job search support including 1-on-1 career coaching, an exclusive job board with 150+ employers, and more.

Google Career Certificates
https://www.youtube.com/@GoogleCareerCertificates

Discover Prompt Engineering | Google AI Essentials
https://youtu.be/jNNatjruXx8

## 字幕

[00:00 - 00:05]
prompt engineering involves designing

[00:02 - 00:08]
the best prompt you can to get the

[00:05 - 00:08]
output you

[00:08 - 00:17]
[Music]

[00:14 - 00:20]
want think about how you use language in

[00:17 - 00:23]
your daily life language is used for so

[00:20 - 00:26]
many purposes to build connections

[00:23 - 00:29]
Express opinions or explain

[00:26 - 00:31]
ideas and sometimes you might want to

[00:29 - 00:34]
use language to prompt others to respond

[00:31 - 00:37]
in a particular way maybe you want

[00:34 - 00:40]
someone to give you a recommendation or

[00:37 - 00:42]
clarify something in those cases the way

[00:40 - 00:46]
you phrase your words can affect how

[00:42 - 00:48]
others respond the same is true when

[00:46 - 00:49]
prompting a conversational AI tool with

[00:48 - 00:53]
a question or

[00:49 - 00:55]
request a prompt is text input that

[00:53 - 00:58]
provides instructions to the AI model on

[00:55 - 01:00]
how to generate output for example

[00:58 - 01:02]
someone who owns a clothing store store

[01:00 - 01:05]
might want an AI model to Output new

[01:02 - 01:06]
ideas for how to Market their clothing

[01:05 - 01:10]
this business owner might write the

[01:06 - 01:12]
prompt I own a clothing store we sell

[01:10 - 01:14]
High fashioned women's wear help me

[01:12 - 01:16]
brainstorm marketing ideas in this

[01:14 - 01:20]
section of the course you'll focus on

[01:16 - 01:23]
how to design or engineer effective

[01:20 - 01:25]
prompts to achieve more useful results

[01:23 - 01:28]
from a conversational AI tool my name is

[01:25 - 01:30]
yuang and I'm an engineer at Google I

[01:28 - 01:32]
first became interested in prompting

[01:30 - 01:35]
because getting useful responses from

[01:32 - 01:37]
language models was timec consuming

[01:35 - 01:40]
sometimes it was even quicker for us to

[01:37 - 01:42]
do the work without the use of AI I was

[01:40 - 01:45]
inspired to help our tools be more

[01:42 - 01:47]
efficient not less I'm excited to help

[01:45 - 01:51]
you learn more about developing

[01:47 - 01:54]
effective prompts first you'll discover

[01:51 - 01:57]
how llms generate output in response to

[01:54 - 01:59]
prompts and then you'll explore the role

[01:57 - 02:02]
of prompt Engineering in improving

[01:59 - 02:05]
proving the quality of the output prompt

[02:02 - 02:07]
engineering is the practice of

[02:05 - 02:11]
developing effective prompts that elicit

[02:07 - 02:13]
useful output from generative AI you'll

[02:11 - 02:16]
learn to create clear and specific

[02:13 - 02:19]
prompts one of the most important parts

[02:16 - 02:21]
of prompt engineering the more clear and

[02:19 - 02:24]
specific your prompt the more likely you

[02:21 - 02:27]
are to get useful output another

[02:24 - 02:30]
important part of prompt engineering is

[02:27 - 02:33]
iteration you'll learn about evaluating

[02:30 - 02:35]
output and revising your prompts this

[02:33 - 02:37]
will also help you get the results you

[02:35 - 02:40]
need when leveraging conversational AI

[02:37 - 02:43]
tools in the workplace we'll also

[02:40 - 02:46]
explore a specific prompting technique

[02:43 - 02:48]
called few shot prompting writing

[02:46 - 02:52]
effective prompts involves critical

[02:48 - 02:56]
thinking and creativity it can also be a

[02:52 - 02:58]
fun process and it's a very important

[02:56 - 03:00]
skill to practice if you want to use AI

[02:58 - 03:02]
effectively in the workplace

[03:00 - 03:05]
are you excited to get started on prompt

[03:02 - 03:09]
engineering let's

[03:05 - 03:12]
go it's helpful to understand how llms

[03:09 - 03:15]
work and to be aware of their

[03:12 - 03:17]
limitations a large language model or

[03:15 - 03:19]
llm is an AI model that is trained on

[03:17 - 03:22]
large amounts of text to identify

[03:19 - 03:24]
patterns between words Concepts and

[03:22 - 03:26]
phrases so that it can generate

[03:24 - 03:29]
responses to

[03:26 - 03:33]
prompts so how do llms learn to generate

[03:29 - 03:35]
useful responses to prompts an llm is

[03:33 - 03:38]
trained on millions of sources of text

[03:35 - 03:42]
including books articles websites and

[03:38 - 03:44]
more this training helps the model learn

[03:42 - 03:47]
the patterns and relationships that

[03:44 - 03:51]
exist in human language in general the

[03:47 - 03:54]
more highquality data the model receives

[03:51 - 03:56]
the better its performance will be

[03:54 - 03:58]
because llms can identify so many

[03:56 - 04:01]
patterns in language they can also

[03:58 - 04:04]
predict what word is most likely to come

[04:01 - 04:06]
next in a sequence of words consider a

[04:04 - 04:07]
simple example to get a basic

[04:06 - 04:10]
understanding of how llms predict the

[04:07 - 04:13]
next word in a sequence take the

[04:10 - 04:18]
incomplete sentence after it rained the

[04:13 - 04:20]
street was an llm can predict what word

[04:18 - 04:22]
comes next by Computing the

[04:20 - 04:25]
probabilities for different possible

[04:22 - 04:27]
words based on the available data the

[04:25 - 04:30]
word wet might have a high probability

[04:27 - 04:33]
of being the next word the word clean a

[04:30 - 04:34]
lower probability and the word dry an

[04:33 - 04:37]
extremely low

[04:34 - 04:39]
probability in this case the llm might

[04:37 - 04:41]
complete the sentence by inserting the

[04:39 - 04:44]
word with the highest probability of

[04:41 - 04:46]
coming next in the sequence wet or it

[04:44 - 04:49]
might be another high probability word

[04:46 - 04:52]
like damn an llm may vary in its

[04:49 - 04:56]
response to the same prompt each time

[04:52 - 04:58]
you use it llms use statistics to

[04:56 - 05:00]
analyze the relationships between all

[04:58 - 05:03]
the words in a given sequence and

[05:00 - 05:06]
compute the probabilities for thousands

[05:03 - 05:09]
of possible words to come next in that

[05:06 - 05:11]
sequence this predictive power enables

[05:09 - 05:14]
llms to respond to questions and

[05:11 - 05:16]
requests whether the prompt is to

[05:14 - 05:18]
complete a simple sentence or to develop

[05:16 - 05:22]
a compelling story for a new product

[05:18 - 05:24]
launch or ad campaign although llms are

[05:22 - 05:28]
powerful you may not always get the

[05:24 - 05:30]
output you want sometimes this is

[05:28 - 05:33]
because of limitations in an lm's

[05:30 - 05:36]
training data for instance an lm's

[05:33 - 05:40]
output may be biased because the data it

[05:36 - 05:42]
was trained on contains bias this data

[05:40 - 05:45]
may include news articles and websites

[05:42 - 05:49]
that reflect the unfair biases present

[05:45 - 05:52]
in society for example because of the

[05:49 - 05:54]
data it was trained on an llm may be

[05:52 - 05:56]
more likely to produce output that

[05:54 - 05:59]
Associates a professional occupation

[05:56 - 06:02]
with a specific gender role the training

[05:59 - 06:05]
data that informs an llm can be limited

[06:02 - 06:08]
in other ways as well for instance an

[06:05 - 06:11]
llm might not generate sufficient

[06:08 - 06:14]
content about a specific domain or topic

[06:11 - 06:16]
because the data it was trained on does

[06:14 - 06:19]
not contain enough information about

[06:16 - 06:24]
that topic another factor that can

[06:19 - 06:27]
affect output is the tendency of llms to

[06:24 - 06:31]
hallucinate hallucinations are AI

[06:27 - 06:33]
outputs that are not true while llms are

[06:31 - 06:35]
good at responding to many kinds of

[06:33 - 06:37]
questions and instructions they can

[06:35 - 06:39]
sometimes generate text that is

[06:37 - 06:41]
factually

[06:39 - 06:44]
inaccurate let's say you're researching

[06:41 - 06:47]
a company and you use an llm to help you

[06:44 - 06:50]
summarize the company's history the llm

[06:47 - 06:52]
might hallucinate and provide incorrect

[06:50 - 06:54]
information about certain details such

[06:52 - 06:57]
as the date the company was founded or

[06:54 - 06:59]
the current number of employees a number

[06:57 - 07:01]
of factors can contribute to

[06:59 - 07:04]
hallucinations such as the quality of an

[07:01 - 07:07]
lm's training data the phrasing of the

[07:04 - 07:10]
prompt or the method an llm uses to

[07:07 - 07:13]
analyze text and predict the next word

[07:10 - 07:16]
in a sequence because of an lm's

[07:13 - 07:19]
limitations it's important that you

[07:16 - 07:22]
critically evaluate all llm output to

[07:19 - 07:25]
determine if it is factually accurate is

[07:22 - 07:28]
unbiased is relevant to your specific

[07:25 - 07:31]
request and provides sufficient

[07:28 - 07:33]
information whether you're using AI to

[07:31 - 07:36]
summarize a lenry report generate ideas

[07:33 - 07:39]
for marketing a product or outline a

[07:36 - 07:42]
project plan be sure to carefully check

[07:39 - 07:44]
the quality of the output finally it's

[07:42 - 07:49]
important not to make assumptions about

[07:44 - 07:51]
an lm's capabilities for example just

[07:49 - 07:53]
because it produced high quality output

[07:51 - 07:56]
for a persuasive letter to a customer

[07:53 - 07:58]
don't assume you will get the same

[07:56 - 08:01]
quality output if you use the same

[07:58 - 08:03]
prompt again in the future large

[08:01 - 08:07]
language models are powerful tools that

[08:03 - 08:10]
require human guidance for Effective use

[08:07 - 08:14]
being aware of an lm's limitations can

[08:10 - 08:14]
help you achieve the best possible

[08:14 - 08:19]
results how can you write prompts that

[08:17 - 08:22]
produce useful

[08:19 - 08:24]
output it's generally true that the

[08:22 - 08:27]
quality of what you start with greatly

[08:24 - 08:29]
affects the quality of what you produce

[08:27 - 08:31]
consider cooking for example let's say

[08:29 - 08:34]
you're preparing dinner if you have

[08:31 - 08:36]
fresh high quality ingredients well

[08:34 - 08:39]
you're more likely to produce a great

[08:36 - 08:41]
meal conversely if you're missing an

[08:39 - 08:43]
ingredient or the ingredients aren't

[08:41 - 08:47]
high quality the resulting meal may not

[08:43 - 08:49]
be as good in a similar way the quality

[08:47 - 08:52]
of the prompt that you put into a

[08:49 - 08:54]
conversational AI tool can affect the

[08:52 - 08:57]
quality of the tools

[08:54 - 09:00]
output this is where prompt engineering

[08:57 - 09:03]
comes in prompt engineering ing involves

[09:00 - 09:07]
designing the best prompt you can to get

[09:03 - 09:10]
the output you want from an llm this

[09:07 - 09:12]
includes writing clear specific prompts

[09:10 - 09:14]
that provide relevant

[09:12 - 09:18]
context to gain a better understanding

[09:14 - 09:22]
of the context llms need let's compare

[09:18 - 09:23]
how a person and an llm might respond to

[09:22 - 09:26]
the same

[09:23 - 09:28]
question suppose a vegetarian ask their

[09:26 - 09:30]
friend what restaurant should I go to in

[09:28 - 09:32]
San Francisco

[09:30 - 09:35]
the friend would likely suggest

[09:32 - 09:37]
restaurants with good vegetarian

[09:35 - 09:40]
options however if prompted with the

[09:37 - 09:43]
same question an llm might recommend

[09:40 - 09:46]
restaurants that are not suitable for a

[09:43 - 09:48]
vegetarian a person would instinctively

[09:46 - 09:50]
consider the fact that their friend is a

[09:48 - 09:53]
vegetarian when answering the question

[09:50 - 09:55]
but an llm does not have this prior

[09:53 - 09:58]
knowledge so to get the needed

[09:55 - 10:02]
information from an llm The Prompt must

[09:58 - 10:04]
be more specific in this case The Prompt

[10:02 - 10:08]
needs to mention that the restaurant

[10:04 - 10:10]
should have good vegetarian options

[10:08 - 10:11]
let's explore an example that

[10:10 - 10:14]
demonstrates how you can use prompt

[10:11 - 10:18]
engineering to improve the quality of an

[10:14 - 10:21]
llms output Let's Take on the task of

[10:18 - 10:23]
planning a company event you need to

[10:21 - 10:26]
find a theme for an upcoming conference

[10:23 - 10:28]
let's write a prompt to Gemini to

[10:26 - 10:30]
generate a list of five potential themes

[10:28 - 10:32]
for an event

[10:30 - 10:34]
you can use similar prompts in chat GPT

[10:32 - 10:36]
Microsoft co-pilot or any other

[10:34 - 10:38]
conversational AI

[10:36 - 10:42]
tool now let's review the

[10:38 - 10:44]
response well this isn't what we wanted

[10:42 - 10:46]
we've gotten a list that seems more

[10:44 - 10:49]
related to party themes than themes for

[10:46 - 10:52]
a professional conference our prompt

[10:49 - 10:55]
didn't provide enough context to produce

[10:52 - 10:58]
the output we needed it wasn't clear or

[10:55 - 11:01]
specific enough let's try this again

[10:58 - 11:03]
this time we'll type The Prompt generate

[11:01 - 11:05]
a list of five potential themes for a

[11:03 - 11:07]
professional conference on customer

[11:05 - 11:10]
experience in the hospitality

[11:07 - 11:12]
industry this prompt is much more

[11:10 - 11:13]
specific making it clear that it's a

[11:12 - 11:17]
professional conference on customer

[11:13 - 11:18]
experience in the hospitality industry

[11:17 - 11:21]
let's examine the

[11:18 - 11:23]
response this is much better we

[11:21 - 11:25]
engineered our prompt to include

[11:23 - 11:28]
specific relevant context so Gemini is

[11:25 - 11:31]
able to generate useful output when you

[11:28 - 11:34]
provide clear Specific Instructions that

[11:31 - 11:38]
include necessary context you enable

[11:34 - 11:41]
llms to generate useful output keep in

[11:38 - 11:44]
mind that due to llm limitations there

[11:41 - 11:46]
might be some instances in which you

[11:44 - 11:49]
can't get quality output regardless of

[11:46 - 11:51]
the quality of your prompt for example

[11:49 - 11:54]
if you're prompting the llm to find

[11:51 - 11:56]
information about a current event but

[11:54 - 11:59]
the llm doesn't have access to that

[11:56 - 12:02]
information it won't be able to Prov

[11:59 - 12:05]
provide the output you need and like in

[12:02 - 12:08]
other areas of design prompt engineering

[12:05 - 12:10]
is often an iterative process sometimes

[12:08 - 12:12]
even when you do provide clear and

[12:10 - 12:15]
Specific Instructions you may not get

[12:12 - 12:17]
the output you want on your first try

[12:15 - 12:19]
when our first prompt didn't produce the

[12:17 - 12:22]
response we wanted we revised the prompt

[12:19 - 12:24]
to improve the output the second

[12:22 - 12:26]
iteration provided instructions that

[12:24 - 12:30]
were clear and specific enough to

[12:26 - 12:32]
produce a more useful output

[12:30 - 12:35]
there are multiple ways to leverage an

[12:32 - 12:38]
llm capabilities at work to boost

[12:35 - 12:40]
productivity and creativity a common one

[12:38 - 12:45]
is content creation you can use an llm

[12:40 - 12:47]
to create emails plans ideas and more as

[12:45 - 12:49]
an example you can ask an llm to help

[12:47 - 12:52]
you write an article about a

[12:49 - 12:54]
work-related topic let's prompt Gemini

[12:52 - 12:57]
to create an outline for an article on

[12:54 - 12:58]
data visualization best practices the

[12:57 - 13:00]
article is for entry-level business

[12:58 - 13:02]
analysts

[13:00 - 13:05]
notice that the prompt begins with the

[13:02 - 13:07]
verb create it's often helpful to

[13:05 - 13:10]
include a verb in your prompt to guide

[13:07 - 13:12]
the llm to produce useful output for

[13:10 - 13:14]
your intended

[13:12 - 13:17]
task the output provides a helpful

[13:14 - 13:21]
outline for a first draft of the

[13:17 - 13:24]
article you can also use an llm for

[13:21 - 13:26]
summarization an llm can summarize a

[13:24 - 13:28]
lengthy document's main points for

[13:26 - 13:30]
example you might ask Gemini to

[13:28 - 13:33]
summarize a detailed paragraph about

[13:30 - 13:35]
project management strategies we'll

[13:33 - 13:38]
Begin The Prompt with the verb summarize

[13:35 - 13:40]
and specify that we want the output to

[13:38 - 13:42]
be a single

[13:40 - 13:45]
sentence then we'll include the

[13:42 - 13:45]
paragraph We want Gemini to

[13:46 - 13:51]
summarize the output provides a

[13:48 - 13:53]
convenient one-sentence summary of the

[13:51 - 13:55]
paragraph while this example shows how

[13:53 - 13:58]
you can summarize a single paragraph you

[13:55 - 14:00]
can ask an llm to summarize longer text

[13:58 - 14:02]
and documents too

[14:00 - 14:05]
classification is another possible use

[14:02 - 14:07]
for instance you might prompt the llm to

[14:05 - 14:09]
classify the sentiment or feeling in a

[14:07 - 14:12]
group of customer reviews as positive

[14:09 - 14:15]
negative or neutral let's prompt Gemini

[14:12 - 14:18]
to classify customer reviews about a

[14:15 - 14:19]
retail website's new design as positive

[14:18 - 14:21]
negative or

[14:19 - 14:25]
neutral The Prompt includes the verb

[14:21 - 14:28]
classify to guide the output The Prompt

[14:25 - 14:31]
also contains the reviews in this

[14:28 - 14:34]
example there are are four reviews the

[14:31 - 14:36]
output accurately classifies the first

[14:34 - 14:39]
two reviews as negative the third as

[14:36 - 14:41]
positive and the fourth as neutral

[14:39 - 14:43]
consider how you could leverage an llm

[14:41 - 14:45]
to efficiently complete large

[14:43 - 14:49]
classification

[14:45 - 14:51]
tasks or you can use an llm for

[14:49 - 14:54]
extraction which involves pulling data

[14:51 - 14:56]
from text and transforming it into a

[14:54 - 14:59]
structured format that's easier to

[14:56 - 15:01]
understand suppose you have a report

[14:59 - 15:04]
that provides information about a global

[15:01 - 15:06]
organization you can prompt Gemini to

[15:04 - 15:09]
extract all mentions of cities and

[15:06 - 15:12]
revenue in the report and place them in

[15:09 - 15:14]
a table then we'll include the report in

[15:12 - 15:16]
our prompt Please be aware that you

[15:14 - 15:18]
should not input confidential

[15:16 - 15:20]
information into llms but in this

[15:18 - 15:23]
example the report is not

[15:20 - 15:25]
confidential the output displays a table

[15:23 - 15:27]
with columns for City and revenue this

[15:25 - 15:30]
presents the information in a well

[15:27 - 15:33]
organized format that's easy to review

[15:30 - 15:36]
another use is translation you can

[15:33 - 15:39]
leverage an llm to translate text

[15:36 - 15:41]
between different languages for example

[15:39 - 15:44]
you might ask Gemini to translate the

[15:41 - 15:44]
title of a training session from English

[15:44 - 15:47]
to

[15:44 - 15:49]
Spanish the output includes a variety of

[15:47 - 15:52]
Spanish translations to choose from and

[15:49 - 15:54]
explains the reasoning behind each

[15:52 - 15:57]
translation this information can help

[15:54 - 15:59]
you choose the most useful option for

[15:57 - 16:02]
your audience

[15:59 - 16:04]
or you can use an llm for editing such

[16:02 - 16:08]
as to change the tone of a section of

[16:04 - 16:11]
text from formal to casual and to check

[16:08 - 16:13]
if the text is grammatically correct for

[16:11 - 16:15]
example Gemini can help you edit a

[16:13 - 16:18]
technical analysis about electric

[16:15 - 16:21]
vehicles by making the language more

[16:18 - 16:22]
accessible for a non-technical audience

[16:21 - 16:25]
we'll start the prompt with the verb

[16:22 - 16:27]
edit and specify that the language

[16:25 - 16:29]
should be easy for a non-technical

[16:27 - 16:33]
audience to understand

[16:29 - 16:35]
after this we'll include the technical

[16:33 - 16:37]
analysis the output provides a version

[16:35 - 16:40]
of the analysis that an audience less

[16:37 - 16:42]
familiar with the technical details can

[16:40 - 16:46]
understand this is just one example of

[16:42 - 16:48]
how an llm can help you edit documents

[16:46 - 16:51]
llms can quickly customize the tone

[16:48 - 16:52]
length and format of documents to fit

[16:51 - 16:55]
your

[16:52 - 16:58]
needs one more use for an llm we'll

[16:55 - 17:02]
discuss is problem solving you can use

[16:58 - 17:05]
utilize an llm to generate solutions for

[17:02 - 17:07]
a variety of workplace challenges when

[17:05 - 17:10]
planning a company event for example you

[17:07 - 17:12]
could prompt the llm to find manyu

[17:10 - 17:15]
solutions that accommodate the food

[17:12 - 17:18]
restrictions of multiple guests while

[17:15 - 17:21]
following a holiday themed menu and

[17:18 - 17:23]
here's another example let's say you are

[17:21 - 17:25]
an entrepreneur who recently launched a

[17:23 - 17:28]
new copy editing service let's ask

[17:25 - 17:29]
Gemini to solve a problem related to the

[17:28 - 17:31]
copy editing service

[17:29 - 17:34]
we'll ask for suggestions for increasing

[17:31 - 17:36]
the client base the output provides

[17:34 - 17:38]
specific suggestions for reaching new

[17:36 - 17:41]
clients optimizing services and growing

[17:38 - 17:44]
the business I love these ideas let's

[17:41 - 17:46]
ask Gemini to draft an email so we can

[17:44 - 17:49]
easily share these ideas with

[17:46 - 17:51]
others llms can help you brainstorm

[17:49 - 17:54]
solutions for many different types of

[17:51 - 17:56]
problems I'm definitely excited by the

[17:54 - 17:59]
variety of ways we can leverage llms

[17:56 - 18:01]
when completing workplace tasks it's a

[17:59 - 18:03]
very important skill to practice if you

[18:01 - 18:06]
want to use AI effectively in the

[18:03 - 18:11]
workplace coming up we'll focus more on

[18:06 - 18:13]
evaluating output and iterating on your

[18:11 - 18:16]
prompt have you ever created a

[18:13 - 18:19]
presentation for a client or design a

[18:16 - 18:21]
website for your new business if so you

[18:19 - 18:24]
may have used an iterative process to

[18:21 - 18:26]
achieve your goal in an iterative

[18:24 - 18:29]
process you create a first version

[18:26 - 18:32]
evaluate it and improve upon it for the

[18:29 - 18:34]
next version then you repeat these steps

[18:32 - 18:37]
until you get the desired outcome for

[18:34 - 18:39]
example if you're developing a proposal

[18:37 - 18:41]
report or other document to share with

[18:39 - 18:44]
your co-workers you might produce

[18:41 - 18:46]
multiple drafts and make improvements on

[18:44 - 18:49]
each draft until you are satisfied with

[18:46 - 18:52]
the result taking an iterative approach

[18:49 - 18:55]
is often the most effective way to solve

[18:52 - 18:58]
a problem or develop a product an

[18:55 - 19:00]
iterative process is also effective in

[18:58 - 19:02]
prompt engine engering prompt

[19:00 - 19:04]
engineering often requires multiple

[19:02 - 19:07]
attempts before you get the optimal

[19:04 - 19:09]
output most of the time you won't get

[19:07 - 19:11]
the best result on your first try if you

[19:09 - 19:14]
try something and it doesn't work don't

[19:11 - 19:17]
get discouraged instead carefully

[19:14 - 19:19]
evaluate the output to determine why you

[19:17 - 19:21]
didn't get the response you wanted then

[19:19 - 19:24]
revise your prompt to try for a better

[19:21 - 19:26]
result let's consider possible reasons

[19:24 - 19:29]
you might not get useful output after

[19:26 - 19:32]
creating a clear and specific prompt

[19:29 - 19:35]
first differences in large language

[19:32 - 19:38]
models can affect output each llm is

[19:35 - 19:40]
developed with unique training data and

[19:38 - 19:42]
programming techniques and has different

[19:40 - 19:46]
background knowledge about specific

[19:42 - 19:49]
domains for this reason different models

[19:46 - 19:52]
might respond to similar prompts in

[19:49 - 19:54]
different ways and might fail to provide

[19:52 - 19:57]
an adequate response to some prompts

[19:54 - 20:00]
taking an iterative approach with the

[19:57 - 20:04]
llm you're using will produce the best

[20:00 - 20:06]
results second llm limitations

[20:04 - 20:09]
previously you learned that llm output

[20:06 - 20:12]
May sometimes be inaccurate biased

[20:09 - 20:15]
insufficient irrelevant or inconsistent

[20:12 - 20:17]
you should critically evaluate all llm

[20:15 - 20:20]
output by asking yourself the following

[20:17 - 20:23]
questions is the output accurate is the

[20:20 - 20:26]
output unbiased does the output include

[20:23 - 20:29]
sufficient information is the output

[20:26 - 20:32]
relevant to my project or task

[20:29 - 20:36]
and finally is the output consistent if

[20:32 - 20:38]
I use the same prompt multiple times if

[20:36 - 20:40]
you identify any issues when you

[20:38 - 20:42]
evaluate output iterating on your

[20:40 - 20:44]
initial prompt can often help you

[20:42 - 20:47]
resolve these issues and get better

[20:44 - 20:50]
output to begin if you notice there's

[20:47 - 20:52]
any context missing your prompt add it

[20:50 - 20:54]
your choice of words can also

[20:52 - 20:57]
significantly impact in lm's output

[20:54 - 20:59]
using different words or phrasing in

[20:57 - 21:02]
your prompts often yields different

[20:59 - 21:04]
responses from the model experimenting

[21:02 - 21:07]
with different phrasings can help you

[21:04 - 21:09]
obtain the most useful output now that

[21:07 - 21:11]
you know more about iterative prompting

[21:09 - 21:14]
let's consider an example suppose you

[21:11 - 21:16]
work as a human resources coordinator

[21:14 - 21:18]
for a video production company the

[21:16 - 21:20]
company wants to develop an internship

[21:18 - 21:23]
program for students who are Exploring

[21:20 - 21:25]
Careers in animation and Motion Graphics

[21:23 - 21:27]
design the company is based in the

[21:25 - 21:30]
United States in the state of

[21:27 - 21:32]
Pennsylvania my home state your team

[21:30 - 21:34]
wants to partner with local colleges to

[21:32 - 21:37]
provide internship opportunities for

[21:34 - 21:38]
students in Pennsylvania as a first step

[21:37 - 21:41]
you need to create a list of colleges in

[21:38 - 21:43]
Pennsylvania that have animation

[21:41 - 21:45]
programs the list should include

[21:43 - 21:48]
necessary details about the colleges and

[21:45 - 21:51]
be in a well organized format that your

[21:48 - 21:53]
team can quickly review let's review an

[21:51 - 21:56]
example using Gemini help me find

[21:53 - 22:00]
colleges with animation programs in

[21:56 - 22:02]
Pennsylvania next we examine our output

[22:00 - 22:04]
the output lists colleges in

[22:02 - 22:06]
Pennsylvania that have animation

[22:04 - 22:09]
programs along with further information

[22:06 - 22:11]
related to these programs this is

[22:09 - 22:13]
helpful information but it isn't

[22:11 - 22:15]
structured in a way that your team can

[22:13 - 22:18]
quickly reference when contacting the

[22:15 - 22:20]
colleges organizing the information in a

[22:18 - 22:23]
table would make it easier to read and

[22:20 - 22:24]
understand especially for stakeholders

[22:23 - 22:27]
like your manager who may have limited

[22:24 - 22:30]
time we can iterate on the prompt by

[22:27 - 22:33]
adding context to to specify the desired

[22:30 - 22:34]
format of the output we'll type show

[22:33 - 22:37]
these options as a

[22:34 - 22:38]
table the output displays a table that

[22:37 - 22:40]
provides useful information about the

[22:38 - 22:43]
location of each college and the

[22:40 - 22:45]
specific type of degree it offers now

[22:43 - 22:48]
the list is in a well organized format

[22:45 - 22:50]
that's easier for your team to follow

[22:48 - 22:52]
although the table contains most of the

[22:50 - 22:55]
information your team needs it doesn't

[22:52 - 22:58]
include a key detail whether the school

[22:55 - 23:00]
is a public or private institution your

[22:58 - 23:02]
company wants to offer internships to

[23:00 - 23:05]
students from both public and private

[23:02 - 23:06]
colleges we'll add a new request for

[23:05 - 23:09]
Gemini to include the relevant

[23:06 - 23:11]
information in the table can you add a

[23:09 - 23:14]
column showing whether they are public

[23:11 - 23:16]
or private now the table includes a

[23:14 - 23:18]
column that indicates whether a college

[23:16 - 23:21]
is private or public to share this

[23:18 - 23:23]
information with your team in a format

[23:21 - 23:25]
that's easy to review and understand you

[23:23 - 23:27]
can use the export to Sheets feature

[23:25 - 23:30]
this will allow your team to easily

[23:27 - 23:32]
access and analyze the data and make

[23:30 - 23:35]
informed decisions based on the

[23:32 - 23:37]
results you should apply the same

[23:35 - 23:40]
iterative approach to further tasks when

[23:37 - 23:42]
you develop prompts for additional tasks

[23:40 - 23:45]
be aware that previous prompts made in

[23:42 - 23:47]
the same conversation can influence the

[23:45 - 23:49]
output of your most recent prompt if you

[23:47 - 23:52]
notice this is happening you may want to

[23:49 - 23:55]
start a new conversation iteration is a

[23:52 - 23:57]
key part of prompt engineering by taking

[23:55 - 24:00]
an iterative approach to prompting you

[23:57 - 24:04]
can Leverage llm to provide the most

[24:00 - 24:04]
useful output for your

[24:05 - 24:11]
needs have you ever created Something

[24:08 - 24:14]
New by building upon previous examples

[24:11 - 24:16]
perhaps you used a well-received report

[24:14 - 24:19]
as a reference when writing a similar

[24:16 - 24:21]
report or maybe you used a relevant and

[24:19 - 24:24]
engaging website as a model when

[24:21 - 24:28]
designing your own website examples are

[24:24 - 24:30]
also useful for llms including examples

[24:28 - 24:33]
in your prompt can help an llm better

[24:30 - 24:35]
respond to your request and can be an

[24:33 - 24:38]
especially effective strategy to get

[24:35 - 24:41]
your desired output we're going to

[24:38 - 24:43]
explore how to use examples in prompting

[24:41 - 24:45]
but first let's briefly discuss the

[24:43 - 24:48]
technical term shot in prompt

[24:45 - 24:51]
engineering the word shot is often used

[24:48 - 24:53]
as a synonym for the word example there

[24:51 - 24:55]
are different names for prompting

[24:53 - 24:59]
techniques based on the number of

[24:55 - 25:01]
examples given to the llm zero shot

[24:59 - 25:04]
prompting is a technique that provides

[25:01 - 25:08]
no examples in a prompt while one shot

[25:04 - 25:10]
prompting provides one example and fuse

[25:08 - 25:13]
shot prompting is a technique that

[25:10 - 25:16]
provides two or more examples in a

[25:13 - 25:19]
prompt because examples aren't included

[25:16 - 25:22]
in zero shot prompts the model is

[25:19 - 25:24]
expected to perform the task based only

[25:22 - 25:27]
on its training data and the task

[25:24 - 25:29]
description included in the prompt zero

[25:27 - 25:31]
shot prompting is most most likely to be

[25:29 - 25:35]
effective when you are seeking simple

[25:31 - 25:37]
direct responses zero shot prompting may

[25:35 - 25:40]
not be effective for tasks that require

[25:37 - 25:43]
the llm to respond in a more specific

[25:40 - 25:46]
nuanced way fuse shot prompting can

[25:43 - 25:49]
improve an lm's performance by providing

[25:46 - 25:51]
additional context and examples in your

[25:49 - 25:55]
prompt these additional examples can

[25:51 - 25:57]
help clarify the desired format phrasing

[25:55 - 26:00]
or general pattern few shot prompting

[25:57 - 26:02]
can be use for a range of tasks for

[26:00 - 26:05]
example you might use f shot prompting

[26:02 - 26:08]
to generate content in a particular

[26:05 - 26:10]
style let's say you work for an online

[26:08 - 26:12]
retailer you need to write a product

[26:10 - 26:14]
description for a new skateboard you

[26:12 - 26:17]
already have descriptions for existing

[26:14 - 26:19]
products such as a bicycle and roller

[26:17 - 26:21]
blades you want the skateboard

[26:19 - 26:24]
description to follow a similar style

[26:21 - 26:26]
and format we'll start with a prompt

[26:24 - 26:28]
that begins with some general

[26:26 - 26:30]
instructions write a one sentence

[26:28 - 26:33]
description of a product it should

[26:30 - 26:35]
contain two adjectives that describe the

[26:33 - 26:38]
product we also specify that we want

[26:35 - 26:40]
Gemini to review the examples we provide

[26:38 - 26:42]
and write the description of the

[26:40 - 26:46]
skateboard in the same

[26:42 - 26:48]
style because this is a few shot prompt

[26:46 - 26:51]
we need to provide examples that model

[26:48 - 26:54]
the style we want each example contains

[26:51 - 26:58]
a label indicating the product being

[26:54 - 27:00]
described a bicycle and roller blades

[26:58 - 27:03]
and each description is one sentence

[27:00 - 27:05]
long and contains two adjectives sleek

[27:03 - 27:08]
and durable for the bicycle and smooth

[27:05 - 27:12]
and stylish for the roller

[27:08 - 27:14]
blades next we type the label skateboard

[27:12 - 27:16]
when we add this label and leave the

[27:14 - 27:18]
product description blank we indicate to

[27:16 - 27:20]
Gemini that we want it to complete the

[27:18 - 27:22]
description of the skateboard like it

[27:20 - 27:25]
did with the other two product

[27:22 - 27:27]
descriptions let's review our

[27:25 - 27:29]
output the output offers a product

[27:27 - 27:32]
description of the skateboard that meets

[27:29 - 27:34]
the criteria we requested and is in the

[27:32 - 27:36]
same writing style and format as the

[27:34 - 27:39]
examples we included in our

[27:36 - 27:42]
prompt in this case two examples were

[27:39 - 27:44]
enough to obtain useful results but

[27:42 - 27:46]
there is no definitive rule for the

[27:44 - 27:49]
optimal number of examples to include in

[27:46 - 27:51]
a prompt some llms can accurately

[27:49 - 27:55]
reproduce patterns using only a few

[27:51 - 27:57]
examples while other llms need more at

[27:55 - 27:58]
the same time if you include too many

[27:57 - 28:01]
examples

[27:58 - 28:04]
an lm's responses may become less

[28:01 - 28:07]
flexible and creative and they may

[28:04 - 28:09]
reproduce the examples too closely

[28:07 - 28:12]
experiment with the number of examples

[28:09 - 28:15]
to include to get the best results for

[28:12 - 28:17]
your specific task now you know a

[28:15 - 28:20]
prompting technique that will help you

[28:17 - 28:22]
get better quality output F shot

[28:20 - 28:25]
prompting is an effective strategy that

[28:22 - 28:29]
can help you guide an llm to generate

[28:25 - 28:30]
more useful responses

[28:29 - 28:33]
you've learned a lot about writing

[28:30 - 28:36]
prompts that you can apply to workplace

[28:33 - 28:40]
tasks in this section we discussed large

[28:36 - 28:42]
language model or llm output We examined

[28:40 - 28:44]
how llms produce their output and

[28:42 - 28:47]
potential issues you might encounter in

[28:44 - 28:49]
the output after this we focused on a

[28:47 - 28:52]
key principle of prompt engineering

[28:49 - 28:55]
creating clear and specific prompts you

[28:52 - 28:58]
learn just how important it is to

[28:55 - 29:01]
specify what you want the llm to do and

[28:58 - 29:04]
to include supporting context to help it

[29:01 - 29:06]
provide better output we then went on to

[29:04 - 29:10]
discover how to improve the quality of

[29:06 - 29:12]
AI output through iteration it's

[29:10 - 29:15]
essential that you evaluate your output

[29:12 - 29:18]
and then revise your prompt as

[29:15 - 29:20]
needed lastly we learned about fuse shot

[29:18 - 29:24]
prompting which involves providing

[29:20 - 29:27]
examples to guide the llm I want to

[29:24 - 29:30]
offer a final tip before I go we focused

[29:27 - 29:32]
on promp ing large language models you

[29:30 - 29:35]
can use the same general principles when

[29:32 - 29:37]
you prompt other kinds of AI models too

[29:35 - 29:41]
for instance the next time you want to

[29:37 - 29:44]
use AI to generate an image try to be as

[29:41 - 29:47]
clear and specific as possible and then

[29:44 - 29:49]
iterate to get closer to the output you

[29:47 - 29:52]
want it's been great guiding you through

[29:49 - 29:54]
the process of prompt engineering I hope

[29:52 - 29:56]
you continue to apply and develop these

[29:54 - 29:58]
skills as you leverage conversational AI

[29:56 - 30:01]
tools in the workplace

[29:58 - 30:03]
to continue learning I encourage you to

[30:01 - 30:05]
explore the topic of using AI

[30:03 - 30:07]
responsibly as part of Google AI

[30:05 - 30:10]
Essentials

[30:07 - 30:10]
[Music]

## コメント

### 1. @deekana (👍 15)
The best presenter I have ever listened to. Kudos! Perfect and purposeful use of tone, voice, and body language. Effective delivery. I learned more about presentation skills from this VDO than about prompt engineering lol. The team also did very well with filming, editing and directing. Well done!

> **@teenytinytoons** (👍 0): thought i was watching an apple keynote. he's incredible.

> **@James-g7m7v** (👍 0): He is definitely the worst presenter I have ever seen.

### 2. @AIRecruiterTraining (👍 13)
Table of Contents 
- [00:00] Introduction to Prompt Engineering
  - Prompt engineering is designing prompts to get the desired output from conversational AI tools.
  - Prompt phrasing can influence the AI model's response, similar to how we use language in daily interactions.
- [01:09] Motivation for Prompt Engineering
  - Yufeng, a Google engineer, was motivated by the inefficiency of getting useful responses from language models.
  - The goal is to improve the efficiency of conversational AI tools.
- [03:00] Understanding LLMs for Effective Prompt Engineering
  - Understanding how LLMs work and their limitations is crucial for effective prompt engineering.
  - LLMs are AI models trained on massive amounts of text to identify patterns and generate responses to prompts.
- [04:01] How LLMs Predict Words and Generate Text
  - LLMs are trained on vast amounts of text data to learn language patterns and predict the next word in a sequence.
  - They analyze probabilities to choose the most likely word based on the context, but may vary in their responses.
- [04:57] ⚠ Limitations of LLMs: Bias and Unexpected Outputs
  - LLM outputs can be biased due to biases present in their training data.
  - You may not always get the desired output because LLMs rely on statistical analysis of training data.
- [05:26] ⚠ Limitations of LLMs: Bias and Unexpected Outputs (continued)
  - LLM outputs can be factually inaccurate due to limitations in training data or its analysis methods (hallucinations).
  - Examples of hallucinations include generating incorrect historical data or company information.
- [06:54] Critical Evaluation of LLM Outputs
  - It's crucial to critically evaluate LLM outputs for factual accuracy, bias, relevance, and information sufficiency. 
  - This applies to all LLM outputs, regardless of the task (summarization, marketing ideas, project plans).
- [08:22] Prompt Engineering for Effective LLM Use
  - The quality of the prompt affects the quality of the LLM output, similar to how high-quality ingredients affect a dish.
  - Prompt engineering involves designing clear, specific prompts with relevant context to get the desired output from an LLM.
- [10:23] The Importance of Context in Prompt Engineering
  - Providing clear and specific context in prompts is crucial for getting the desired output from LLMs.
  - Example: A prompt about a professional conference will lead to different results than a prompt about a party.
- [11:25] Prompt Engineering as an Iterative Process
  - Even well-designed prompts may require iteration to achieve the best results from LLMs.
  - The first attempt may not be perfect, so revising the prompt based on the initial output can be necessary.
- [12:21] LLMs for Content Creation and Summarization Tasks
  - LLMs can be used to create content (articles, outlines, emails) and summarize lengthy documents.
  - Provide clear instructions and specify the desired task (create, summarize) in the prompt.
  - Example prompts are given for creating an article outline and summarizing a paragraph. 
- [14:24] LLMs for Text Classification and Data Extraction
  - LLMs can be used to classify text data (e.g., sentiment analysis of reviews) and extract information from text to a structured format (e.g., creating tables from reports).
  - Examples are provided for classifying customer reviews and extracting city and revenue data from a report.
- [15:25] LLMs for Translation and Text Editing
  - LLMs can translate text between languages and edit text for tone, style, and grammar.
  - Examples are given for translating a training session title and changing the tone of a technical analysis.
- [18:22] Iterative Prompt Engineering for Optimal Output
  - Prompt engineering is an iterative process, similar to developing a product or presentation.
  - It often requires multiple tries and revisions to the prompt before achieving the desired LLM output.
  - Don't be discouraged by initial failures; instead, evaluate the output and revise the prompt accordingly.
- [19:19] Iterative Prompt Improvement for Better LLM Output
  - Getting the best output from an LLM often requires iteration on the prompt.
  - Evaluate the output after each attempt and revise the prompt to address shortcomings.
  - Reasons to revise the prompt include missing context, inaccurate information, or unsuitable formatting.
- [21:42] Refining Prompts for Well-Structured Output
  - LLMs can be instructed to deliver output in a specific format (e.g., table).
  - Use clear and specific language to indicate the desired format in the prompt.
  - The example focuses on creating a well-organized table with relevant college information.
- [24:11] Using Examples to Improve Prompt Efficiency
  - Including examples (or "shots") in prompts can significantly improve the quality of LLM output.
  - There are different prompting techniques based on the number of examples provided: zero-shot, one-shot, and few-shot.
  - Zero-shot prompts give no examples, while few-shot prompts provide two or more examples to guide the LLM. 
- [26:06] Using Few-Shot Prompting to Achieve Specific Style or Format
  - Few-shot prompting provides multiple examples (2+) to guide the LLM towards a desired output style or format. 
  - The prompt should specify the number of adjectives, sentence length, and overall style desired.
  - The example focuses on creating a product description that matches the style of previous examples.

### 3. @abhijeetcreates (👍 3)
What an amazingly crafted learning video, this man has become my most favourite teacher (who has actually added to my skills). although i knew most of the concepts already , | "This lecture enhanced those skills like and Upgrade"... These 30 minutes are gonna help me out many times in the future . A Big Thanks to you. |

> **@teenytinytoons** (👍 0): saved it my AI playlist as well

### 4. @djarshad009 (👍 8)
One of the best and most informative videos about Prompt Engineering. The presenter is extremely clear and simple to understand, his passion for AI is very prominent through his speech. 

Good Job 

> **@teenytinytoons** (👍 0): 100000% agree.

### 5. @DJ_QUANT (👍 0)
Solid breakdown—prompt quality is everything. Promptomizer is great for rewording prompts into stronger versions.

### 6. @ahmedhammam (👍 0)
7:35 
Be specific while writing a prompt to get the most precise result.
Human supervision is necessary to avoid hallucinations 😊
9:25 
LLM is based on possibilities you have to be specific....

### 7. @attorneyarrasmith (👍 1)
Forget ChatGPT—this guy just taught me public speaking through a prompt video. 🎤🔥

### 8. @chrismeyers4678 (👍 0)
Great video thanks!
At time 7:15 you suggest we need to be critical the "output" fact is accurate, unbiased and relevant. If it is a fact why do I need to check if it's biased. According to Gemini, a fact itself cannot be biased. Help please. At what point can I just trust it works correctly and not have to check? Is this a free vs pay version thing?

### 9. @chemtech7 (👍 0)
Very happy that I’ve already been practicing everything mentioned here & more correctly.

### 10. @capt2026 (👍 9)
Crystal clear presentation.  Good job!

> **@James-g7m7v** (👍 0): A bad job was done.

### 11. @bahlechonco211 (👍 27)
i love this this video but why are you crouching? the furniture looks small

> **@coolfrisbee** (👍 4): I created a prompt for a tiny chair

> **@bahlechonco211** (👍 2): @@UIUXexe 😂😂😂😂

> **@bahlechonco211** (👍 0): @@coolfrisbee 😂😂😂

> **@dasalsakid** (👍 1): 😂😂😂😂 so true

### 12. @mahmoudabdelazizabdelaziz2793 (👍 9)
I am against using the word engineering in “prompt engineering”. 
I understand that coding is undergoing significant changes with the introduction of LLMs such as GPT, however, in my opinion we need to be more precise in choosing words that match the actual task being done by the software developer. 
Engineering design has a very precise meaning and it incorporates a lot of stages starting from setting quantitative measurable specifications, proposing multiple design alternatives, evaluating them against the specifications, iterative analysis and design usually involving mathematics and physics, selecting a design after examining trade offs among conflicting metrics, implementing the selected design, evaluating the implementation on different platforms (if feasible), module testing, integration testing, and more. Those engineering design stages constitute a cycle that can be revisited with each iteration including even the specifications stage which can be modified if necessary, but with caution and after agreement with the project stakeholders. 
In that context, software engineering for example might not satisfy all the above while being considered engineering nevertheless, but with some reservation in my opinion. 
However, prompt engineering barely includes a very small fraction of engineering design stages, and thus in my opinion should be given another name.

> **@zealousprogrammer4539** (👍 2): I agree I am also against data "scientists" term in data science.

> **@andremax75** (👍 3): I always refer to it as prompt writing instead. It's not a job for a coder, but for a skilled writer.

> **@ErwinSalasErwin** (👍 0): Engineering is meant to solve problems with technology, since prompt engineering do it, it’s ok to say engineering

> **@Balloonbot** (👍 1): "Prompt pooping"

> **@otenyop** (👍 0): I agree 100%

### 13. @looitaiyew6142 (👍 1)
Hi Yufeng, Many Thanks for the presentation.Well done.

### 14. @thawafkhan14 (👍 0)
Very interesting and get alot of knowledge regarding AI.
Thanks Google Career Certificate Program

### 15. @ailearnershub (👍 1)
Super'o'Super presentation and explanation and emphasis - Thanks Gentleman...

### 16. @Farhadfarhadifar9094 (👍 0)
Thank you for this informative presentation. Good job!

### 17. @hmnemonic (👍 2)
I love his voice…❤ Thanks for your presentation!

> **@Berserq** (👍 0): Seems like an AI

### 18. @Footballfantastic07 (👍 2)
thanks for this type of knowledge pls update us with type of videos in future

> **@GoogleCareerCertificates** (👍 0): Subscribe to our channel for updates and future videos!

### 19. @Paul-e9x4h (👍 0)
Memang ini bisa sangat membantu dalam mengelola manajemen dan accounting perusahaan dalam proses inventarisasi dan  sistem  pembukuan anggaran

### 20. @LAG455 (👍 4)
Thank you for your teaching ☺️

