# 20240417 東急エージェンシー様 1on1 [Vol.01]

▶️アーカイブ：

[https://vimeo.com/935728644/9ae64584f0?share=copy](https://vimeo.com/935728644/9ae64584f0?share=copy)

[https://vimeo.com/935728644/9ae64584f0?share=copy](https://vimeo.com/935728644/9ae64584f0?share=copy)

▶️ログ：

[meeting_saved_chat.txt](20240417%20%E6%9D%B1%E6%80%A5%E3%82%A8%E3%83%BC%E3%82%B7%E3%82%99%E3%82%A7%E3%83%B3%E3%82%B7%E3%83%BC%E6%A7%98%201on1%20%5BVol%2001%5D%20d226573670cc47d293d4aa84dc7d724e/meeting_saved_chat.txt)

[meeting_saved_closed_caption.txt](20240417%20%E6%9D%B1%E6%80%A5%E3%82%A8%E3%83%BC%E3%82%B7%E3%82%99%E3%82%A7%E3%83%B3%E3%82%B7%E3%83%BC%E6%A7%98%201on1%20%5BVol%2001%5D%20d226573670cc47d293d4aa84dc7d724e/meeting_saved_closed_caption.txt)

▶️まとめ：

1. ***データの前処理の徹底***
    - 生データには余計な情報が多く含まれるため、分析前に不要な要素を取り除き、GPTに理解しやすい形に整形することが重要。
    - データの品質を高めるために、GPTを活用してテキストを要約・整形することも有効。
2. ***分析対象に関する最新知識のGPTへの付与***
    - GPTの学習データには最新の情報が含まれていないため、分析対象の最新トピックについては別途GPTに知識を与える必要がある。
    - 特にGPT-3.5-turboを使用する場合、Wikipediaページなどから最新情報を抽出し、事前に読み込ませることが肝要。
3. ***前処理と分析の切り分け***
    - データ前処理と分析を一気通貫で行おうとすると、どこでバグが発生するかわかりにくくなるリスクがある。
    - モジュールを分けて、前処理を専門に行うモジュールと、分析を行うモジュールを切り分けて開発することで、問題の切り分けと品質の向上が期待できる。

▶️結論：

A）3.5turbo利用なら

[[GPT for Sheets and Docs] [GPT for Excel]](https://gptforwork.com/) 等を活用して、GPT関数を駆使して

エクセル or Googleスプレットシートで前捌きが必要と感じた

とはいえ3.5の集計レベルでは限界あり

B）4.0turbo利用であっても

データ集計系は怪しい

GPT’s活用しPythonでコードカウントしないとエビデンスなき結果になると想定

ただ文字面からエビデンス妥当性は出せないが、大まかな傾向として納得感ある

ざっくりレビューは可能とみえる

よって組み合わせる

C）[GPT関数]前捌き→　[Claude3]高性能LLM→　[Python]Stack.AI

１、.csvを前捌きでデータクリーニングして、メタ情報を付け加える

２、高性能LLMのパワーで、アンケート自由回答の内容を読み込み

３、自由度の高いNo-Codeツールで自由にPythonを動かし集計

結果：難易度[8/10]

▶️議事録

> ***0分〜15分***
> 
> 
> ---
> 
> 13:00 - 13:15 ソーシャルリスニングとAIの活用に関する議論
> 
> - トークウォーカーの機能紹介: キーワードを入力するとSNSやウェブニュースの関連投稿を収集・表示。最新のOpenAI Japanの話題も即座に検索可能。[13:01:33]
> - 分析用データのエクスポート: 投稿内容や関連データをExcelやCSV形式でエクスポート可能。いいね数などを基準に有用な投稿を絞り込める。[13:04:03]
> - iPhone 15の評判分析の試み: 関連投稿から良い点・悪い点をGPTにまとめさせたが、最新情報の不足からGPTがライトニングポートの使用有無を誤認。[13:08:07]
> - データ前処理の重要性: 生データは余計な情報が多いため、GPTで要約・整形し品質を高める必要がある。高精度には手間がかかる。[13:10:09]
> - GPTへの最新知識の付与: 分析対象の最新トピックはGPTに別途知識を与える必要あり。Wikipedia情報などを事前に読み込ませることが有効。[13:14:32]
> - GPTのバージョン差: GPT-3.5-turboなど古いモデルは質問設計が不十分だとハルシネーションが起こりやすい。GPT-4の利用が望ましい。[13:14:02]
> 
> ***15分〜30分***
> 
> ---
> 
> 13:15 - 13:30 データ前処理の具体的な手順の議論
> 
> - 生データのクリーニングの必要性: 余計な記号や色などを取り除き、GPTに理解しやすい形に整形する。[13:15:55]
> - GPTへの最新知識の付与手順: Wikipedia等から最新情報を取得し、それをGPTに読み込ませる。GPT-3.5-turboの場合は特に重要。[13:17:04]
> - CSVを用いたデータ変換の試行: ExcelでCSV形式で保存し、メモ帳で開くことでカンマ区切りデータを得る。1列だとカンマ区切りにならない問題が発生。[13:20:56]
> - 行列の入れ替えによる問題解決: Excelの貼り付けオプションで行と列を入れ替えることで、適切なカンマ区切りデータが得られることを確認。[13:28:30]
> - 前処理済みデータの活用方針: 整形したデータをGPTに入力し、より正確な分析結果が得られることを期待。社内展開に向けた課題認識。[13:29:51]
> 
> ***30分〜45分***
> 
> ---
> 
> 13:30 - 13:45 トークン数の確認とGPT-4の活用方針の議論
> 
> - CSVデータのカンマ区切り化: ExcelでCSV保存し、メモ帳で開くとカンマ区切りデータになることを確認。1列だとカンマ区切りにならない問題を行列の入れ替えで解決。[13:30:28]
> - トークン数のカウント: トークナイザーを用いて、データのトークン数を確認。GPT-3.5-turboは最大16,000トークン、GPT-4は128,000トークンまで処理可能。[13:38:39]
> - GPT-4の活用の必要性: ツイッターのデータ分析では、精度向上のため1000件程度の投稿を扱う必要があり、GPT-4の利用が不可欠。[13:43:18]
> - 大量データの扱い方: 128,000トークンを超えるとGPT-4でも処理が困難になるため、適切なデータ量に調整することが重要。[13:41:46]
> - 前処理の重要性の再確認: 不要なデータを取り除く前処理が分析精度向上のカギを握る。GPTとスプレッドシートの連携など、高度な前処理手法の検討が必要。[13:44:09]
> 
> ***45分〜60分***
> 
> ---
> 
> 13:45 - 13:57 トライアル結果の確認とまとめ
> 
> - GPT-3.5-turboとGPT-4の比較: 左側のウィンドウでGPT-3.5-turbo、右側でGPT-4を使用し、同じプロンプトでの出力結果を比較。[13:46:00]
> - プロンプトの調整: 絶対条件にレポート末尾にカウント数を追加する指示を記載。マークダウン記法の適切な使用（スペース、コロン、ハイフンなど）により精度向上を図る。[13:48:03]
> - カウントの信頼性の問題: GPT-3.5-turboではカウント数に関するハルシネーションが発生。より正確な集計にはデータの前処理が不可欠。[13:50:49]
> - 前処理の重要性の再確認: ユーザーの回答が構造化されていないため、一発でのデータ処理は困難。GPTの性能向上を待つか、前処理の工夫が必要。[13:52:04]
> - 今後の進め方: 得られた知見を基に、村上氏が別の商品・サービスでも試行錯誤を重ねる。問題があれば佐藤氏にアドバイスを求め、精度の高いプロンプトを作成・共有していく。[13:53:58]
> - 前処理と分析の分離: 品質向上のため、前処理用と分析用のモジュールを分けて作ることも検討。ワンストップでの処理は現時点では難しい。[13:55:14]
> 

完